---
title: 01 图像模糊
aliases: 
tags: 
create_time: 2023-07-18 16:32
uid: 202307181632
banner: "[[5d2311df03d1540dbafa87708ce9a075_MD5.jpg]]"
---

# **十种图像模糊算法横向对比**

在展开全文，对这十种图像模糊算法进行分别介绍之前，这一节中先做一个总览，即一个横向的对比。要评判一种模糊算法的好坏，主要有三个标准：

*   **模糊品质（Quality）** 。模糊品质的好坏是模糊算法是否优秀的主要指标。  
*   **模糊稳定性（Stability）** 。模糊的稳定性决定了在画面变化过程中，模糊是否稳定，不会出现跳变或者闪烁。  
*   **性能（Performance）** 。性能的好坏是模糊算法是否能被广泛使用的关键所在。  

以下是本文涉及的十种模糊算法在标准情况下以上述三个指标作为评判标准的横向对比：

![[3be6ec8fa57874619388a514613d5378_MD5.jpg]]

从上表的对比可以看到，除了 Grainy Blur 因其模糊质感的特殊性获得了 “一般” 的模糊品质评级之外，另外九种模糊算法在模糊品质和稳定性这两方面都获得了不错的评级。这是因为给到足够的迭代次数，且不做 RT 的 DownSample，他们都可以收敛到一个高品质的模糊质感。

最终的分化在于性能，这才是评判一种算法性价比是否高，能否广泛用于实时渲染的关键因素。其中，可以看到仅双重模糊（Dual Blur）和粒状模糊（Grainy Blur）两种算法，获得了高的性能评级。当然，这是针对标准的算法而言，其他八种算法如果进行进一步的性能优化，也能具有更佳的性能。

# 1 高斯模糊（Gaussian Blur）
![[Pasted image 20221209111419.png]]


高斯模糊在图像处理领域，通常用于减少图像噪声以及降低细节层次，以及对图像进行模糊，其视觉效果就像是经过一个半透明屏幕在观察图像。

**从数字信号处理的角度看，图像模糊的本质一个过滤高频信号，保留低频信号的过程。过滤高频的信号的一个常见可选方法是卷积滤波**。从这个角度来说，图像的高斯模糊过程即图像与正态分布做卷积。由于正态分布又叫作 “高斯分布”，所以这项技术就叫作高斯模糊，而由于高斯函数的傅立叶变换是另外一个高斯函数，所以**高斯模糊对于图像来说就是一个低通滤波器。**
## 高斯方程
N 维空间高斯方程可以表示为：

$${\displaystyle G(r)={\frac {1}{{\sqrt {2\pi \sigma ^{2}}}^{N}}}e^{-r^{2}/(2\sigma ^{2})}} $$

在二维空间定义为：

$$G(u,v) = \frac{1}{2\pi \sigma^2} e^{-(u^2 + v^2)/(2 \sigma^2)} $$
高斯函数的 3 维图示：
![[a0d895f58fc674e3e16149395ba19929_MD5.jpg]]

- $\sigma$ 为为标准方差（一般取值为 1）
 - $r^2 = u^2 + v^2$ 为模糊半径，uv 分别对应了当前位置到卷积核中心的整数距离。要构建一个高斯核，我们只需要计算高斯核中各个位置对应的高斯值。
 - **为了保证滤波后的图像不会变暗，我们需要对高斯核中的权重进行归一化，即让每个权重除以所有权重的和，这样可以保证所有权重的和为 1。** 因此，高斯函数中 e 前面的系数实际不会对结果有任何影响。

![[Pasted image 20221209111535.png]]
>计算高斯核的过程

**高斯方程很好地模拟了邻域每个像素对当前处理像素的影响程度——距离越近，影响越大。高斯核的维数越高, 模糊程度越大。

## 高斯核
用于高斯模糊的**高斯核（Gaussian Kernel）** 是一个正方形的像素阵列，其中像素值对应于 2D 高斯曲线的值。高斯核中的每个元素都是基于高斯方程构建。

$\frac1{256}\cdot\begin{bmatrix} 1&4&6&4&1\\ 4&16&24&16&4\\ 6&24&36&24&6\\ 4&16&24&16&4\\ 1&4&6&4&1 \end{bmatrix}$

>图 一个典型的高斯核

图像中的每个像素被乘以高斯核，然后将所有这些值相加，得到输出图像中此处的值。

![[ce284695da5938ab8cd5a34fae02ad5d_MD5.jpg]]

- @ 高斯核特性：
1. **高斯核满足线性可分**（Linearly separable）：
$$二维高斯核变换=水平方向一维高斯核变换+竖直方向一维高斯核变$$
**这样只需要 $O(n\times M\times N)+O(m\times M\times N)$ 的计算复杂度，而原先的计算复杂度为 ${\displaystyle O(m\times n\times M\times N)}$** ，其中 $M, N$ 是需要进行滤波的图像的维数，$m、n$ 是滤波器的维数。

以下为一个高斯核的线性分解过程：

$\frac1{256}\cdot\begin{bmatrix} 1&4&6&4&1\\ 4&16&24&16&4\\ 6&24&36&24&6\\ 4&16&24&16&4\\ 1&4&6&4&1 \end{bmatrix} = \frac1{256}\cdot\begin{bmatrix} 1\\4\\6\\4\\1 \end{bmatrix}\cdot\begin{bmatrix} 1&4&6&4&1 \end{bmatrix}$

2. **高斯核具有对称性**，因此两个一维高斯核中包含了很多重复的权重。**对于一个大小为 5 的一维高斯核，我们实际只需要记录 3 个权重值即可。**
## 实现思路
**实现思路：** 采用经过线性分解的高斯核的方案，分两个 pass
1. 第一个 Pass 使用使用竖直方向的一维高斯核对图像进行滤波
2. 第二个 Pass 再使用再使用水平方向的一维高斯核对图像进行滤波。
3. 用乒乓 RT 交互 blit 的方法。

**脚本：**
- 模糊迭代次数 `iterations`：次数越多越模糊
- 模糊半径 `blurRadius`，控制 shader 的 `_BlurOffset`，越大模糊程度越高，不会影响采样数。过大会造成虚影
- 降采样 `downSample`：越大，需要处理的像素数越少可以提高性能，同时进一步提高模糊程度。过大会使图像像素化

```cs
//获取rtDescriptor，可以重写RT的信息
rtDescriptor.width /= downSample.value; //降采样
rtDescriptor.height /= downSample.value;
//创建临时RT0
RenderingUtils.ReAllocateIfNeeded(ref m_TempRT0, rtDescriptor);
```

以下是开启高斯模糊后处理前后的对比图：
![[a2ba8a50d99b076ab4deb9c58000d109_MD5.jpg]]

![[545ea3268f770ff2a09fcfd78cedc57f_MD5.jpg]]
[X-PostProcessing-Library/Assets/X-PostProcessing/Effects/GaussianBlur at master · QianMo/X-PostProcessing-Library (github.com)](https://github.com/QianMo/X-PostProcessing-Library/tree/master/Assets/X-PostProcessing/Effects/GaussianBlur)

# 2 方框模糊（Box Blur）

方框模糊（Box Blur），又常被称为盒式模糊，**其中所得到的图像中的每个像素具有的值等于其邻近的像素的输入图像中的平均值**。和高斯模糊一样，Box Blur 也是**低通滤波器**的一种形式。在图像处理领域，Box Blur 通常用于近似高斯模糊。因为根据中心极限定理，重复应用 Box Blur 可以得到和高斯模糊非常近似的模糊表现。

可以将 3 x 3 的 box blur 的 kernel 表示为如下矩阵

$${\displaystyle {\frac {1}{9}}{\begin{bmatrix}1&1&1\\1&1&1\\1&1&1\end{bmatrix}}} $$

而 2x2 的 box blur 的 kernel 表示为如下矩阵：

$${\displaystyle {\frac {1}{4}}{\begin{bmatrix}1&1\\1&1\end{bmatrix}}} $$

Box Blur 和高斯模糊的性质对比可见下图：

![[5398ef6ab756ba6d3396a4b8991b89db_MD5.jpg|450]]

图 3D 结构，2D 结构和示例矩阵对比（a）Box Blur（b）Gaussian Blur

以下是 Box Blur 的作用过程的总结：

![[2230f76a87bb09b1f3c5c17914d68de7_MD5.jpg|500]]

**Box Blur 也是线性可分的**，如有需要，也可以借助其此性质，如下所示：（这样算法和高斯模糊就同一了，只是权重不同而已）

![[cab011fd5ca1b902a762b581322e43c7_MD5.png]]

另外 box blur 也有不少扩展与变体，比如 Tent Blur（两次 Box Blur）、Quadratic Blur（三次 Box Blur）等，具体本文暂时就不展开了。

**Box Blur 的渲染效果接近高斯模糊，但性价比并不高，需要较多的迭代次数才能达到高品质的模糊效果。**

# 3 Kawase 模糊（Kawase Blur）

Kawase Blur 于 Masaki Kawase 在 GDC2003 的分享《Frame Buffer Postprocessing Effects in DOUBLE-S.T.E.A.L (Wreckless)》中提出。Kawase Blur 最初用于 Bloom 后处理特效，但其可以推广作为专门的模糊算法使用，且在模糊外观表现上与高斯模糊非常接近。 **Kawase Blur 的思路是对距离当前像素<mark style="background: #FF5582A6;">越来越远</mark>的地方对四个角进行采样，且在两个大小相等的纹理之间进行乒乓式的 blit**。
**创新点在于，采用了随迭代次数移动的 blur kernel**，而不是类似高斯模糊，或 box blur 一样从头到尾固定的 blur kernel。

![[b67f169b4a49c39a056a28de995c347e_MD5.png]]

![[ab5d93ebb5e83d884f816efdcdf41b6b_MD5.png]]

实践数据表明，在相似的模糊表现下，**Kawase Blur 比经过优化的高斯模糊的性能约快 1.5 倍到 3 倍**。

具体思路是在 runtime 层，基于当前迭代次数，对每次模糊的半径进行设置，而 Shader 层实现一个 4 tap 的 Kawase Filter 即可：

```
half4 KawaseBlur(TEXTURE2D_ARGS(tex, samplerTex), float2 uv, float2 texelSize, half pixelOffset)
{
    half4 o = 0;
    o += SAMPLE_TEXTURE2D(tex, samplerTex, uv + float2(pixelOffset +0.5, pixelOffset +0.5) * texelSize); 
    o += SAMPLE_TEXTURE2D(tex, samplerTex, uv + float2(-pixelOffset -0.5, pixelOffset +0.5) * texelSize); 
    o += SAMPLE_TEXTURE2D(tex, samplerTex, uv + float2(-pixelOffset -0.5, -pixelOffset -0.5) * texelSize); 
    o += SAMPLE_TEXTURE2D(tex, samplerTex, uv + float2(pixelOffset +0.5, -pixelOffset -0.5) * texelSize); 
    return o * 0.2;
}
```

**完整的 Runtime + Shader 实现可见：**

[X-PostProcessing/Effects/KawaseBlur](https://github.com/QianMo/X-PostProcessing-Library/tree/master/Assets/X-PostProcessing/Effects/KawaseBlur)

Kawase Blur 渲染效果接近高斯模糊，但具有更好的性能：

![[d39a692992d38006b1bbed1d4ce968a1_MD5.jpg]]

# 4 双重模糊（Dual Blur）

Dual Kawase Blur，简称 Dual Blur，是 SIGGRAPH 2015 上 ARM 团队提出的一种衍生自 Kawase Blur 的模糊算法。其由两种不同的 Blur Kernel 构成，如下图所示。

![[524fa1af87d3e60617d5a0577a5befc6_MD5.jpg]]

**相较于 Kawase Blur 在两个大小相等的纹理之间进行乒乓 blit 的的思路，Dual Kawase Blur 的核心思路在于 <mark style="background: #FF5582A6;">blit 过程中进行降采样和升采样</mark>, 即对 RT 进行了降采样以及升采样。** 如下图所示：

![[4cf428fa94dc86dddc4c25bfd85980c4_MD5.jpg]]

由于灵活的升降采样带来了 blit RT 所需计算量的减少等原因， Dual Kawase Blur 相较于上文中提到的 Gauusian Blur、Box Blur、Kawase Blur 等 Blur 算法，**有更好的性能**，下图是相同条件下的性能对比。

![[0d29b413f35d49316fad8e8f0056f212_MD5.png]]

可以看到，Dual Kawase Blur 具有最佳的性能表现。


Dual Kawase Blur 的 Fragment Shader 实现为：

```
half4 Frag_DownSample(v2f_DownSample i): SV_Target
{
    half4 sum = SAMPLE_TEXTURE2D(_MainTex, sampler_MainTex, i.uv) * 4;
    sum += SAMPLE_TEXTURE2D(_MainTex, sampler_MainTex, i.uv01.xy);
    sum += SAMPLE_TEXTURE2D(_MainTex, sampler_MainTex, i.uv01.zw);
    sum += SAMPLE_TEXTURE2D(_MainTex, sampler_MainTex, i.uv23.xy);
    sum += SAMPLE_TEXTURE2D(_MainTex, sampler_MainTex, i.uv23.zw);

    return sum * 0.125;
}

half4 Frag_UpSample(v2f_UpSample i): SV_Target
{
    half4 sum = 0;
    sum += SAMPLE_TEXTURE2D(_MainTex, sampler_MainTex, i.uv01.xy);
    sum += SAMPLE_TEXTURE2D(_MainTex, sampler_MainTex, i.uv01.zw) * 2;
    sum += SAMPLE_TEXTURE2D(_MainTex, sampler_MainTex, i.uv23.xy);
    sum += SAMPLE_TEXTURE2D(_MainTex, sampler_MainTex, i.uv23.zw) * 2;
    sum += SAMPLE_TEXTURE2D(_MainTex, sampler_MainTex, i.uv45.xy);
    sum += SAMPLE_TEXTURE2D(_MainTex, sampler_MainTex, i.uv45.zw) * 2;
    sum += SAMPLE_TEXTURE2D(_MainTex, sampler_MainTex, i.uv67.xy);
    sum += SAMPLE_TEXTURE2D(_MainTex, sampler_MainTex, i.uv67.zw) * 2;

    return sum * 0.0833;
}
```

**完整的 Runtime + Shader 实现可见：**

[X-PostProcessing/DualKawaseBlur](https://github.com/QianMo/X-PostProcessing-Library/tree/master/Assets/X-PostProcessing/Effects/DualKawaseBlur)

XPL 中也提供了启发自 Dual Kawase Blur 的 Dual Gaussian Blur、Dual Box Blur、Dual Tent Blur 的实现。

*   **Dual Gaussian Blur**：[X-PostProcessing/Effects/DualGaussianBlur](https://github.com/QianMo/X-PostProcessing-Library/tree/master/Assets/X-PostProcessing/Effects/DualGaussianBlur)
*   **Dual Box Blur**：[X-PostProcessing/Effects/DualBoxBlur](https://github.com/QianMo/X-PostProcessing-Library/tree/master/Assets/X-PostProcessing/Effects/DualBoxBlur)
*   **Dual Tent Blur**：[X-PostProcessing/Effects/DualTentBlur](https://github.com/QianMo/X-PostProcessing-Library/tree/master/Assets/X-PostProcessing/Effects/DualTentBlur)  
    

Dual Kawase Blur 最终的模糊效果截图如下，可以看到其与高斯模糊的模糊表现也非常接近：

![[48c61a875f7050abc45063e7b55aadf6_MD5.jpg]]

以下是在初始 RT DownScale 为 1、Iteration 为 5 的设置下，Dual Kawase Blur 的渲染步骤：

![[4808a07560c0d020afc55101358e81cb_MD5.gif]]

同样，对模糊半径（Blur Radius）参数的调节，可以控制 Dual Kawase Blur 模糊的程度：

![[2b2492e79dd57b72b42e264a266ec452_MD5.gif]]

# 五、散景模糊（Bokeh Blur）

**散景（Bokeh）亦称焦外成像**，是一个摄影名词，一般**表示在景深较浅的摄影成像中，落在景深以外的画面，会有逐渐产生松散模糊的效果**。散景效果有可能因为摄影技巧或光圈孔形状的不同，而产生各有差异的效果。例如镜头本身的光圈叶片数不同（所形成的光圈孔形状不同），会让圆形散景呈现不同的多角形变化。此外，反射式镜头焦外的散景，会呈现独有的甜甜圈形状。

![[72d7bbe2169823531c5fddfe6e72cf39_MD5.jpg]]
>图不同相机参数下得到的不同散景模糊（Bokeh Blur）

散景（Bokeh）在摄影学中被称为焦外成像，而在光学上被称为 Circle of Confusion, CoC（弥散圆 / 散光圈 / 散射圆盘 ），即下图橙色 Image Plane 中的蓝色 C 所示区域。由于不同的物距（物体到镜头的距离）投影到镜头所形成的焦点不同，但 Image Plane 只能放在某个点上，所以就形成了 Circle of Confusion, CoC（弥散圆）。

![[4ab5aacc4baa6ad5839db4ae9c309bb7_MD5.jpg]]

图 散景（Bokeh）成因 （图片来自 GPU Gems 1）

![[b87c8901d62c00e238af7abf67d39867_MD5.jpg]]

图 散景（Bokeh）大小不同的成因，即 Circle of Confusion, CoC（弥散圆）的大小与人眼分辨率不同的区域。

镜头本身的光圈叶片数不同（所形成的光圈孔形状不同），会让散景形状呈现不同的多角形变化。从最初的多边形，过渡到最终的圆形。

![[8a9c0a33a624d3492e9a0d4d1c7370bf_MD5.jpg]]
>图不同光圈叶片数的镜头，决定了不同的散景形状

![[a11dc10be160ee0fb3e44be71b718acf_MD5.jpg]]
>图不同光圈叶片数的镜头，决定了不同的散景形状

![[63a2f401548ee204b104d6a2907c0d73_MD5.jpg]]
>图不同光圈数值的镜头形态，决定了不同的散景形态

从上图可以看出， 由于光圈大小和叶片数量的不同，散景（Bokeh）的形态各异。

在图形学领域模拟散景（Bokeh）的方法有很多，本文将以最标准的圆形散景为例，采用 Golden angle([https://en.wikipedia.org/wiki/Golden_angle](https://en.wikipedia.org/wiki/Golden_angle)) 的思路进行散景模糊（Bokeh Blur）算法的实现。

具体而言，算法思路是基于对大量离散螺旋型（spiral）分布的点的渲染，来模拟出圆形 Bokeh 的形状。

![[32968929fc6a49b44aa162ac095c193a_MD5.png]]

核心的 Shader 算法实现为：

```
half4 BokehBlur(VaryingsDefault i)
{
    half2x2 rot = half2x2(_GoldenRot);
    half4 accumulator = 0.0;
    half4 divisor = 0.0;

    half r = 1.0;
    half2 angle = half2(0.0, _Radius);

    for (int j = 0; j < _Iteration; j++)
    {
        r += 1.0 / r;
        angle = mul(rot, angle);
        half4 bokeh = SAMPLE_TEXTURE2D(_MainTex, sampler_MainTex, float2(i.texcoord + _PixelSize * (r - 1.0) * angle));
        accumulator += bokeh * bokeh;
        divisor += bokeh;
    }
    return accumulator / divisor;
}
```

即对于每一次迭代，让采样 uv 旋转一个角度，经过足够次数的迭代后，众多以圆形分散开来的点，将一起组成合适的 Bokeh 形状。

**完整的 Runtime + Shader 实现可见：**

[X-PostProcessing/Effects/BokehBlur](https://github.com/QianMo/X-PostProcessing-Library/tree/master/Assets/X-PostProcessing/Effects/BokehBlur)

下图为最终实现的效果图：

![[908ad5ed27b865f74260b6e32b9d6708_MD5.jpg]]

不同模糊半径（Blur Radius）变化，可以控制不同的 Bokeh 半径的变化：

![[6d6ec3238d3c35d6eb1580d3344b66a7_MD5.gif]]

# 六、移轴模糊 （Tilt Shift Blur）

移轴模糊（Tilt Shift Blur），又称镜头模糊（Lens Blur） ，是源自摄影领域的一种模糊算法。

在摄影领域，移轴摄影（Tilt-Shift Photography）泛指利用移轴镜头创作的作品，所拍摄的照片效果就像是缩微模型一样，非常特别。移轴镜头的作用，本来主要是用来修正以普通广角镜拍照时所产生出的透视问题，但后来却被广泛利用来创作变化景深聚焦点位置的摄影作品。移轴镜摄影是将真实世界拍成像假的一样，使照片能够充分表现出 “人造都市” 的感觉。

![[2ee9d1dc336ea7dbc319c4cc6fb639fc_MD5.jpg]]

图 移轴摄影作品

在后处理渲染中进行移轴摄影的模拟，可以采用 Grident uv 算法控制画面区域模糊强度，配合全屏模糊算法的方式来实现。

采用 Grident uv 算法控制画面区域模糊强度的 Shader 核心实现如下：

```
float TiltShiftMask(float2 uv) {
    float centerY = uv.y * 2.0 - 1.0 + _Offset; // [0,1] -> [-1,1]
    return pow(abs(centerY * _Area), _Spread);
}
```

得到的屏幕模糊强度的 mask 图如下：

![[b57fa2fbc10c79ed648499e3348ff3db_MD5.png]]

接着，配合合适的全屏图像模糊算法，如 Bokeh Blur，便可以营造出移轴摄影的画面感：

![[ced19b98ffbb6a430ae8b1b1affa6cdc_MD5.jpg]]

**完整的 Runtime + Shader 实现可见：**

[X-PostProcessing/TiltShiftBlurV2](https://github.com/QianMo/X-PostProcessing-Library/tree/master/Assets/X-PostProcessing/Effects/TiltShiftBlurV2)

**这里也有了另一个版本的实现：**

[X-PostProcessing/TiltShiftBlur](https://github.com/QianMo/X-PostProcessing-Library/tree/master/Assets/X-PostProcessing/Effects/TiltShiftBlur)

对模糊半径（Blur Radius）参数的调节，可以用于控制移轴 Bokeh 半径的变化：

![[7942e290d43af6c78a9218f94a058ca5_MD5.gif]]

在一定的区域平滑度（Area Smooth）设置下，调节区域尺寸（Area Size）可以控制移轴模糊区域的变化：

![[6d6be69ae1b2a81e62089ed3f2e17950_MD5.gif]]

# 七、光圈模糊（Iris Blur）

光圈模糊（Iris Blur）是 Photoshop CS6 中新增的功能，用于模拟浅景深的效果。

可以根据用户不同的输入参数，将普通照片模拟出景深以及散景的效果。（PS: Photoshop 中也同样有 Tilf-Shift Blur 功能）

![[a4ddac1669442d17c77d10576067b513_MD5.jpg]]

图 Photoshop 中的光圈模糊（Iris Blur）功能

![[62f6e809669e1349cbfa83669b3be7b3_MD5.jpg]]

图 Photoshop 中的光圈模糊（Iris Blur）功能

在后处理渲染中进行光圈模糊的模拟，可以采用一个径向的 Grident uv 算法沿轴心控制画面区域模糊强度，并配合全屏模糊算法的方式来实现。

采用径向 Grident uv 算法控制画面区域模糊强度的 Shader 核心实现如下：

```
float IrisMask(float2 uv) {
    float2 center = uv * 2.0 - 1.0 + _Offset; // [0,1] -> [-1,1] 
    return dot(center, center) * _AreaSize;
}
```

得到的屏幕模糊强度的 mask 图如下：

![[7490a7ab5d3a6fdcd61046da81001113_MD5.jpg]]

同样，配合合适的全屏图像模糊算法，如 Bokeh Blur，便可以营造出移轴摄影的画面感：

![[993a24b844bc2eea1904bff0ec175b82_MD5.png]]

**光圈模糊（Iris Blur）完整的 Runtime + Shader 实现可见：**

[X-PostProcessing/Effects/IrisBlurV2](https://github.com/QianMo/X-PostProcessing-Library/tree/master/Assets/X-PostProcessing/Effects/IrisBlurV2)

**这里也有另一个版本的实现：**

[X-PostProcessing/Effects/IrisBlur](https://github.com/QianMo/X-PostProcessing-Library/tree/master/Assets/X-PostProcessing/Effects/IrisBlur)

对模糊半径（Blur Radius）参数的调节，可以用于控制光圈 Bokeh 半径的变化：

![[2290da2d7961fd3343c586fb14f74e09_MD5.gif]]

同样，调节区域尺寸（Area Size）可以控制光圈模糊区域的变化：

![[72a93531e1b32cdabb3925e64a67bde1_MD5.gif]]

# 8 粒状模糊（Grainy Blur）

粒状模糊（Grainy Blur）是一种低成本的模糊方法，在**单 pass** 下即可有合适的模糊表现，性能出色，且其模糊质感有点类似在画面上蒙了一层细碎的冰霜。

**其思路是基于随机 uv 进行采样的抖动，以对粗粒度的模糊进行模拟。** 

核心算法的 Shader 实现如下：

```cs
float Rand(float2 n)
	{
		return sin(dot(n, half2(1233.224, 1743.335)));
	}
	
	half4 GrainyBlur(VaryingsDefault i)
	{
		half2 randomOffset = float2(0.0, 0.0);
		half4 finalColor = half4(0.0, 0.0, 0.0, 0.0);
		float random = Rand(i.texcoord);
		
		for (int k = 0; k < int(_Iteration); k ++)
		{
			random = frac(43758.5453 * random + 0.61432);;
			randomOffset.x = (random - 0.5) * 2.0;
			random = frac(43758.5453 * random + 0.61432);
			randomOffset.y = (random - 0.5) * 2.0;
			
			finalColor += SAMPLE_TEXTURE2D(_MainTex, sampler_MainTex, half2(i.texcoord + randomOffset * _BlurRadius));
		}
		return finalColor / _Iteration;
	}
```


![[5e521d9fce1475d4a235b85f67334f93_MD5.gif]]

# 9 径向模糊（Radial Blur）

径向模糊（Radial Blur）可以给画面带来很好的速度感，是各类游戏中后处理的常客，也常用于 Sun Shaft 等后处理特效中作为光线投射的模拟。

![[f4e7e425889af9b7ccde315a7d806f65_MD5.jpg]]
径向模糊实现起来比较简单，径向模糊的特点是从某个像素为中心向外辐射状扩散，因此需要采样的像素在原像素和中间点像素的连线上，不同连线上的点不会相互影响。简单的说，就是像素的颜色是由该像素的点与中心点之间连线上进行采样，然后求将这些采样点颜色的加权平均和作为该像素的颜色。
![[Pasted image 20230722135607.jpg]]

**径向模糊的原理：**
1. 首先选取一个径向轴心（Radial Center）
2. 然后将每一个采样点的 uv 基于此径向轴心进行偏移（offset），并进行一定次数的迭代采样
3. 最终将采样得到的 RGB 值累加，并除以迭代次数。

其核心算法的 Shader 代码实现如下：

```c
half4 RadialBlur(VaryingsDefault i)
{
    float2 blurVector = (_RadialCenter - i.texcoord.xy) * _BlurRadius;

    half4 acumulateColor = half4(0, 0, 0, 0);

    [unroll(30)]
    for (int j = 0; j < _Iteration; j ++)
    {
        acumulateColor += SAMPLE_TEXTURE2D(_MainTex, sampler_MainTex, i.texcoord);
        i.texcoord.xy += blurVector;
    }

    return acumulateColor / _Iteration;
}
```


![[057c64a81ce91934b7c784eccae025be_MD5.gif]]

# 10 方向模糊（Directional Blur）

方向模糊（Directional Blur）可以看做是径向模糊（Radial Blur）的一个变体。其主要思路是传入一个角度，然后在 runtime 层计算出对应的矢量方向：

```cs
float sinVal = (Mathf.Sin(m_angle) * m_blurRadius) / m_OffsetIterations;
float cosVal = (Mathf.Cos(m_angle) * m_blurRadius) / m_OffsetIterations;
m_blitMaterial.SetVector(s_DirectionalOffset, new Vector2(sinVal, cosVal));
```

然后，在 Shader 层，将每一个采样点的 uv 基于此方向进行**正负两次**偏移（offset），接着进行一定次数的迭代采样，最终将采样得到的 RGB 值累加，并除以迭代次数，得到最终的输出。

核心算法的 Shader 代码实现如下：

```cs
float4 frag(Varyings i) : SV_Target
{
    float4 color = float4(0, 0, 0, 0);

    for (int j = 0; j < _OffsetIterations; j++)
    {
        color += SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_BlitTexture, i.uv - _DirectionalOffset*j);
    }

      color /= _OffsetIterations;
    
    return color;
}
```


![[b6a3c0bf5e103dc4aaed1043607192a9_MD5.gif]]

# 11 运动模糊（Motion Blur）
## 累计缓冲 Accumulation Buffer
**利用一块累计缓存 (即一张RT) 来混合多张连续图像，当物体快速移动产生多张图像后，取它们之间的平均值作为最后的模糊图像。**
这种暴力方法**性能消耗大**，因为想要获取多张帧图像往往意味着我们需要在同一帧里渲染多次场景。

**优化**：不在一帧中把场景渲染多次，但保存之前的渲染结果，不断把当前的 RT 叠加到之前的 RT，从而产生一种运动轨迹的视觉效果。

**缺点**：混合了连续帧之间的图像，如果移动速度过快会使区域内平均叠加的图像变得过于稀疏，甚至会暴露出单帧图片，所以不适合速度变化过大的场合。

**两个 Pass**：一个更新 A 通道 RGB 通道，一个更新 A 通道。之所以要把 RGBA 通道分开，是因为更新 RGB 时我们需要设置它的 A 通道来混和图像，但又不希望 A 通道的值写入渲染纹理中。
```cs
float4 fragRGB(Varyings i) : SV_Target
{
    //a通道存储运动模糊的权重，以便进行透明度混合
    return float4(SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_BlitTexture, i.uv).rgb, _BlurTrain);
}

float4 fragA(Varyings i) : SV_Target
{
    //不让渲染纹理受到混合时使用的透明度值的影响
    return SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_BlitTexture, i.uv);
}

Pass
{
    Name "UpdateRGB"
    Tags
    {
        "LightMode" = "UniversalForward"
    }
    Blend SrcAlpha OneMinusSrcAlpha //透明度混合
    ColorMask RGB //只写入RGB通道，A通道只用来混合而不写入

    HLSLPROGRAM
    #pragma vertex vert
    #pragma fragment fragRGB
    ENDHLSL
}

Pass
{
    Name "UpdateA"
    Tags
    {
        "LightMode" = "UniversalForward"
    }
    Blend One Zero
    ColorMask A //只写入A通道

    HLSLPROGRAM
    #pragma vertex vert
    #pragma fragment fragA
    ENDHLSL
}
```
## 速度缓冲 Velocity Buffer

**速度缓冲（又称速度映射图）**中存储了各个像素当前的运动速度，然后利用该值来决定模糊的方向和大小。

**速度缓冲的生成方法：**
1. <mark style="background: #FF5582A6;">方法一</mark>：把场景中所有物体的速度渲染到一张纹理。
    - 缺点在于需要修改场景中所有物体的 shader 代码，使其添加计算速度的代码并输出到一张纹理（速度映射图）中。
2. <mark style="background: #FF5582A6;">方法二</mark>：《GPU Gems3》在第 27 章中介绍了一种生成速度映射图的方法。这种方法**利用深度纹理在片元着色器中为每个像素计算其在世界空间下的位置**，这是通过使用当前的视角 $*$ 投影矩阵（VP 矩阵）的逆矩阵对 NDC 下的顶点坐标进行变换得到的。**当得到世界空间中的顶点坐标后，我们使用前一帧的VP 矩阵对其进行变换，得到该位置在前一帧中的 NDC 坐标。然后，我们计算前一帧和当前帧的位置差，生成该像素的速度。** 
    - 优点：可以在一个屏幕后处理步骤中完成整个效果的模拟，
    - 缺点：需要在片元着色器中进行两次矩阵乘法的操作，对性能有所影响。
      
```cs
//脚本
//设置上一帧VP矩阵和当前帧VP逆矩阵
m_blitMaterial.SetMatrix("_PreviousViewProjectionMatrix", previousViewProjectionMatrix);
Matrix4x4 currentViewProjectionMatrix = renderingData.cameraData.camera.projectionMatrix * renderingData.cameraData.camera.worldToCameraMatrix;
Matrix4x4 currentViewProjectionInverseMatrix = currentViewProjectionMatrix.inverse;
m_blitMaterial.SetMatrix("_CurrentViewProjectionInverseMatrix", currentViewProjectionInverseMatrix);
previousViewProjectionMatrix = currentViewProjectionMatrix;
```

```cs
//片元着色器计算
//获取屏幕空间UV
float2 ScreenUV = GetNormalizedScreenSpaceUV(i.positionCS);

//用屏幕UV采样屏幕深度纹理得到像素的非线性深度
float depth = SAMPLE_DEPTH_TEXTURE(_CameraDepthTexture, sampler_CameraDepthTexture, ScreenUV).r;
// 当前帧NDC空间坐标
float4 currentPosNDC = float4(ScreenUV.x * 2 - 1, ScreenUV.y * 2 - 1, 2*depth-1, 1);

//得到当前帧世界空间坐标
float4 D = mul(_CurrentViewProjectionInverseMatrix, currentPosNDC);
float4 currentPosWS = D / D.w;

//上一帧裁剪空间坐标
float4 previousPosCS = mul(_PreviousViewProjectionMatrix, currentPosWS);
//做齐次除法得到上一帧NDC坐标
float4 previousPosNDC = previousPosCS / previousPosCS.w;

// NDC坐标差作为速度向量
float2 velocity = currentPosNDC.xy - previousPosNDC.xy;

float2 uv = i.uv;
float4 color = SAMPLE_TEXTURE2D(_BlitTexture, sampler_BlitTexture, uv);
uv += velocity * _BlurSize; //速度偏移uv进行采样
for (int it = 1; it < 3; it++, uv += velocity * _BlurSize)
{
    float4 currentColor = SAMPLE_TEXTURE2D(_BlitTexture, sampler_BlitTexture, uv);
    color += currentColor;
}
color /= 3;

return half4(color.rgb, 1.0);
```
