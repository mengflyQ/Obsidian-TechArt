GIF

![[cc874f74717d1f125b89391a74ccb833_MD5.webp]]

打个广告： 喜欢研究卡渲技术的朋友欢迎进群 950138189。

  在之前的文章里，我们曾在 [urp 管线的自学 hlsl 之路 第十四篇 广告牌算法](https://www.bilibili.com/read/cv6483887)介绍过《入门精要》的广告牌算法，其核心原理是通过构造一个旋转矩阵来变换顶点，但是它有个缺陷，就是当镜头处理垂直状态时，镜头会瞬间翻转 180；并且对于像镜头炫光这种效果在深度处理上还是使用 unity 自带的深度测试，并不能实现上动图中的遮挡闪烁效果。

本文章的广告牌算法和手动深度测试的算法并非本人原创，是根据 colin 大大的开源项目（https://github.com/ColinLeung-NiloCat/UnityURP-BillboardLensFlareShader）算法得来，colin 大大还有几个其他的开源工程大家可以去瞧瞧看，此文里有 2 个重点：1 广告牌算法；2 手动进行的深度测试。  
        **广告牌算法**：广告牌算法的目的是将平面始终平行于当前的摄像机的 xy 轴所在平面，在 unity 里创建一个 quad，这里即是需要将 quad 的 xy 平面始终平行于相机的 xy 平面。

![[b6084a6f892d9c81e18f3f47a41eb155_MD5.webp]]

该算法的核心思想是把模型坐标系下的坐标如点 A（x0，y0）的 x0 和 y0 看做点 A 相对于模型坐标系下的 X 轴和 Y 轴取的数据；则把它转换到相机坐标系下的行为，可以看作 A‘相对于相机坐标系下的 X 轴和 Y 轴取的数据定为（x0，y0）的操作。

![[220d2a710d40e84cb3ab60f3ec92cd1d_MD5.webp]]

故我们可以这么写：先取模型坐标系的原点坐标（0，0，0，1）变换到相机坐标系下，即使用 MV 矩阵，得到模型坐标系原点在相机坐标系的坐标；然后它再加上顶点相对于模型坐标系的坐标数值，就得到了顶点在相机坐标系下的数值。即：顶点 A 在模型坐标系下 X 轴取 x0，Y 轴取 y0，那我 A'在相机坐标系下的 X 轴也取 x0，Y 轴也取 y0，然后修正一下 2 坐标系的相对平移位置关系，就可以得到一个 qual 始终在相机坐标系的 XY 平面平行的 qual，然后变换到裁剪空间下，我们的广告牌技术就搞定了，是不是超简单！代码如下。

![[92ba904cfa4008126dd5f92c424b8ee9_MD5.webp]]

然后设置一下渲染类型和渲染队列，混合模式，关掉深度写入和深度测试（后面我们会实现自己的深度测试故需关掉 unity 自己的深度测试）。但是我想通过 Transform 的 scale 组件来控制缩放值，怎么实现？

GIF

![[0efa63dd3bec98c37a7a6f0cf7287cb9_MD5.webp]]

手动缩放

这就需要上一篇文章里 [URP 管线的自学 HLSL 之路 第三十一篇 更好的广告牌算法 -- 数学篇](https://www.bilibili.com/read/cv7010790)的结论了，不需要 C# 传值就可以直接通过模型转世界的矩阵得到 scale 值（为什么是模型转世界的矩阵？因为 Transform 的数据全是相对数据，是模型相对于父级的数据，这里是模型相对于世界的数据），代码如下，这样就可以在 Scale 里控制 qual 的缩放了。

![[2389c4cfef285abb4842671ce7676198_MD5.webp]]

这里下图的需要实现的效果，不能通过 unity 自带的深度测试来完成，故需要关掉 unity 的深度测试，我们手动来实现。它的实现原理是拿轴心所在位置的深度值和轴心附近小范围的多个像素区域采样的深度图的大小进行深度测试比较，得到一个可见度的比值：全部都通过测试，则当前颜色全部显示；测试通过一部分，则当前颜色半透明；测试全部失败，当前颜色全部不显示。

GIF

![[d9cf61409487d98bdc6b4d33c270e7af_MD5.webp]]

自定义深度测试

这个操作我们需要采样深度值，故需要在管线配置文件里打开 URP 内置的深度图获取。在这篇文章 [urp 管线的自学 hlsl 之路 第十六篇 屏幕深度制作护盾特效](https://www.bilibili.com/read/cv6519977)里讲过了深度图的内容，读者不熟悉的可以去这里看看。

前面都是开胃菜，从这里开始就是本文里最复杂的内容了，深度图的深度信息必须得到当前像素的屏幕坐标，我们需要在轴心位置附近构建一个小的区域，然后在这个小的区域内采样每个像素的深度值，但是为了更好的节省性能，我决定在顶点着色器里进行这个深度采样操作：由于顶点着色器并未进行光栅化，所以我们需要手动在顶点着色器里进行简陋版的三角形遍历，做一个低精度版本的线性插值来达到我们的需求。

先定义一个比例值，即轴心附近区域和整个 qual 区域的比例，我在代码里定义为 sampleRange=0.02，我们只需要在中心这个小小的区域里进行采样即可。

![[afa61829a614645ab189f942357a6dd7_MD5.webp]]

手动线性插值

然后对这个小的区域进行线性插值，我定义一个轴的单向插值个数（sampleCounts）为 3，即该轴的正方向插入 3 个点，则一个轴插入了 2*3+1=7 个值，故在这个小小的区域里，我们总共插入了（2*3+1）（2*3+1）=49 个采样点供后续对深度图进行采样，这样简单的线性插值就完成了。

为了方便计算，我直接在裁剪空间里对这个小区域的 49 个顶点进行插值计算得到 xy 坐标（samplePosition），这里不取 z 是因为我们不需要 z，我们只要得到裁剪空间下的 xy 进行透除得到 NDC 坐标再从（-1，1）重映射到（0，1）得到屏幕坐标，同时也要考虑 openGL 和 DX 平台差异去决定是否翻转 y 轴。代码里我们通过一个 for 循环去遍历全部的采样点的坐标，然后把它转换到屏幕坐标系下。一个 qual 的顶点数只有 4 个顶点所以循环的计算量可以忽略不计的。

![[41fcb8cbac002b06ebbe4d8e690dee6a_MD5.webp]]

得到屏幕坐标系下的坐标后，我们对它进行判断，如果不在屏幕内，直接进入下一个迭代；如果在屏幕里，在对深度图进行采样，同样由于未光栅化的原因我们只能采样 midmap 图像，然后变换到相机空间下得到线性深度，然后和相机空间下的轴位置的深度（这里取 z 的负值，因为相机空间是右手坐标系，z 轴正向和相机朝向相反）进行深度测试比较，如果测试通过，passCounts 计数增加 1。

![[3b3876c08af7c2515dba6c8f051bb7d1_MD5.webp]]

然后我们把 passCounts 除以总采样点数，得到的 0-1 之间的一个比例并赋予给 color 到片元着色器里去显示出来去 debug：

![[ce0b875f8398ea761d1751c7ed3a6f18_MD5.webp]]

然后我们旋转镜头，当没有建筑物遮挡时，qual 就是纯白色，即所有采样点的深度测试全部通过；当半透明时，即是部分采样点通过深度测试；当为纯透明时，即没有采样点通过深度测试。

GIF

![[88e6231814eb5e01134e8b7b30d1aca0_MD5.webp]]

然后我们把贴图颜色也乘上 color 值，就可以得到合理的效果了。这里我也增加了一个相机距离的蒙版效果：当镜头太靠近时，qual 也会变成全透明的渐变效果。深度小于 0.1 时，完全不可见，深度大于 2 时，完全可见。

![[48fcbf4084ee54132f92456868db6bf7_MD5.webp]]

GIF

![[b7b1761cb4d5b2f39b0d86281ecdbb2e_MD5.webp]]

然后我考虑到有时候的镜头光晕不一定是水平的，我在前面的代码里从摄像机坐标转换到裁剪坐标系下的过程中增加了一个旋转矩阵去旋转镜头光晕，一个 qual 的顶点数只有 4 个所以旋转矩阵的计算量可以忽略不计的，随便用。关于旋转矩阵的推导我已经讲了多次，读者可以在[第三十篇文章](https://www.bilibili.com/read/cv7010790)或者[第二十一篇](https://www.bilibili.com/read/cv6640445)里找到过程，但是这里要注意一下我们也在这一步进行了缩放，他两的顺序是先缩放了在旋转！和 unity 的处理方式先缩放在旋转在平移的顺序一致，这样才能保证缩放的均匀性。

![[ff266a91067100f9d919a1356ba187d3_MD5.webp]]

GIF

![[59089c3503625dc4ee60337e5d405407_MD5.webp]]

然后我修修改改一些细节，整个代码就大功告成了，我把源码也放到了码云上供大家参考，如果我有说错的地方望读者提出讨论。码云地址如下（码云有时候会抽风，我后面申请个 git 账号丢 github 上吧）  

https://gitee.com/matrixry/codes/[AV28](https://www.bilibili.com/video/av28)cjsbpt0zgy5qxkmhu83

**shader 源码如下**

 Shader "Unlit/NewUnlitShader"

{

 Properties

 {

 _MainTex("MainTex",2D)="White"{}

 [HDR]_BaseColor("BaseColor",Color)=(1,1,1,1)

 _Rotate("Rotate",Range(0,3.14))=0

 }

 SubShader

 {

 Tags{

 "RenderPipeline"="UniversalRenderPipeline"

 "Queue"="Overlay"

 }

 HLSLINCLUDE

 #include "Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl"

 CBUFFER_START(UnityPerMaterial)

 float4 _MainTex_ST;

 half4 _BaseColor;

 float _Rotate;

 CBUFFER_END

 TEXTURE2D(_MainTex);

 SAMPLER(sampler_MainTex);

 TEXTURE2D(_CameraDepthTexture);

 SAMPLER(sampler_CameraDepthTexture);

 struct a2v

 {

 float4 positionOS:POSITION;

 float2 texcoord:TEXCOORD;

 };

 struct v2f

 {

 float4 positionCS:SV_POSITION;

 float2 texcoord:TEXCOORD;

 float4 color:COLOR;

 };

 ENDHLSL

 pass

 {

 Tags{

 "LightMode"="UniversalForward" "RenderType"="Overlay" 

 }

 Blend one one 

 ZWrite off

 ZTest always 

 HLSLPROGRAM

 #pragma vertex VERT

 #pragma fragment FRAG

 v2f VERT(a2v i)

 {

 v2f o;

 o.texcoord=TRANSFORM_TEX(i.texcoord,_MainTex);

 float4 pivotWS=mul(UNITY_MATRIX_M,float4(0,0,0,1));

 float4 pivotVS=mul(UNITY_MATRIX_V,pivotWS);

 float ScaleX=length(float3(UNITY_MATRIX_M[0].x,UNITY_MATRIX_M[1].x,UNITY_MATRIX_M[2].x));

 float ScaleY=length(float3(UNITY_MATRIX_M[0].y,UNITY_MATRIX_M[1].y,UNITY_MATRIX_M[2].y));

 //float ScaleZ=length(float3(UNITY_MATRIX_M[0].z,UNITY_MATRIX_M[1].z,UNITY_MATRIX_M[2].z));// 暂时不用上

 // 定义一个旋转矩阵

 float2x2 rotateMatrix={cos(_Rotate),-sin(_Rotate),sin(_Rotate),cos(_Rotate)};

 // 用来临时存放旋转后的坐标

 float2 pos=i.positionOS.xy*float2(ScaleX,ScaleY);

 pos=mul(rotateMatrix,pos);

 float4 positionVS= pivotVS+float4(pos,0,1);// 深度取的轴心位置深度，xy 进行缩放

 o.positionCS=mul(UNITY_MATRIX_P,positionVS);

 float sampleCounts=3;// 这个值越大，线性插值精度越高，计算量也越大

 float singeAxisCounts=2*sampleCounts+1;

 float totalCounts=pow(singeAxisCounts,2);

 float passCounts=0;

 float sampleRange=0.2;// 中心区域的比例

 float pivotDepth=-pivotVS.z;// 取相机空间轴心的线性深度

 float4 pivotCS=mul(UNITY_MATRIX_P,pivotVS);// 得到裁剪空间的轴心位置

 for(int x=-sampleCounts;x<=sampleCounts;x++)

 {

 for(int y=-sampleCounts;y<=sampleCounts;y++)

 {

 float2 samplePosition=pivotCS.xy+o.positionCS.xy*sampleRange*float2(x,y)/singeAxisCounts;// 裁剪空间的采样位置

 float2 SSuv=samplePosition/o.positionCS.w*0.5+0.5;// 把裁剪空间手动透除，变换到 NDC 空间下，并根据当前平台判断是否翻转 y 轴

 #ifdef UNITY_UV_STARTS_AT_TOP

 SSuv.y=1-SSuv.y;

 #endif

 if(SSuv.x<0||SSuv.x>1||SSuv.y<0||SSuv.y>1)

 continue;// 如果满足跳出本次循环进入下次循环

 float sampleDepth=SAMPLE_TEXTURE2D_LOD(_CameraDepthTexture,sampler_CameraDepthTexture,SSuv,0).x;// 采样当前像素点的深度值

 sampleDepth=LinearEyeDepth(sampleDepth,_ZBufferParams);// 把它变换到线性空间

 passCounts+=sampleDepth>pivotDepth?1:0;// 把采样点的深度和模型的轴心深度进行对比 

 }

 }

 //o.positionCS=passCounts<1?float4(999,999,999,1):o.positionCS;// 如果一个点也没通过，则把裁剪空间的坐标丢远些, 但是有 bug

 o.color=_BaseColor*_BaseColor.a;

 o.color*=passCounts/totalCounts;

 o.color*=smoothstep(0.1,2,pivotDepth);// 在考虑个深度方向的蒙板，深度小于 0.1 时，完全不可见，深度大于 2 时，可见；

 return o;

 }

 half4 FRAG(v2f i):SV_TARGET

 {

 half4 tex=SAMPLE_TEXTURE2D(_MainTex,sampler_MainTex,i.texcoord);

 return tex*i.color;

 }

 ENDHLSL

 }

 }

}