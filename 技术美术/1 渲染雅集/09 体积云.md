学习了一段时间的 RayMarching 体积云算法，大概有了一些简单的成果，在这里做一个完整的实现记录，本文所使用的 Unity 版本为 _**Unity 2021.3 LTS**_，由于文章的篇幅过长，所以分上下两个章节。

## **1. 理解 RayMarching 体积云的原理**

在实现进行工程实践之前，我们先大概的了解一下 RayMarching 算法的基本原理。

### **1.1 一种巧妙的理解 RayMarching 算法的方法**

我想了很久关于怎么理解 **RayMarching**，也就是**光线步进算法**。经过了苦思冥想，我找到了一种很棒的理解方法。这个方法来源于《守墓人》主程序的技术分享 [[1 前言]](https://zhuanlan.zhihu.com/p/533853808#ref_1)。

[《守墓人》主程：如何用像素风做出真实的游戏世界_技术宅也爱玩游戏的博客 - CSDN 博客](https://blog.csdn.net/wubaohu1314/article/details/120359442)

其中提到了高度雾的做法，如下图所示：

![[a4668dc0f9ee2ea6b30d4d405ba0231c_MD5.png]]

它是将多层带透明度的迷雾贴图，按照一定的间隔排列起来，就形成了这样的效果。如下图所示：

![[63461d551004819f57f0331564ede496_MD5.png]]

所以我们也可以模拟一下这个做法。关于带有随机透明度的贴图，我们可以用柏林噪声实现。关于柏林噪声的实现方法，可以参考我的文章。

[王子饼干：游戏开发技术杂谈 5：柏林噪声 2](https://zhuanlan.zhihu.com/p/360235233)

文章中的代码支持生成一张 2D 的柏林噪声，如下图所示：

![[1b223dfe58be80b98a9f1060d10039d7_MD5.png]]

随后编写一个简单的 Shader，采样这张图的明度值，输出到透明度值，以此来渲染一个 Quad 模型。

![[e51718d3596a5706e4f86fe73eaad4f7_MD5.jpg]]

复制多个 Quad 模型，然后按照一定的距离排列。渲染的结果如下图所示：

![[f9c0bb6dc2708f8363ddcbe219a43db6_MD5.png]]

它好像形成了一定的体积雾的感觉，虽然只是伪装出来的效果而已，不过它正好解释了什么是 RayMarching 算法。先思考一下为什么会渲染出上面这个效果，上面的贴图都是带有一定的透明度的，当我们的视线穿过一张 Quad 时，如果穿过的部分有雾的部分（也就是透明度不为 0 的部分），浓度就会增加一点。正是这样的层层遮挡，最终才有了这样的效果。

也就是说，每当视线穿过一层雾的贴图时，都会累加一个点的雾的强度值，最后得到的总强度值就是我们看到的雾的浓度。

![[2528329589cfa7d438623a19e7650d58_MD5.png]]

这就是光线步进算法的基本思想。所以它是一种基于积分的方法，层级之间的间隔就是微分，每经过一个微分的距离的时候就叠加一个积分，最终得到的总积分就是雾的强度。

### **1.2 从原理到工程实践的一部分思考**

RayMarching 的原理可以说相当简单，但是工程实践的难度略大。所以我们可以先尝试提出一些问题，并围绕这些问题进行一定的思考。

*   如何发射一条射线？
*   朝着什么方向发射这条射线？
*   既然是体积雾 / 云渲染，总得有一块体积，这个体积是怎么表达的。
*   如何采样浓度，以及浓度如何产生差异？

### **1.3 如何发射一条射线**

RayMarching 并非真正的发射一条射线，因为它总是间隔一段距离（Quad 之间的距离）进行一次浓度采样（获取 Quad 表面的雾的强度值），所以它有点像是每隔一段距离叠加某一个值。这里面会产生几个参数信息。

首先是射线的发出点，一般是摄像机的位置。可以设为`entryPoint`，其次是步进的距离，这个可以调整的参数，可以设为`stepDst`，其次是发射的方向，这个暂时不清楚，可以简单记录为`rayDir`，还有就是要检测的总长度（毕竟不可能无限的检测下去），所以将射线的总长度记录为`totalDst`，有了这几个参数，我们可以写一些伪代码来表示发射一条射线的基本操作。

```
float raymarching(float3 entryPoint, float3 rayDir, float stepDst, float totalDst){
    // RayMarching伪代码实现

    float cloudDensity = 0;		// 总的雾浓度
    float3 currentPoint = entryPoint;	// 从射线的发出点开始检测
    int stepCount = totalDst / stepDst;	// 计算要迭代的步数
    for(int i = 0; i < stepCount; i ++){
        // 向dir方向行走stepDst个距离，得到一个新的点，并在新的点进行浓度的采样，
        // 至于如何采样可以暂时不用管。循环stepCount次

        currentPoint += rayDir * stepDst;
        cloudDensity += sampleDensity(currentPoint);
    }
    return cloudDensity;
}
```

其中`rayDir`和`sampleDensity`我们都可以暂时先忽略。

### **1.4 发射的方向**

这里面的几个参数，`entryPoint`就是摄像机的位置，`stepDst`和`totalDst`是人为设定的参数条件，所以唯一不清楚的就是`rayDir`到底是朝什么方向发射。

思考这个问题如果过于复杂的话，可以反过来思考，也就是只朝一个特定的方向发射一条射线会发射什么？

![[6e3f7732fd7c3fec5dd3f0d7e34f6b4c_MD5.png]]

其实仔细想一下就会发现，这根本不是方向的问题，而是如果只发射一条射线，你最终得到的只有一条线上的浓度，我们要的是整个体积内的浓度。所以显而易见的，我们需要发射一堆射线，并且是朝着每个可能的方向发射。

**那么这个可能的方向是什么意思呢？**

如果什么都不管的话，可以以一个球的中心为起点，朝着球面的每个点发射射线，这样就可以包含所有的方向了。不过很显然的，这里面有不少射线是我们不需要的。

从优化的角度来看，我们应该朝着可以被看见的方向发射射线。这个可以被看见的方向就是**屏幕空间（Screen Space）**，如下图所示。

![[4ed9a2c3eb2528ef480eba92d5e8ea7b_MD5.png]]

**这也是为什么体积雾它是一个后期处理特效，现在已经有了相机的世界坐标位置，理论上，如果我们可以找到屏幕上每个像素点的世界坐标位置，通过两个世界坐标相减后标准化，就是所需要的方向了**。朝着所有方向发射射线并采样浓度，最后合并到一起，就形成了我们的体积雾 / 云。

所以还需要知道如何从屏幕坐标计算得到世界坐标，这个知识点是一个相对底层的知识点。

### **1.5 体积是如何形成的**

首先体积这个玩意肯定不是写死在代码里的，而是由开发者手动指定的，它是一个范围，可能是球形，也有可能是长方体，但是显然，它肯定是坐落于世界坐标系中的某个范围。**不过重点不是范围，而是世界坐标系。**

在上述的伪代码中，有一行代码为`cloudDensity += sampleDensity(currentPoint);`，显然这里的`sampleDensity`是一个黑盒，但是不妨将这个黑盒稍微拨开一点。虽然不知道它是怎么采样浓度的，但是函数依赖的参数是一个世界坐标，既然它是根据世界坐标来采样的，那么它必然要处理这个坐标。

所以不妨先设每个点的浓度均为一个定值`density`，如果发射的射线铺满了整个空间，那么获取到的浓度几乎都是一样的，整个屏幕空间里就全是白色的，就像是一块白色的瓷砖遮住了相机一样。

在这个条件之下，我们肯定不希望所有的地方都有雾，相反，如果可以手动指定一块地方有雾岂不是更好。**假设手动指定了一个范围，恰好我们还有采样用的世界坐标。现在判断这个坐标是否处于这个范围中，如果是，则采样值为`density`，如果不是则返回 0，这样最终就会形成一块体积，而且这个体积是我们所指定的范围。**

所以为了能够计算一个 3D 的坐标是否处于一个范围中，还需要一种碰撞盒算法，不过这里可以暂时不考虑，只要知道有这个知识点即可。

### **1.6 浓度如何产生差异**

前面我们设每个点的浓度都是一个定值，这样形成的雾就像是一块砖一样，很均匀也很稳定。但是显然是不对的，也就是说，每个点的浓度是不一样的，这个时候还需要依赖一种 3D 噪声去处理它。这个知识点比较独立也比较简单，本文不会展开说。

## **2.URP 的后期处理脚本**

如果你不使用 URP 来编写后期处理脚本，或者你只是想了解 RarMarching 的算法，不进行任何工程实践，可以直接跳过该节。

_**本文使用的 Unity 版本为 2021.3 LTS 版本，理论上的最低兼容版本是 2019.4 LTS 版本，**_新建一个 URP 的工程，注意是 **3D(URP)** 模板。这个模板自动配置了 URP 的环境，不需要再手动配置了。

![[f5e0942cac433e596b7cfe876207bc66_MD5.png]]

### **2.1 URP 的后期处理脚本规范**

要在 URP 中增加一个后期处理特效，首先需要追加三个新的脚本，由于本文中这几个脚本不是重点，所以如果你希望尝试工程实践的话，直接复制我的脚本即可。三个脚本分别是

*   `CustomRenderFeature.cs`
*   `CustomRenderPass.cs`
*   `VolumetricCloud.cs`

每个脚本的功能和规范我单独说明

### **2.2 VolumetricCloud.cs**

这个脚本的功能很简单，就是在 Volume 追加一个自定义的后期处理特效。体积云特效需要的参数非常多，所以这个脚本还需要随着本文的进度一点点的追加内容，在这里我们就象征性的先追加一个色彩的参数即可。（直接创建一个`VolumetricCloud.cs`，然后复制所有的代码即可。）

```
using UnityEngine;
using UnityEngine.Rendering;
using UnityEngine.Rendering.Universal;

[System.Serializable]
[VolumeComponentMenuForRenderPipeline("Custom/VolumetricCloud", typeof(UniversalRenderPipeline))]
public class VolumetricCloud: VolumeComponent, IPostProcessComponent{

    [Tooltip("Base Color")]
    public ColorParameter baseColor = new ColorParameter(new Color(1, 1, 1, 1));

    public bool IsActive() => true;
    public bool IsTileCompatible() => false;
    public void load(Material material, ref RenderingData data){
        /* 将所有的参数载入目标材质 */

        material.SetColor("_BaseColor", baseColor.value);
    }
}
```

有了这个脚本之后，我们就可以在 Volume 菜单中找到这个特效选项了。

![[638054fa979d2d7a70f53b329ad06d7d_MD5.jpg]]

不过它还没有什么实际的作用，只是负责把参数传递给材质而已。

### **2.3 CustomRenderFeature.cs**

`CustomRenderFeature`可以在 URP 的渲染器中追加一个新的 **RenderFeature**，注意脚本名也必须是`CustomRenderFeature.cs`，在当前的这个案例中，创建的 RenderFeature 有两个参数，一个是该 RenderFeature 所用的 Shader，另外一个是`RenderPass`执行的时间。默认为`BeforeRenderingPostProcessing`，顾名思义就是在后期处理之前开始执行。

```
public class CustomRenderFeature: ScriptableRendererFeature{

    [SerializeField] private Shader shader;             // 手动指定该RenderFeature的所用到的Shader
    [SerializeField] private RenderPassEvent evt = RenderPassEvent.BeforeRenderingPostProcessing;
    private Material matInstance;                       // 创建一个该Shader的材质对象
    private CustomRenderPass pass;                      // RenderPass

    public override void AddRenderPasses(ScriptableRenderer renderer, ref RenderingData renderingData){

        if(shader == null)return;
        if(matInstance == null){
            matInstance = CoreUtils.CreateEngineMaterial(shader);
        }
        RenderTargetIdentifier currentRT = renderer.cameraColorTarget;
        pass.Setup(currentRT, matInstance);
        renderer.EnqueuePass(pass);
    }
    public override void Create(){
        
        pass = new CustomRenderPass();
        pass.renderPassEvent = evt;
    }
}
```

之后就可以在 URP 的渲染器中添加这个 RenderFeature 了。

![[bc4749ee8a5381fdfbb2446942bfe1e4_MD5.png]]

### **2.4 CustomRenderPass.cs**

CustomRenderPass 决定了如何渲染这个对象，这个对象除了被`CustomRenderFeature.cs`引用之外，它同时也将会获取`VolumetricCloud.cs`，将材质和渲染数据传递给它，这也是为什么`VolumetricCloud.cs`中有一个`load`函数。它就是负责将所有的参数传递到 Shader 里用的。

```
using UnityEngine;
using UnityEngine.Rendering;
using UnityEngine.Rendering.Universal;

public class CustomRenderPass: ScriptableRenderPass{

    const string customPassTag = "Custom Render Pass";
    private VolumetricCloud parameters;
    private Material mat;
    private RenderTargetIdentifier sourceRT;
    private RenderTargetHandle tempRT;

    public void Setup(RenderTargetIdentifier identifier, Material material){
        
        this.sourceRT = identifier;
        this.mat = material;
    }
    public override void Execute(ScriptableRenderContext ctx, ref RenderingData data){

        VolumeStack stack = VolumeManager.instance.stack;
        parameters = stack.GetComponent<VolumetricCloud>();
        CommandBuffer command = CommandBufferPool.Get(customPassTag);
        Render(command, ref data);
        ctx.ExecuteCommandBuffer(command);
        CommandBufferPool.Release(command);
        command.ReleaseTemporaryRT(tempRT.id);
    }
    public void Render(CommandBuffer command, ref RenderingData data){

        if(parameters.IsActive()){
            parameters.load(mat, ref data);
            RenderTextureDescriptor opaqueDesc = data.cameraData.cameraTargetDescriptor;
            opaqueDesc.depthBufferBits = 0;
            command.GetTemporaryRT(tempRT.id, opaqueDesc);
            command.Blit(sourceRT, tempRT.Identifier(), mat);
            command.Blit(tempRT.Identifier(), sourceRT);
        }
    }
}
```

### **2.2 一个后期处理 Shader 的模板**

有了以上的三个脚本之后，就可以开始写 Shader 的代码了，不过在此之前还是先验证一下是否可行，所以先写一个简单的后处理 Shader，让屏幕色彩与我们指定的参数混合后输出。

```
Shader "RedSaw/VolumetricCloud"{
    
    Properties{
        // 着色器输入
        _MainTex("Main Texture", 2D) = "white"{}
        _BaseColor("Base Color", Color) = (1, 1, 1, 1)
    }
    SubShader{
        Tags{
            "RenderPipeline" = "UniversalRenderPipeline"
        }
        pass{

            Cull Off
            ZTest Always
            ZWrite Off
            
            HLSLPROGRAM
 #include "Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl"
 #include "Packages/com.unity.render-pipelines.core/ShaderLibrary/SpaceTransforms.hlsl"
 #pragma vertex Vertex
 #pragma fragment Pixel

                Texture2D _MainTex;
                SamplerState sampler_MainTex;
                half4 _BaseColor;

                struct vertexInput{
                    float4 vertex: POSITION;
                    float2 uv: TEXCOORD0;
                };
                struct vertexOutput{
                    float4 pos: SV_POSITION;
                    float2 uv: TEXCOORD0;
                };

                vertexOutput Vertex(vertexInput v){

                    vertexOutput o;
                    o.pos = TransformObjectToHClip(v.vertex);
                    o.uv = v.uv;
                    return o;
                }
                half4 Pixel(vertexInput IN): SV_TARGET{

                    half4 col = _MainTex.Sample(sampler_MainTex, IN.uv);
                    return col * _BaseColor;
                }
            ENDHLSL
        }
    }
}
```

将这个 Shader 作为一个参数放到我们自定义的 RenderFeature 的参数槽中

![[e025600b32a0b903a0918d151214760b_MD5.png]]

然后在 Volume 中调整一下 VolumetricCloud 的色彩参数，它应该可以改变屏幕当前的色彩。

![[55e0588a8c27c87f1f20631187eeb286_MD5.png]]

看到这样的画面，这个后处理 Shader 就算是起效了。之后我们的主要代码都会在这个后处理 Shader 中进行。我的博客中的代码都是经过测试后才写出来的，如果是同版本，同管线的 Unity 复制代码后不可以运行，可以在评论区指正我的错误，感谢。

## **3.RayMarching 体积云实现**

### **3.1 重建世界坐标**

为了获取所有的射线方向，首先的目标就是重建世界坐标 [[2 基本认知和应用]](https://zhuanlan.zhihu.com/p/533853808#ref_2)。URP 如何重建世界坐标，可以参考 Unity 官方的文档。不过我还是会按照自己的思路重新走一遍。

[https://docs.unity3d.com/Packages/com.unity.render-pipelines.universal@11.0/manual/writing-shaders-urp-reconstruct-world-position.html](https://docs.unity3d.com/Packages/com.unity.render-pipelines.universal@11.0/manual/writing-shaders-urp-reconstruct-world-position.html)

我们已经知道了相机的位置，（即`_WorldSpaceCameraPos`），也知道屏幕的每个点坐标 (即 uv 坐标)。所以如果我们可以通过屏幕坐标来获取每个点的世界坐标`worldPos`，就可以得到射线的发射方向了 (`worldPos - _WorldSpaceCameraPos.xyz`)

虽然被渲染的场景是一个 3D 的场景，但是最终输出的结果是一个 2D 的屏幕，所以其实 Z 轴信息被隐藏了，这个被隐藏的 Z 轴信息最终变成了一个纹理信息，我们称其为**深度贴图**。深度贴图默认是关闭的，要启用深度贴图，首先需要在`Universal Render Pipeline Asset`中开启。

![[5fcf36adf5ec4d055a60c3fb31586108_MD5.png]]

开启后，只要在 Shader 中定义一个变量`_CameraDepthTexture`，Unity 就会把深度纹理填充到这个对象里，不过在 URP 管线里，可以通过引用一个库来代替声明这个变量，如下所示：

```
#include "Packages/com.unity.render-pipelines.universal/ShaderLibrary/DeclareDepthTexture.hlsl"
```

这个库还提供了一个函数`SampleSceneDepth`，提供 UV 就可以获取深度值了。有了深度值之后，通过下面的函数可以计算世界坐标。代码来源于 Unity 的官方文档。

```
float3 GetWorldPosition(float3 positionHCS){
    /* get world space position from clip position */

    float2 UV = positionHCS.xy / _ScaledScreenParams.xy;
    #if UNITY_REVERSED_Z
    real depth = SampleSceneDepth(UV);
    #else
    real depth = lerp(UNITY_NEAR_CLIP_VALUE, 1, SampleSceneDepth(UV));
    #endif
    return ComputeWorldSpacePosition(UV, depth, UNITY_MATRIX_I_VP);
}
```

代码比较直观，首先传入的参数是`positionHCS`，这个坐标就是在顶点着色器里通过`TransformObjectToHClip`得到的裁剪坐标，这个坐标除以`_ScaledScreenParams.xy`后会被映射为 NDC 坐标，组合屏幕 NDC 坐标与深度值就是完整的 NDC 坐标了，此后分别与逆投影矩阵和逆视角矩阵左乘就可以还原世界坐标了，这些都是关于渲染管线的知识点，如果不清楚可以参考我写的文章。或者可以参考 OpenGL 投影矩阵推导 [[3 PBR 基本原理和实现]](https://zhuanlan.zhihu.com/p/533853808#ref_3)。

[王子饼干：游戏开发技术杂谈 3：OpenGL 投影矩阵](https://zhuanlan.zhihu.com/p/115028420)

有了世界坐标之后，可以用世界坐标减去相机的世界坐标，得到一个方向向量，并正则化为一个方向。

```
half4 Pixel(vertexOutput IN): SV_TARGET{
    
    float3 worldPosition = GetWorldPosition(IN.pos);
    float3 rayDir = normalize(worldPosition - _WorldSpaceCameraPos.xyz);
    return half4(rayDir, 0);
}
```

这个方向就是我们要发射射线的方向，如果直接输出这个方向到色彩的话，我们应该得到下面的图像。注意该图像应该是 Game 窗口而非 Scene 窗口

![[c8fcedc9c97c74412c22892c9b862e77_MD5.png]]

### **3.2 RayMarching 算法的基本测试**

在有了射线方向之后，不妨先做一个简单的测试，我们朝着射线方向步进 128 次，每次步进距离为 0.5，对于任何一个点，如果它位于我们指定的方块内，则给浓度增加 0.02，之后用总的浓度和主纹理色彩叠加，应该可以得到一块雾。

首先为了避免重复计算，我们对现在的世界坐标代码做一个基本调整

```
half4 Pixel(vertexOutput IN): SV_TARGET{
    
    // 采样主纹理
    half4 albedo = _MainTex.Sample(sampler_MainTex, IN.uv);

    // 重建世界坐标
    float3 worldPosition = GetWorldPosition(IN.pos);
    float3 rayPosition = _WorldSpaceCameraPos.xyz;
    float3 worldViewVector = worldPosition - rayPosition;
    float3 rayDir = normalize(worldViewVector);
    
    // 基本的RayMarching测试..
    // ..
}
```

那么简单回顾一下步进需要的几个基本参数，首先是步进的起点`rayPosition`，然后是步进的步长`stepSize`，然后是步进的次数`stepCount`，最后是每次采样的浓度值，不过这里只是做简单测试，这个值我们固定设为 0.02 即可。

```
half4 Pixel(vertexOutput IN): SV_TARGET{

    // 采样主纹理
    half4 albedo = _MainTex.Sample(sampler_MainTex, IN.uv);

    // 重建世界坐标
    float3 worldPosition = GetWorldPosition(IN.pos);
    float3 rayPosition = _WorldSpaceCameraPos.xyz;
    float3 worldViewVector = worldPosition - rayPosition;
    float3 rayDir = normalize(worldViewVector);

    // 简单的RayMarching测试
    float3 stepVec = rayDir * 0.5;                      // 步进向量，它由长度值乘方向得到
    float3 currentPoint = rayPosition;                  // 要采样的世界坐标，初始位置为相机的位置
    float totalDensity = 0;                             // 总浓度
    for(int i = 0; i < 128; i ++){
        // 将测试点向前rayDir方向推移小一段距离
        currentPoint += stepVec;

        // 检测给定的点是否处于一个长宽高为10的正方体内部
        if(currentPoint.x > 0 && currentPoint.y > 0 && currentPoint.z > 0 &&
           currentPoint.x < 10 && currentPoint.y < 10 && currentPoint.z < 10)
            totalDensity += 0.02;
    }
    return albedo + totalDensity;
}
```

输出的结果如下图所示

![[ab8fcd75ab71c492ad217bde3352b404_MD5.png]]

这就是最基础的 RayMarching 步进算法的运用了。由于步长和步数的不同，它的细节程度会有所差异，下面分别是步长为 0.1，步数为 256 和步长为 1，步数为 64 的效果。

![[506bfacd08992cddde90d2dafaa7cfc5_MD5.png]]

![[d0c8a560d9a8536c7e3d2998c22f49b8_MD5.png]]

虽然基本实现了 RayMarching，不过这个小的案例还没有彻底解决问题。它还有一个核心的问题需要解决，就是遮挡关系。如果我们将镜头移动到 cube 的后面，会发现 cube 并没有挡住方块。

![[cee083873b549cc3f8c07077b4432609_MD5.png]]

回头去看一下代码会发现，代码中其实只有一个条件，这个条件就是当测试点在雾的内部，所以，即便物体把我们的雾挡住了，它仍然会进行采样，所以这是我们需要去解决的问题。

```
// 简单的RayMarching测试
float3 stepVec = rayDir * 0.3;                      // 步进向量，它由长度值乘方向得到
float3 currentPoint = rayPosition;                  // 要采样的世界坐标，初始位置为相机的位置
float totalDensity = 0;                             // 总浓度
for(int i = 0; i < 128; i ++){
    // 将测试点向前rayDir方向推移小一段距离
    currentPoint += stepVec;

    // 检测给定的点是否处于一个长宽高为10的正方体内部
    if(currentPoint.x > 0 && currentPoint.y > 0 && currentPoint.z > 0 &&
       currentPoint.x < 10 && currentPoint.y < 10 && currentPoint.z < 10)
        totalDensity += 0.02;
}
return albedo + totalDensity;
```

### **3.3 用射线包围盒算法解决遮挡问题**

那么这个问题其实已经有前人解决过了，有一种专门针对射线碰撞的算法，它是 Nvidia 公司研发的一种算法 [[01 光学原理]](https://zhuanlan.zhihu.com/p/533853808#ref_4)。对该算法有兴趣的可以通过下面的链接了解。

[https://jcgt.org/published/0007/03/04/](https://jcgt.org/published/0007/03/04/)

那么在这里，我们直接给出它的代码并解释其作用。具体的代码实现如下：

```
float2 rayBoxDst(float3 boundsMin, float3 boundsMax, float3 rayOrigin, float3 rayDir){
    /*  通过boundsMin和boundsMax锚定一个长方体包围盒
        从rayOrigin朝rayDir发射一条射线，计算从rayOrigin到包围盒表面的距离，以及射线在包围盒内部的距离
        关于更多该算法可以参考：https://jcgt.org/published/0007/03/04/ 
	*/

    float3 t0 = (boundsMin - rayOrigin) / rayDir;
    float3 t1 = (boundsMax - rayOrigin) / rayDir;
    float3 tmin = min(t0, t1);
    float3 tmax = max(t0, t1);

    float dstA = max(max(tmin.x, tmin.y), tmin.z);
    float dstB = min(tmax.x, min(tmax.y, tmax.z));

    float dstToBox = max(0, dstA);
    float dstInsideBox = max(0, dstB - dstToBox);
    return float2(dstToBox, dstInsideBox);
}
```

该函数通过两个 3D 坐标来锚定一个长方体的信息，传入发射起点和发射方向，可以计算得到两个值，一个值是`dstToBox`，另外一个是`dstInsideBox`，它们的关系如下图所示。其中黑色的方块代表体积云的实际体积，而红色的线条就是`dstToBox`，蓝色的线条则是`dstInsideBox`，所以这个里面，实际要采样的距离就只有蓝色的一段。

![[5d3e7d32e693c97f8e49d45fb16b065a_MD5.png]]

现在考虑向其中增加一个物体，如下图所示：

![[82b1ba1b513a8ce5659e0aeae833b282_MD5.png]]

其中紫色的部分就是物体，而绿色的线则是发射点到物体的距离，它可以直接通过两个世界坐标相减之后求模长获取，如下代码所示。

```
float dstToOpaque = length(worldPosition - _WorldSpaceCameraPos.xyz);
```

所以用`dstToOpaque`和`dstToBox`比较可知物体是否挡住雾，当`dstToOpaque`大于`dstToBox`时，物体被碰撞盒挡住，反之则是碰撞盒被物体挡住了。直观的来说，如果物体把碰撞盒挡住了，那么就完全没有必要采样了。而如果碰撞盒把物体挡住了，则需要比较`dstToOpaque - dstToBox`与`dstInsideBox`，较短的那个是我们要采样的距离。如下对比图所示：

![[5f3eed1da4e1c2b815df5809eae8a1e7_MD5.png]]

![[36967b26f1fd3f8bd337030d7dd9fc67_MD5.png]]

所以最终要步进的距离就是

```
float dstToOpaque = length(worldPosition - _WorldSpaceCameraPos.xyz);
float dstLimit = min(dstToOpaque - dstToBox, dstInsideBox);
```

和刚才还是有很大的不同的，所以之前的代码就不用管了，我们直接重写这段代码。不过开始之前还是回顾一下 RayMarching 的基本参数

*   **发射起点**  
    我们最一开始是从摄像机位置开始发射的，但其实完全没有必要，严格来说应该是从红色线段的末端，也就是从体积雾的入口点开始发射采样射线。所以发射起点有了一定的变化，如下所示：

```
float3 entryPoint = rayPos + rayDir * dstToBox;
```

*   **发射步长**  
    步长也不是乱设的，之前只是做简单的测试，但其实它应该是计算出来的，为了尽量的节省每次采样所耗费的性能，我们应该使得每次采样都是有意义的。那么采样的总距离应该是`dstInsideBox`，而步长则是总距离除以步数

```
float stepSize = dstInsideBox / stepCount;
```

*   **步数**  
    这个就是可以手动设置的一个参数了，步数越高则越细节，同时也更耗费性能。步数越低则效果越差，但速度也越快。
*   **浓度值**  
    当前这个案例中，浓度值依然是定值即可，但回顾刚才的测试，我们发现，步长越长，采样到的浓度好像越高，反之则越低。这是因为我们刚才是用了一个定值 0.02，但是积分的时候是必须同时把微分算进去的。微分是什么呢？其实就是步长`stepSize`。所以每次累计的浓度应该是下面代码所展示的，浓度改为 0.1 主要是因为 stepSize 本身就不大，所以不用再缩小了。

```
float density = 0.1 * stepSize;
```

对于这段代码，可以先构建需要的参数信息，最重要的就是碰撞信息了，我们还是将雾的体积暂时写死，后面可以将其作为一个参数开放给 Volume 去编辑。然后`dstToOpaque`就直接求模长即可，这也是为什么我们先计算了`worldViewVector`以备后续使用。

```
float2 rayBoxInfo = rayBoxDst(float3(-10, -10, -10), float3(10, 10, 10), rayPosition, rayDir);
float dstToBox = rayBoxInfo.x;
float dstInsideBox = rayBoxInfo.y;
float dstToOpaque = length(worldViewVector);
float dstLimit = min(dstToOpaque - dstToBox, dstInsideBox);
```

然后补充上面所说的所有 RayMarching 参数信息。

```
int stepCount = 32;                                     // 总步数
float3 entryPoint = rayPosition + rayDir * dstToBox;    // 采样起点
float stepSize = dstInsideBox / stepCount;              // 步长
float totalDensity = 0;                                 // 浓度积分
float dstTravelled = 0;					// 已经走过的距离，与dstLimit所比较可以中断当前的采样
```

然后就可以开始构建我们的采样过程了。如下代码所示。

```
for(int i = 0; i < stepCount; i ++){
    if(dstTravelled < dstLimit){
        totalDensity += stepSize * 0.1;
    }else{
        break;
    }
    dstTravelled += stepSize;
}
```

非常简单啊，由于碰撞实现计算好了，所以就剩采样了。如果`dstTravelled`还在`dstLimit`内的话，就继续采样，否则就中断。结果如下图所示。

![[7c8f50ed2c65c0786e3c76e68729b186_MD5.png]]

这样遮挡关系就是正确的了。

### **3.4 用噪声实现浓度差异**

现在这个雾还是一整块，很均匀，有点固体二氧化碳的意思。所以我们可以通过噪声来让它变得不那么均匀。所以我们需要 3D 噪声的手段，正好我之前研究过两种不同的 3D 噪声，一种是 3D 柏林噪声，一种是 3D 威利噪声。

[王子饼干：游戏开发技术杂谈 6：柏林噪声 3](https://zhuanlan.zhihu.com/p/360887339)[王子饼干：游戏开发技术杂谈 11: Worley 噪声](https://zhuanlan.zhihu.com/p/514422184)

具体的实现细节可以参考我的文章，或者也可以自己在 Github 上面搜一下 3DNoise，这里其实有一个不错的项目 [[5]](https://zhuanlan.zhihu.com/p/533853808#ref_5)。

[GitHub - mtwoodard/TextureGenerator: 3D and 2D Texture generation using the compute shaders within the Unity engine.](https://github.com/mtwoodard/TextureGenerator)

所谓 3D 噪声，就是把一堆连续变化的 2D 噪声贴图贴在一起吧，然后我们可以通过一个世界坐标在噪声中进行采样，采样得到的结果就可以作为浓度了。在此之前我们可以先给 Volume 脚本追加三个新的参数。

```
using UnityEngine;
using UnityEngine.Rendering;
using UnityEngine.Rendering.Universal;

[System.Serializable]
[VolumeComponentMenuForRenderPipeline("Custom/VolumetricCloud", typeof(UniversalRenderPipeline))]
public class VolumetricCloud: VolumeComponent, IPostProcessComponent{

    [Tooltip("Base Color")]
    public ColorParameter baseColor = new ColorParameter(new Color(1, 1, 1, 1));
    [Tooltip("Density Noise")]
    public Texture3DParameter densityNoise = new Texture3DParameter(null);
    [Tooltip("Density Noise Scale")]
    public Vector3Parameter densityNoiseScale = new Vector3Parameter(Vector3.one);
    [Tooltip("Density Noise Offset")]
    public Vector3Parameter densityNoiseOffset = new Vector3Parameter(Vector3.zero);

    public bool IsActive() => true;
    public bool IsTileCompatible() => false;
    public void load(Material material, ref RenderingData data){
        /* 将所有的参数载入目标材质 */

        material.SetColor("_BaseColor", baseColor.value);
        if(densityNoise != null){
            material.SetTexture("_DensityNoiseTex", densityNoise.value);
        }
        material.SetVector("_DensityNoiseScale", densityNoiseScale.value);
        material.SetVector("_DensityNoiseOffset", densityNoiseOffset.value);
    }
}
```

同样，Shader 中也要声明这三个变量以备填充。注意它是`sampler3D`类型的，至于为什么我暂时也不清楚，反正我用`Texture3D`就会有 bug，如果有谁知道可以告诉我~

```
Texture2D _MainTex;
SamplerState sampler_MainTex;
half4 _BaseColor;
sampler3D _DensityNoiseTex;
float3 _DensityNoise_Scale;
float3 _DensityNoise_Offset;
```

我用我的脚本生成了一个 3D 的威利噪声，看起来大概像下面这样。

![[f8ee719cd24621759d9b938eb2385826_MD5.png]]

之后我们把这块噪声塞入 Volume

![[ce75025f4528ad3b6adc5cffe566aecc_MD5.png]]

有了噪声之后，我们可以写一个简单的函数来采样浓度。

```
float sampleDensity(float3 position){
    /* sample density accordding to world position */
    
    float3 uvw = position * _DensityNoise_Scale + _DensityNoise_Offset;
    return tex3D(_DensityNoiseTex, uvw).r;
}
```

求`uvw`的过程就是`TRANSFORM_TEX`的过程，只不过它只适合 2D，有了偏移和缩放之后就可以通过这些参数简单的控制雾的外观了。不过我们之前的采样循环中并没有计算当前的采样点，就是单纯的循环累计一个定值。

```
for(int i = 0; i < stepCount; i ++){
    if(dstTravelled < dstLimit){
        totalDensity += stepSize * sampleDensity;
    }else{
        break;
    }
    dstTravelled += stepSize;
}
```

所以我们可以追加几行代码，用于将`currentPoint`向`rayDir`的方向慢慢推移。

```
int stepCount = 32;                                     // 总步数
float3 entryPoint = rayPosition + rayDir * dstToBox;    // 采样起点
float stepSize = dstInsideBox / stepCount;              // 步长
float3 stepVec = stepSize * rayDir;                     // 步长 * 方向
float totalDensity = 0;                                 // 浓度积分
float dstTravelled = 0;                                 // 已经走过的距离
float3 currentPoint = entryPoint;                       // 当前点
for(int i = 0; i < stepCount; i ++){
    if(dstTravelled < dstLimit){
        totalDensity += stepSize * sampleDensity(currentPoint);
    }else{
        break;
    }
    currentPoint += stepVec;
    dstTravelled += stepSize;
}
```

首先我们计算了一下步进向量，也就是步长乘方向，非常简单。然后追加了当前点`currentPoint`，每次循环后，它都会向前推移一点。最后就是采样的值由定值转为了`sampleDensity(currentPoint)`。结果如下图所示：

![[aa00e0d64c6ef47bf11890ef5573b35d_MD5.png]]

看着还行吧。

### **3.5 摩尔消光系数**

不过有一个特别要注意的东西就是，我们采样到的这块看起来像是雾的东西本质上是**浓度（density）**。浓度应该是一个影响外观的参数，而非最终的结果。所以之前所做的事情就只是将浓度给可视化了而已。所以接下来大概了解一下这个浓度是如何影响光照的。

摩尔消光系数是对溶液的能见度的一种描述手段，这个值越大，这个溶液就会显得很厚实，这个值越小，则溶液显得更加通透。摩尔消光系数就是溶液对光的吸收效果。这个玩意也称**摩尔吸光度** [[6]](https://zhuanlan.zhihu.com/p/533853808#ref_6)。

![[0fe0c0e68fad6e0026387be4cb95ca85_MD5.jpg]]

从图中可以看见，向下发射的光束，逐渐的被溶液吸收了。

消光系数近似的服从于**比尔朗博定律（Beer–Lambert‘s Law）**[[7]](https://zhuanlan.zhihu.com/p/533853808#ref_7)，它是光吸收的基本定律，适用于所有的电磁辐射 和所有的吸光物质，包括气体、固体、液体、分子、原子和离子 **在体积云的情况下，它可以用于基于光学厚度可靠地计算透射率。**

$$A = εcl$$

计算结果为吸光度 $A$ ，其中 $\epsilon$ 是消光系数，这个跟溶液的化学特性有关，有的溶液（比如水）比较透，有的就比较混浊（比如某些有机物），其中 $c$ 为溶液的浓度，而 $l$ 为当前光线行走的长度，光线行进的距离越长，则吸光度越大。

那么回头来看我们之前的计算出来的 totalDensity，这个其实是溶液的总的浓度，它应该应该对当前的画面有一个遮光的效果。totalDensity 越大，则能见度越低，反之则越高，如果 totalDensity 为 0，那么我们应该看不到任何雾。那么最合适描述它的就是 exp 函数了。当然，是反过来的。

$$f(x) = exp(-x)$$

![[8c6a00f77f4b0a7feaff98e083442b90_MD5.png]]

如果浓度为 0，则函数值为 1，如果浓度不断提高，则结果越来越趋向于 0，这个值很适合用于对当前的屏幕色彩进行遮挡。为了追加吸光度参数，我们可以在 Volume 中追加一个`float`类型的参数`_Absorption`，它表示吸光度。同样在 Shader 中追加一个 float 声明。这里就不贴代码了。

那么最终的 totalDensity 的计算如下所示

```
for(int i = 0; i < stepCount; i ++){
    if(dstTravelled < dstLimit){
        totalDensity += stepSize * sampleDensity(currentPoint);
    }else{
        break;
    }
    currentPoint += stepVec;
    dstTravelled += stepSize;
}
return albedo * exp(-totalDensity * _Absorption);
```

而最终的画面色彩通过`exp(-x * _Absorption)`函数来进行色彩的衰减。最终的画面效果如下图所示，此时我设消光度为 4.15

![[451e11f190326c426ff4a3768696c03c_MD5.png]]

我们可以调整一下这个参数为 1.26 和 6，并观察最终的结果。

![[5893a554ddbc260ea69454786129fcb6_MD5.png]]

![[136d6d00c0822be860713e27fcec2da0_MD5.png]]

这样我们就基本完成了消光度的部分了。

### **3.6 光照计算引导**

RayMarching 的光照是本文的最后一个部分了，这个看起来是比较困难的一个东西。不过我还是倾向于从一些比较简单的点入手，那么在接触 RayMarching 的光照计算之前，可以先了解一下在计算机图形学里，一个光照是怎么计算出来的。

我们应该都接触过**兰伯特定律（Lambert's Law）**[[8]](https://zhuanlan.zhihu.com/p/533853808#ref_8)，这个是用来计算漫反射的基本光学模型，在现代也被称为原始着色算法。

$$S = \max(0, \frac{L \cdot N}{\lvert L\rvert\lvert N\rvert}) CI$$

其中 $S$ 是最终表面的光照结果，而 $L$ 是光照到这个点的方向， $N$ 就是这个点的法线了。 $I$ 和 $C$ 是光照的强度与色彩。所以如果深入思考这个事情的话，你会发现这个公式里，与这个物体有关的东西就只有法线和物体的位置，调整物体的法线和位置才能引发光照的变化，除此之外就没有了。说白了，**要计算一个点的色彩，我们必须找到和这个点有关的所有跟光照有关的信息**。所以我们围绕一个给定的点，尝试去找到这个点所有的参数信息，并计算出一个光照的强度值。

### **3.7 RarMarching 光照计算原理**

我们会朝着每个可能的方向发射无数的射线，直到铺满整个屏幕。所以，体积云最终呈现的光照效果就是由每个方向的射线所返回的光照结果。**所以 RayMarching 的计算中，围绕的具体对象不再是一个物体表面的点，而是每个采样的方向。**这点非常重要！这也是单一物体的材质和后期处理特效的区别。

![[e0cb9497ed6c3142b750c3a7ba0f9e86_MD5.png]]

不过由于 GPU 并行处理的特性，只要关注一条方向上的射线是怎么处理的就行了，在开始之前不妨先回顾一下之前的浓度的计算。

![[a96c4767734c68ae8e0a82dcbf1a4c44_MD5.png]]

之前是创建了一个测试点`testPoint`，然后把这个测试点不断的向`rayDir`的方向推进一小段距离`stepSize`，然后得到的点再采样浓度，最后叠加起来。所以我们可以认为，这条射线的光照信息，与采样的每个点的某个值都有关系。最后的结果是每个点的值叠加所得到的。那么我们可以先列出这个积分公式的基本结构。

$$\int_{0}^{l}{L(x)dx}$$

它表示从雾的入口点开始，要积分的距离长度为 $l$ ，也就是我们计算出来的`dstLimit`， $L(x)$ 是每个采样点的光照计算， $dx$ 则是距离的微分，也就是`stepSize`。

重点就是窥探一下 $L(x)$ 函数，也就是关于某个采样点的光照计算，它的内部有些什么。或者换个说法，它与哪些东西有关。

**光照信息 1. 采样点的浓度**

和一瓶溶液不太一样，体积雾和体积云并不是一个均匀的体系，它是弥漫着的，也就是它的浓度非常的不均匀。对于这样的一个对象来说，它每个点的浓度都是有差异的，有的地方浓度值很高，所以不透光，而有的时候浓度相对低，所以更透光一些，**而有一些地方浓度为 0，也就是没有浓度，或者说，这块区域是空的**。如果一束光射向一朵云，对于这样空白的区域，它肯定会直接穿过去，而不是照到一个什么实体上，所以浓度高的地方，它更像是一个实体，更能够作为这束光的载体。而浓度低甚至没有浓度的地方，无法作为光的载体。

这个点同样也有点类似于**丁达尔效应（Tyndall effect）**，对于丁达尔效应来说，它能够出现是因为空气中弥漫着微小的颗粒，这些颗粒就作为了光的载体，如果空气中没有这些小颗粒，显然就很难出现丁达尔效应了。这些弥漫着微小颗粒的地方，可以视为浓度高的地方，而光可以直接穿过去的地方，则是浓度低的地方。

所以一个采样点的光照强度，与该点的浓度 $D_{x}$ 有直接关系。

$$\int_{0}^{l}{D_{x}L(x)dx}$$

**光照信息 2. 采样点的接收到光线强度**

对于任何一个采样点来说，它的光照强度最直观的就是与该点的所接收到的光线的量有关系。如下图所示

![[8f7d7ab1ce76d04e6920cfb114b9fb82_MD5.png]]

对于打过来的一束光，最上面的部分就会更亮，而下面的部分所接收到的光就会少很多。也就是说，这点的光照与该点到光源的距离有关，这么说只对了一半，**因为别忘了体积云是一个不均匀的体系，一束光打过来，除了跟距离有关，更多的还与光路上的浓度有关**。那么怎么计算一个点到目标的总浓度已经很清楚了，依然是靠 RayMarching 算法，也就是要从该点到光源做一次 RayMarching 的步进积分。

![[36d3da1c262bf54b98e1ea263d33ef57_MD5.png]]

不过显然，这只是一个采样点到光源的浓度，每个点都得采样一次，这也是为什么 RayMarching 体积雾的性能消耗比较大，只是这次的积分不用步进太多次。它的公式大概如下所示：

$$I_{x} = \exp(-\int_{0}^{l_{x}}{D_{y}}dy)$$

比较好理解，首先可以把这个公式简化为：

$$I_{x} = exp(-R)$$

其中 $R$ 就是从采样点到光源方向的总浓度，如果浓度为 0，表示此处是空的，则目标点接收到的光线值就是满值 1，如果浓度是一个比较大的值，说明这束光已经穿过层层障碍，最终的光线值就是一个很小的值了，内部就是具体的积分公式了。

$$R = \int_{0}^{l_{x}}{D_{y}dy}$$

其中积分的距离是 $l_{x}$ ，也就是光源从体积雾的入口点到采样点 $x$ 的距离， $D_{y}$ 就是这条光路上每个点的浓度，也就是主要的积分值， $dy$ 则是微分，也就是每次光线积分步进的距离。由于每个采样点都需要做一次这样的积分，所以采样的过程如下图所示

![[798f8f4a634d98ef181a24d72c930bdd_MD5.png]]

这就是第二个与采样点 $x$ 有关的重要参数了，据此我们可以改写我们的光照公式。

$$S = \int_{0}^{l}{D_{x}\exp(-\int_{0}^{l_{x}}{D_{y}dy}) L(x)dx}$$

这样一来，留在 $L(x)$ 函数里的东西就不多了，还有东西，但东西不多。

**光照信息 3. 视角方向对光的吸收**

前面已经大概知道了怎么样采样每个采样点 $x$ 接收到的光照强度 $I_{x}$ 了，我们可以将它视为一个小的光源，它接收到了主光源的能量所以发光了，但是它要回传到人的眼睛里还得再次经过云层，如下图所示：

![[394df25f3b2b60256f0e5fdb1140796d_MD5.png]]

所以这个点的光还会继续受到一次云层的吸收，这个吸收与该点到观察点的浓度有关，也就是图中的红色线段的部分，**这个部分的浓度就是在进行光线步进时的当前总浓度。**所以不需要重新计算了，有了这个当前总浓度，就用它来给 $I_{x}$ 做一次衰减就可以了，由于这个当前总浓度是一个相对特殊的值，我将其记为 $N_{x}$ ，表示视角方向上采样点 $x$ 点的临时总浓度。

据此我们可以写出 $L(x)$ 函数的最后部分了。其中 $C$ 为摩尔消光系数。

$$S = \int_{0}^{l}{D_{x}\exp(-\int_{0}^{l_{x}}{D_{y}dy})\exp(-N_{x}C) dx}$$

然后可以稍微合并一下 exp 函数的里的内容。

$$S = \int_{0}^{l}{D_{x}\exp(-[\int_{0}^{l_{x}}{D_{y}dy} + N_{x}C]) dx}$$

这个公式其实还没有列完，不过它已经可以解释基本原理了，剩下的部分等基础的代码实践完成后再补充。

**3.8 基础代码实现**

_由于在编写博客的过程中不断的对代码进行重写和调试，原有的结构可能被打乱了，如果此时追加新的代码可能与之前的代码会产生冲突，为了读者能够直接复制代码并得到效果，在这里把前一步的片元着色器的代码重新贴出来，也就是到摩尔消光系数的那块。当然，这次的代码我也上传到了 GitHub，可以在文章的末尾找到。_

```
half4 Pixel(vertexOutput IN): SV_TARGET{
                    
    // 采样主纹理
    half4 albedo = _MainTex.Sample(sampler_MainTex, IN.uv);

    // 重建世界坐标
    float3 worldPosition = GetWorldPosition(IN.pos);
    float3 rayPosition = _WorldSpaceCameraPos.xyz;
    float3 worldViewVector = worldPosition - rayPosition;
    float3 rayDir = normalize(worldViewVector);

    // 碰撞体积计算
    float2 rayBoxInfo = rayBoxDst(float3(-10, -10, -10), float3(10, 10, 10), rayPosition, rayDir);
    float dstToBox = rayBoxInfo.x;
    float dstInsideBox = rayBoxInfo.y;
    float dstToOpaque = length(worldViewVector);
    float dstLimit = min(dstToOpaque - dstToBox, dstInsideBox);

    // 浓度采样
    int stepCount = 32;                                         // 采样的次数
    float stepSize = dstInsideBox / stepCount;                  // 步进的长度
    float3 stepVec = rayDir * stepSize;                         // 步进向量
    float3 currentPoint = rayPosition + dstToBox * rayDir;      // 采样起点
    float totalDensity = 0;
    float dstTravelled = 0;
    for(int i = 0; i < stepCount; i ++){
        if(dstTravelled < dstLimit){
            totalDensity += stepSize * sampleDensity(currentPoint);
            currentPoint += stepVec;
            dstTravelled += stepSize;
            continue;
        }
        break;
    }
    return albedo * exp(-totalDensity * _Absorption);
}
```

和之前的代码应该也差不多，不过为了保险还是重新贴一下，它的结果是下面这样的。

![[e5c92d7c202b7ade25d3a2084ee99ebe_MD5.png]]

那么回归正题，追加光照的第一步是编写一个采样光照强度的函数，也就是从采样点朝着主光源发射一条射线，并按照 RayMarching 的算法来进行这条光路上的云的浓度，然后用这个浓度来对光照强度进行衰减。注意，我们给出的体积云的范围还是写死的，后面这个部分需要调整为一个参数。

```
float lightPathDensity(float3 position, int stepCount){
    /* sample density from given point to light 
       within target step count */

    // URP的主光源位置的定义名字换了一下
    float3 dirToLight = _MainLightPosition.xyz;

    /* 这里的给传入的方向反向了一下是因为，rayBoxDst的计算是要从
       目标点到体积，而采样时，则是反过来，从position出发到主光源*/
    float dstInsideBox = rayBoxDst(float3(-10, -10, -10), float3(10, 10, 10), position, 1/dirToLight).y;

    // 采样
    float stepSize = dstInsideBox / stepCount;
    float totalDensity = 0;
    float3 stepVec = dirToLight * stepSize;
    for(int i = 0; i < stepCount; i ++){
        position += stepVec;
        totalDensity += max(0, sampleDensity(position) * stepSize);
    }
    return totalDensity;
}
```

该函数可以对给定的点朝主光源发射一条射线并采样这条光路上的浓度。对照之前的公式

$$S = \int_{0}^{l}{D_{x}\exp(-[\int_{0}^{l_{x}}{D_{y}dy} + N_{x}C]) dx}$$

由于 $D_{x}$ 是一个通用的部分，可以先将其计算出来并乘以微分`stepSize`

```
int stepCount = 32;                                         // 采样的次数
float stepSize = dstInsideBox / stepCount;                  // 步进的长度
float3 stepVec = rayDir * stepSize;                         // 步进向量
float3 currentPoint = rayPosition + dstToBox * rayDir;      // 采样起点
float totalDensity = 0;
float dstTravelled = 0;
for(int i = 0; i < stepCount; i ++){
    if(dstTravelled < dstLimit){
        float Dx = sampleDensity(currentPoint) * stepSize;
        totalDensity += Dx;

        currentPoint += stepVec;
        dstTravelled += stepSize;
        continue;
    }
    break;
}
```

有了`lightPathDensity`函数之后，可以直接通过当前点来计算得到光路的总的浓度了。

```
float Dx = sampleDensity(currentPoint) * stepSize;
totalDensity += Dx;
float pathDensity = lightPathDensity(currentPoint, 8);		// 步进默认为8次
```

之后可以叠加公式中的 $N_{x}C$ 项，然后通过 $exp(-x)$ 衰减后与 $D_{x}$ 项相乘得到这个点的光照强度积分。所以除了`totalDensity`，还需要一个新的积分结果，`lightIntensity`。

```
float lightIntensity = 0;
```

然后把上面说的写到代码中

```
float Dx = sampleDensity(currentPoint) * stepSize;
totalDensity += Dx;
float lightPathDensity = lightMarching(currentPoint, 8);
lightIntensity += exp(-(lightPathDensity + totalDensity * _Absorption)) * Dx;
```

对输出的部分，如下所示。

```
float3 cloudColor = _MainLightColor.xyz * lightIntensity * _BaseColor.xyz;
return half4(albedo * exp(-totalDensity * _Absorption) + cloudColor, 1);
```

将`lightIntensity`混合到主光源色彩与参数色彩`_BaseColor`，然后叠加到之前的黑色部分即可。结果如下所示：

![[a751ba582d24675eca056b9200b03dd7_MD5.png]]

这个就是 RayMarching 体积云的基本实现了，不过还没有结束，我们需要追加控制参数。

**3.9 追加控制参数**

需要追加的参数还有一些，比如体积雾的范围，采样的次数等，不过这个比较简单，这里就不展开说了。主要了解的参数是针对光照控制的。

在`lightPathDensity`函数中，在完成了光照的总体采样之后，我们直接丢入了 $exp(-x)$ 函数中，显然，我们可以追加一个针对光照的摩尔吸光度参数 $C_{L}$ ，据此可以调整之前的公式如下：

$$S = \int_{0}^{l}{D_{x}\exp(-[\int_{0}^{l_{x}}{D_{y}dy}C_{L} + N_{x}C]) dx}$$

追加参数的 cs 脚本就不贴了，这里贴一下光照强度的计算。

```
lightIntensity += exp(-(lightPathDensity * _LightAbsorption + totalDensity * _Absorption)) * Dx;
```

之后可以通过这个参数来控制光在云层中的消光度，如下图所示。

![[d4062f450f27f47fbde557ff4b28cdc7_MD5.gif]]

除了光照的消光度，还可以追加一个参数用于控制`lightIntensity`的大小，这个比较简单，就是在上述公式外部添加一个乘子 $P$ ，但是效果比较好。先把公式列一下。

$$S = P\int_{0}^{l}{D_{x}\exp(-[\int_{0}^{l_{x}}{D_{y}dy}C_{L} + N_{x}C]) dx}$$

然后在 Volume 中追加一个参数`lightPower`来控制它即可，别忘了在 Shader 中也加一个参数`_LightPower`。

```
float3 cloudColor = _MainLightColor.xyz * lightIntensity * _BaseColor.xyz * _LightPower;
```

![[aebbd389541f0f84d2dd44a547175ccf_MD5.gif]]

下面是我调整的最终效果

![[a3395bf20502f591f285f603092f8aa4_MD5.png]]

除了这两个参数其实还可以追加更多的参数，另外就是类似于光照采样次数，浓度采样次数，体积云的范围，都可以设为参数，只是这些比较简单，就没有写。

## **4. 总结**

RayMarching 体积雾是一个相对比较有难度的知识点，由于内容非常多，所以分了上下两篇，光照上还有一个重要的部分没有解决，就是散射的部分，这里面涉及到米氏散射和瑞利散射的知识点。除了散射还有云的流动效果，虽然这个案例中，用脚本控制偏移也可以让它滚动起来，但是效果还非常初级。

我也看了非常多别人的文章，但是文中的公式按照自己的想法构建的，没想到它真的可以渲染出效果。但是其中肯定还有很多纰漏和问题，如果发现问题，欢迎在评论区指正，对于我的错误，希望大家多多包涵~

这些内容我们都在下篇中解决，这次的代码已经上传到了 GitHub，其中包含了三个脚本，一个 Shader，一块无缝的威利噪声。如果这篇文章对你有帮助的话，可以给我的项目贡献一个⭐，感谢！

[UnityShaderMagicBook/Shader 魔法学笔记 / Shader 魔法学笔记 1.RayMarching 体积云 URP 实现. 上 at master · 529324416/UnityShaderMagicBook](https://github.com/529324416/UnityShaderMagicBook/tree/master/Shader%E9%AD%94%E6%B3%95%E5%AD%A6%E7%AC%94%E8%AE%B0/Shader%E9%AD%94%E6%B3%95%E5%AD%A6%E7%AC%94%E8%AE%B01.RayMarching%E4%BD%93%E7%A7%AF%E4%BA%91URP%E5%AE%9E%E7%8E%B0.%E4%B8%8A)

## 参考

1.  [^](https://zhuanlan.zhihu.com/p/533853808#ref_1_0) 守墓人主程序技术分享 [https://blog.csdn.net/wubaohu1314/article/details/120359442](https://blog.csdn.net/wubaohu1314/article/details/120359442)
2.  [^](https://zhuanlan.zhihu.com/p/533853808#ref_2_0)Unity 文档：重建世界坐标 [https://docs.unity3d.com/Packages/com.unity.render-pipelines.universal@11.0/manual/writing-shaders-urp-reconstruct-world-position.html](https://docs.unity3d.com/Packages/com.unity.render-pipelines.universal@11.0/manual/writing-shaders-urp-reconstruct-world-position.html)
3.  [^](https://zhuanlan.zhihu.com/p/533853808#ref_3_0)OpenGL 投影矩阵推导 [http://www.songho.ca/opengl/gl_projectionmatrix.html](http://www.songho.ca/opengl/gl_projectionmatrix.html)
4.  [^](https://zhuanlan.zhihu.com/p/533853808#ref_4_0)RayBox 碰撞盒算法 [https://jcgt.org/published/0007/03/04/](https://jcgt.org/published/0007/03/04/)
5.  [^](https://zhuanlan.zhihu.com/p/533853808#ref_5_0)3D 噪声生成器 [https://github.com/mtwoodard/TextureGenerator](https://github.com/mtwoodard/TextureGenerator)
6.  [^](https://zhuanlan.zhihu.com/p/533853808#ref_6_0) 摩尔消光度 [https://zh.wikipedia.org/zh-cn/%E8%8E%AB%E8%80%B3%E5%90%B8%E5%85%89%E5%BA%A6](https://zh.wikipedia.org/zh-cn/莫耳吸光度)
7.  [^](https://zhuanlan.zhihu.com/p/533853808#ref_7_0) 比尔朗伯定律 [https://en.wikipedia.org/wiki/Beer%E2%80%93Lambert_law](https://en.wikipedia.org/wiki/Beer–Lambert_law)
8.  [^](https://zhuanlan.zhihu.com/p/533853808#ref_8_0) 兰伯特光照模型 [https://en.wikipedia.org/wiki/Lambertian_reflectance](https://en.wikipedia.org/wiki/Lambertian_reflectance)