![](https://img-blog.csdnimg.cn/6089daa72d9c4ff8902d275b114284d2.jpeg)

ğŸŠé¡¶ä¼šçš„ä»£ç å¹²å‡€åˆ©ç´¢ï¼Œå€Ÿé‰´å…¶å®Œæˆäº†ä»¥ä¸‹å·¥ç¨‹

ğŸŠæœ¬å·¥ç¨‹é‡‡ç”¨ [Pytorch æ¡†æ¶](https://so.csdn.net/so/search?q=Pytorch%E6%A1%86%E6%9E%B6&spm=1001.2101.3001.7020)ï¼Œä½¿ç”¨ä¸Šæ¸¸è¯­è¨€æ¨¡å‹ + ä¸‹æ¸¸ç½‘ç»œæ¨¡å‹çš„ç»“æ„å®ç° IMDB æƒ…æ„Ÿåˆ†æ

ğŸŠé¢„è®­ç»ƒå¤§[è¯­è¨€æ¨¡å‹](https://so.csdn.net/so/search?q=%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B&spm=1001.2101.3001.7020)å¯é€‰æ‹© Bertã€Roberta

ğŸŠä¸‹æ¸¸ç½‘ç»œæ¨¡å‹å¯é€‰æ‹© BiLSTMã€LSTMã€TextCNNã€GRUã€Attention ä»¥åŠå…¶ç»„åˆ

ğŸŠè¯­è¨€æ¨¡å‹å’Œç½‘ç»œæ¨¡å‹æ‰©å±•æ€§è¾ƒå¥½ï¼Œå¯ä»¥æ­¤ä¸º BaseLine å†ä½¿ç”¨ä½ çš„æ•°æ®é›†ï¼Œæ¨¡å‹

ğŸŠæœ€ç»ˆçš„å‡†ç¡®ç‡å‡åœ¨ 90% ä»¥ä¸Š

ğŸŠé¡¹ç›®å·²å¼€æºï¼Œclone ä¸‹æ¥å†é…ä¸ªç®€å•ç¯å¢ƒå°±èƒ½è·‘

ğŸ¥³ğŸ¥³ğŸ¥³æœ‰å°ä¼™ä¼´è¯¢é—®å¦‚ä½•èåˆä½¿ç”¨ Attentionã€LSTM+TextCNN å’Œ Lstm+TextCNN+Self-Attention çš„ç½‘ç»œæ¨¡å‹ï¼Œç°æºç å·²ç»é‡æ–°ä¸Šä¼ ï¼ˆ2023-03ï¼‰ï¼Œå¤§å®¶å¯ä»¥æ£æ‘©ä¸€ä¸‹æ˜¯å¦‚ä½•ç»“åˆçš„ï¼Œå¦‚æ­¤ï¼Œå¯¹ç…§ç±»ä¼¼çš„åšæ³•ï¼Œæ¨å¹¿åˆ°å…¶ä»–æ¨¡å‹ä¸Š

å¦‚æœè¿™ç¯‡æ–‡ç« å¯¹æ‚¨æœ‰å¸®åŠ©ï¼ŒæœŸå¾…å¤§ä½¬ä»¬ Github ä¸Šç»™ä¸ªâ­ï¸â­ï¸â­ï¸

## ä¸€ã€Introduction

### 1.1 ç½‘ç»œæ¶æ„å›¾

è¯¥ç½‘ç»œä¸»è¦ä½¿ç”¨ä¸Šæ¸¸é¢„è®­ç»ƒæ¨¡å‹ + ä¸‹æ¸¸æƒ…æ„Ÿåˆ†ç±»æ¨¡å‹ç»„æˆ

![](https://img-blog.csdnimg.cn/00c5b759508d4228bf299519c49b563b.png)

### 1.2Â å¿«é€Ÿä½¿ç”¨

è¯¥é¡¹ç›®å·²å¼€æºåœ¨ Github ä¸Šï¼Œåœ°å€ä¸º [sentiment_analysis_Imdb](https://github.com/BeiCunNan/sentiment_analysis_Imdb "sentiment_analysis_Imdb")

![](https://img-blog.csdnimg.cn/direct/aa2cc3cc58f44fd28fd7f34adb01f2bc.png)

ä¸»è¦ç¯å¢ƒè¦æ±‚å¦‚ä¸‹ï¼ˆç¯å¢ƒä¸è¦å¤ªè€åŸºæœ¬æ²¡å•¥é—®é¢˜çš„ï¼‰

![](https://img-blog.csdnimg.cn/e57e37bc6ee54bd2b30038312977d74e.png)

ä¸‹è½½è¯¥é¡¹ç›®åï¼Œé…ç½®ç›¸å¯¹åº”çš„ç¯å¢ƒï¼Œåœ¨ config.py æ–‡ä»¶ä¸­é€‰æ‹©æ‰€éœ€çš„è¯­è¨€æ¨¡å‹å’Œç¥ç»ç½‘ç»œæ¨¡å‹å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œè¿è¡Œ main.py æ–‡ä»¶å³å¯

![](https://img-blog.csdnimg.cn/3384f41ee3554f2ca42b96cd520355af.png)

### Â 1.3Â å·¥ç¨‹ç»“æ„

![](https://img-blog.csdnimg.cn/b8b25641c41c47669a58f06890ab6888.png)

*   logsÂ  æ¯æ¬¡è¿è¡Œç¨‹åºåçš„æ—¥å¿—æ–‡ä»¶é›†åˆ
*   config.py å…¨å±€é…ç½®æ–‡ä»¶
*   data.py æ•°æ®è¯»å–ã€æ•°æ®æ¸…æ´—ã€æ•°æ®æ ¼å¼è½¬æ¢ã€åˆ¶ä½œ DataSet å’Œ DataLoader
*   main.py ä¸»å‡½æ•°ï¼Œè´Ÿè´£å…¨æµç¨‹é¡¹ç›®è¿è¡Œï¼ŒåŒ…æ‹¬è¯­è¨€æ¨¡å‹çš„è½¬æ¢ï¼Œæ¨¡å‹çš„è®­ç»ƒå’Œæµ‹è¯•
*   model.py ç¥ç»ç½‘ç»œæ¨¡å‹çš„è®¾è®¡å’Œè¯»å–

## äºŒã€Config

çœ‹äº†å¾ˆå¤šè®ºæ–‡æºä»£ç ä¸­éƒ½ä½¿ç”¨ parser å®¹å™¨è¿›è¡Œå…¨å±€å˜é‡çš„é…ç½®ï¼Œå› æ­¤ä½œè€…ä¹Ÿç…§è‘«èŠ¦ç”»ç“¢ç¼–å†™äº† config.py æ–‡ä»¶ï¼ˆé€‚é…çš„è¯ä¸€èˆ¬åªæ”¹ Base éƒ¨åˆ†ï¼‰

```python
import argparse
import logging
import os
import random
import sys
import time
from datetime import datetime
 
import torch
 
 
def get_config():
    parser = argparse.ArgumentParser()
    '''Base'''
 
    parser.add_argument('--num_classes', type=int, default=2)
    parser.add_argument('--model_name', type=str, default='bert',
                        choices=['bert', 'roberta'])
    parser.add_argument('--method_name', type=str, default='fnn',
                        choices=['gru', 'rnn', 'bilstm', 'lstm', 'fnn', 'textcnn', 'attention', 'lstm+textcnn',
                                 'lstm_textcnn_attention'])
 
    '''Optimization'''
    parser.add_argument('--train_batch_size', type=int, default=4)
    parser.add_argument('--test_batch_size', type=int, default=16)
    parser.add_argument('--num_epoch', type=int, default=50)
    parser.add_argument('--lr', type=float, default=1e-5)
    parser.add_argument('--weight_decay', type=float, default=0.01)
 
    '''Environment'''
    parser.add_argument('--device', type=str, default='cpu')
    parser.add_argument('--backend', default=False, action='store_true')
    parser.add_argument('--workers', type=int, default=0)
    parser.add_argument('--timestamp', type=int, default='{:.0f}{:03}'.format(time.time(), random.randint(0, 999)))
 
    args = parser.parse_args()
    args.device = torch.device(args.device)
 
    '''logger'''
    args.log_name = '{}_{}_{}.log'.format(args.model_name, args.method_name,
                                          datetime.now().strftime('%Y-%m-%d_%H-%M-%S')[2:])
    if not os.path.exists('logs'):
        os.mkdir('logs')
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)
    logger.addHandler(logging.StreamHandler(sys.stdout))
    logger.addHandler(logging.FileHandler(os.path.join('logs', args.log_name)))
    return args, logger
```

## ä¸‰ã€Data

### 3.1 æ•°æ®å‡†å¤‡

é¦–å…ˆéœ€è¦ä¸‹è½½ IMDB æ•°æ®é›†ï¼Œå¹¶å¯¹å…¶è¿›è¡Œåˆæ­¥å¤„ç†ï¼Œå…¶å¤„ç†è¿‡ç¨‹å¯å‚è€ƒä¸€ä¸‹æ–‡ç«  [IMDB æ•°æ®é¢„å¤„ç†](https://beicunnan.blog.csdn.net/article/details/127196715?spm=1001.2014.3001.5502 "IMDBæ•°æ®é¢„å¤„ç†")

ä¹Ÿå¯ä»¥ç›´æ¥ä» Github ä¸Šè·å–å·²å¤„ç†å¥½çš„æ•°æ®é›†ï¼Œå¤„ç†å¥½çš„æ•°æ®æ ¼å¼å¦‚ä¸‹

![](https://img-blog.csdnimg.cn/7cdf7726e2174c7e8ae1bde93a7df380.png)

### 3.2 æ•°æ®é¢„å¤„ç†

ç”±äº IMDB æ•°æ®é‡éå¸¸åºå¤§ï¼Œä½¿ç”¨å…¨æ•°æ®çš„è®­ç»ƒæ—¶é—´éå¸¸é•¿ï¼ˆç®—åŠ›å¥½çš„å°ä¼™ä¼´å¯å¿½ç•¥ï¼‰ï¼Œå› æ­¤è¿™é‡Œä½¿ç”¨ 10% çš„æ•°æ®é‡è¿›è¡Œè®­ç»ƒ

```python
data = pd.read_csv('datasets.csv', sep=None, header=0, encoding='utf-8', engine='python')
    len1 = int(len(list(data['labels'])) * 0.1)
    labels = list(data['labels'])[0:len1]
    sentences = list(data['sentences'])[0:len1]
    # split train_set and test_set
    tr_sen, te_sen, tr_lab, te_lab = train_test_split(sentences, labels, train_size=0.8)
```

### 3.3 åˆ¶ä½œ DataSet

åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¹‹åå°±å¯ä»¥åˆ¶ä½œè‡ªå·±çš„ DataSet

```python
# Dataset
    train_set = MyDataset(tr_sen, tr_lab, method_name, model_name)
    test_set = MyDataset(te_sen, te_lab, method_name, model_name)
```

MyDataset çš„ç»“æ„å¦‚ä¸‹

*   ä½¿ç”¨ split æ–¹æ³•å°†æ¯ä¸ªå•è¯æå–å‡ºæ¥ä½œä¸ºåç»­ bertToken çš„è¾“å…¥
*   åç»­åˆ¶ä½œ DataLoader éœ€è¦ä½¿ç”¨ collate_fn å‡½æ•°å› æ­¤éœ€è¦é‡å†™__getitem__æ–¹æ³•

```python
class MyDataset(Dataset):
    def __init__(self, sentences, labels, method_name, model_name):
        self.sentences = sentences
        self.labels = labels
        self.method_name = method_name
        self.model_name = model_name
        dataset = list()
        index = 0
        for data in sentences:
            tokens = data.split(' ')
            labels_id = labels[index]
            index += 1
            dataset.append((tokens, labels_id))
        self._dataset = dataset
 
    def __getitem__(self, index):
        return self._dataset[index]
 
    def __len__(self):
        return len(self.sentences)
```

### 3.4 åˆ¶ä½œ DataLoader

å¾—åˆ° DataSet ä¹‹åå°±å¯ä»¥åˆ¶ä½œ DataLoader äº†

*   é¦–å…ˆéœ€è¦ç¼–å†™ my_collate å‡½æ•°ï¼Œè¯¥å‡½æ•°çš„åŠŸèƒ½æ˜¯å¯¹æ¯ä¸€ä¸ª batch çš„æ•°æ®è¿›è¡Œå¤„ç†
*   åœ¨è¿™é‡Œçš„æ•°æ®å¤„ç†æ˜¯å°†æ–‡æœ¬æ•°æ®è¿›è¡Œ Tokenizer åŒ–ä½œä¸ºåç»­ Bert æ¨¡å‹çš„è¾“å…¥
*   é€šè¿‡è®¡ç®—å¯å¾—çŸ¥ 80% å¥å­çš„é•¿åº¦ä½äº 320ï¼Œå› æ­¤å°†å¥å­é•¿åº¦å›ºå®šä¸º 320ï¼Œå¤šæˆªå°‘è¡¥
*   partial æ˜¯ Python åå‡½æ•°ï¼Œä½¿ç”¨è¯¥å‡½æ•°åï¼Œmy_collate çš„è¾“å…¥å‚æ•°åªæœ‰ä¸€ä¸ª batch

```python
def my_collate(batch, tokenizer):
    tokens, label_ids = map(list, zip(*batch))
 
    text_ids = tokenizer(tokens,
                         padding=True,
                         truncation=True,
                         max_length=320,
                         is_split_into_words=True,
                         add_special_tokens=True,
                         return_tensors='pt')
    return text_ids, torch.tensor(label_ids)
```

```python
# DataLoader
    collate_fn = partial(my_collate, tokenizer=tokenizer)
    train_loader = DataLoader(train_set, batch_size=train_batch_size, shuffle=True, num_workers=workers,
                              collate_fn=collate_fn, pin_memory=True)
    test_loader = DataLoader(test_set, batch_size=test_batch_size, shuffle=True, num_workers=workers,
                             collate_fn=collate_fn, pin_memory=True)
```

åˆ°æ­¤æˆ‘ä»¬å°±å®Œæˆåˆ¶ä½œäº† DataLoaderï¼Œåç»­ä» DataLoader ä¸­å¯è·å–ä¸€ä¸ªä¸ª batch ç» Tokenizer åŒ–åçš„æ•°æ®

## å››ã€Language model

å¯¹äºç½‘ç»œæ¨¡å‹æ¥è¯´ï¼Œåªèƒ½æ¥å—æ•°å­—æ•°æ®ç±»å‹ï¼Œ**å› æ­¤æˆ‘ä»¬éœ€è¦å»ºç«‹ä¸€ä¸ªè¯­è¨€æ¨¡å‹ï¼Œç›®çš„æ˜¯å°†æ¯ä¸ªå•è¯å˜æˆä¸€ä¸ªå‘é‡ï¼Œæ¯ä¸ªå¥å­å˜æˆä¸€ä¸ªçŸ©é˜µ**ã€‚å…³äºè¯­è¨€æ¨¡å‹ï¼Œå…¶å·²ç»å‘å±•å†å²éå¸¸æ‚ ä¹…äº†ï¼ˆå‘å±•å†å²å¦‚ä¸‹ï¼‰ï¼Œ**å…¶ä¸­ Bert æ¨¡å‹æ˜¯ Google å¤§ç¥å‡ºçš„å…·æœ‰é‡Œç¨‹ç¢‘æ€§è´¨çš„æ¨¡å‹ï¼Œå› æ­¤æœ¬ç¯‡åšå®¢ä¹Ÿä¸»è¦é‡‡ç”¨æ­¤æ¨¡å‹**

![](https://img-blog.csdnimg.cn/0fef69e5e0474fd28d45598afa6b0717.png)

æ‰€ç”¨çš„æ¨¡å‹éƒ½æ˜¯é€šè¿‡ API ä»Â  [Hugging Face å®˜ç½‘](https://huggingface.co/models?pipeline_tag=sentence-similarity&sort=downloads "Hugging Faceå®˜ç½‘") ä¸­ç›´æ¥ä¸‹è½½çš„

Hugging Face ä¸­æœ‰éå¸¸å¤šå¥½ç”¨çš„è¯­è¨€æ¨¡å‹ï¼Œå°ä¼™ä¼´ä»¬ä¹Ÿå¯å°è¯•å…¶ä»–æ¨¡å‹

![](https://img-blog.csdnimg.cn/53bd5e762ef149b1a301ecba90fc9d39.png)

ä½¿ç”¨ AutoModel.from_pretrained æ¥å£ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹

ä½¿ç”¨ AutoTokenizere.from_pretrained æ¥å£ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹çš„åˆ†è¯å™¨

```python
# Create model
        if args.model_name == 'bert':
            self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
            self.input_size = 768
            base_model = AutoModel.from_pretrained('bert-base-uncased')
        elif args.model_name == 'roberta':
            self.tokenizer = AutoTokenizer.from_pretrained('roberta-base', add_prefix_space=True)
            self.input_size = 768
            base_model = AutoModel.from_pretrained('roberta-base')
        else:
            raise ValueError('unknown model')
```

ä¸‹è½½åˆ›å»ºå¥½ Bert ä¹‹åï¼Œåœ¨è®­ç»ƒå’Œæµ‹è¯•çš„æ—¶å€™ï¼Œæ¯æ¬¡ä» DataLoadr ä¸­è·å–ä¸€ä¸ªä¸ªç»è¿‡ Tokenizer åˆ†è¯ä¹‹å Batch çš„æ•°æ®ï¼Œéšåå°†å…¶æŠ•æ”¾åˆ°è¯­è¨€æ¨¡å‹ä¸­

*   ** input ï¼šinput çš„æ•°æ®æ˜¯ Tokenizer åŒ–åçš„æ•°æ®å¦‚ {'input_id' : ~ , 'token_type_ids' : ~ , 'attention_mask' : ~}ï¼Œ** æ˜¯å°†é‡Œé¢çš„ä¸‰ä¸ª dict åˆ†æˆä¸€ä¸ªä¸ªç‹¬ç«‹çš„ dictï¼Œå³ { 'input_id' : ~} ï¼Œ{Â 'token_type_ids' : ~} ï¼Œ{Â 'attention_mask' : ~}
*   raw_outputs è·å–çš„æ˜¯ Bert çš„è¾“å‡ºï¼ŒBert çš„è¾“å‡ºä¸»è¦æœ‰å››ä¸ªï¼Œå…¶ä¸­ last_hidden_state è¡¨ç¤ºçš„æ˜¯æœ€åä¸€å±‚éšè—å±‚çš„çŠ¶æ€ï¼Œä¹Ÿå°±æ˜¯æ¯ä¸ªå•è¯çš„ Token çš„é›†åˆ

```python
raw_outputs = self.base_model(**inputs)
tokens = raw_outputs.last_hidden_state
```

**åˆ°æ­¤ï¼Œä¸Šæ¸¸è¯­è¨€æ¨¡å‹å…¨éƒ¨ç»“æŸï¼Œå¾—åˆ°äº†ä¸€ä¸ªä¸ªç»è¿‡ Bert åç»´åº¦ä¸º`[batch å¤§å°ï¼Œå¥å­é•¿åº¦ï¼Œå•è¯ç»´åº¦]` çš„æ•°æ®**

## äº”ã€Neural networkÂ model

å…³äº RNN çš„åŸç†è§£é‡Šï¼Œå°ä¼™ä¼´å¯ä»¥çœ‹ä»¥ä¸‹æ–‡ç« 

[ã€Deep Learning 7ã€‘RNN å¾ªç¯ç¥ç»ç½‘ç»œ](https://blog.csdn.net/ccaoshangfei/article/details/127513564?spm=1001.2014.3001.5501 "ã€Deep Learning 7ã€‘RNNå¾ªç¯ç¥ç»ç½‘ç»œ")

### 5.1 RNN

Bilstmã€lstmã€gru æœ¬è´¨ä¸Šæ¥è¯´éƒ½æ˜¯å±äº RNN æ¨¡å‹ï¼Œå› æ­¤æˆ‘ä»¬å°±ä»¥ RNN æ¨¡å‹ä¸ºä¾‹å­çœ‹çœ‹ä¸Šæ¸¸ä»»åŠ¡çš„æ•°æ®æ˜¯å¦‚ä½•è¿›å…¥åˆ°ä¸‹æ¸¸æ–‡æœ¬åˆ†ç±»çš„

**RNN è¾“å…¥å‚æ•°**

*   input_sizeï¼šæ¯ä¸ªå•è¯ç»´åº¦
*   hidden_sizeï¼šéšå«å±‚çš„ç»´åº¦ï¼ˆä¸€èˆ¬è®¾ç½®ä¸å¥å­é•¿åº¦ä¸€è‡´ï¼‰
*   num_layersï¼šRNN å±‚æ•°ï¼Œé»˜è®¤æ˜¯ 1ï¼Œå•å±‚ LSTM
*   biasï¼šæ˜¯å¦ä½¿ç”¨ bias
*   batch_firstï¼šé»˜è®¤ä¸º Falseï¼Œå¦‚æœè®¾ç½®ä¸º Trueï¼Œåˆ™è¡¨ç¤ºç¬¬ä¸€ä¸ªç»´åº¦è¡¨ç¤ºçš„æ˜¯ batch_size
*   dropoutï¼šéšæœºå¤±æ´»ï¼Œä¸€èˆ¬åœ¨æœ€ç»ˆåˆ†ç±»å™¨é‚£å—å†™å±‚ Dropoutï¼Œè¿™é‡Œå°±ä¸ç”¨äº†
*   bidirectionalï¼šæ˜¯å¦ä½¿ç”¨ BiLSTM

Â ç”±äºæ˜¯æƒ…æ„Ÿåˆ†ææ–‡æœ¬äºŒåˆ†ç±»ä»»åŠ¡ï¼Œå› æ­¤è¿˜éœ€è¦ä¸€ä¸ª FNN+Softmax åˆ†ç±»å™¨è¿›è¡Œåˆ†ç±»é¢„æµ‹

```python
class Rnn_Model(nn.Module):
    def __init__(self, base_model, num_classes, input_size):
        super().__init__()
        self.base_model = base_model
        self.num_classes = num_classes
        self.input_size = input_size
        self.Rnn = nn.RNN(input_size=self.input_size,
                          hidden_size=320,
                          num_layers=1,
                          batch_first=True)
        self.fc = nn.Sequential(nn.Dropout(0.5),
                                nn.Linear(320, 80),
                                nn.Linear(80, 20),
                                nn.Linear(20, self.num_classes),
                                nn.Softmax(dim=1))
        for param in base_model.parameters():
            param.requires_grad = (True)
```

Â å†æ¥çœ‹çœ‹ RNN çš„ä¼ æ’­è¿‡ç¨‹

 **RNN è¾“å‡ºå‚æ•°**

output, (hn, cn) = lstm(inputs)

output_last = output[:,-1,:]

*   outputï¼šæ¯ä¸ªæ—¶é—´æ­¥è¾“å‡º
*   output_lastï¼šæœ€åä¸€ä¸ªæ—¶é—´æ­¥éšè—å±‚ç¥ç»å…ƒè¾“å‡ºï¼Œä¹Ÿå°±æ˜¯æœ€ç»ˆçš„ç‰¹å¾è¡¨ç¤º
*   hnï¼šæœ€åä¸€ä¸ªæ—¶é—´æ­¥éšè—å±‚çš„çŠ¶æ€
*   cnï¼šæœ€åä¸€ä¸ªæ—¶é—´æ­¥éšè—å±‚çš„é—å¿˜é—¨å€¼

ç”±äºæˆ‘ä»¬ç”¨ä¸åˆ° hn å’Œ cnï¼Œå› æ­¤ç›´æ¥ä½¿ç”¨_æ¥ä»£æ›¿

```python
def forward(self, inputs):
        
        # ä¸Šæ¸¸ä»»åŠ¡
        raw_outputs = self.base_model(**inputs)
        cls_feats = raw_outputs.last_hidden_state
    
        # ä¸‹æ¸¸ä»»åŠ¡
        outputs, _ = self.Rnn(cls_feats)
        outputs = outputs[:, -1, :]
        outputs = self.fc(outputs)
        return outputs
```

Â è¾“å‡ºçš„ outputs å°±æ˜¯é¢„æµ‹ç»“æœ

### 5.2 GRU

å…¶å®ç†Ÿæ‚‰äº† RNN ç½‘ç»œæ¨¡å—ä¹‹åï¼Œå…¶ä»–å‡ ä¸ªç½‘ç»œæ¨¡å—çš„ä¹Ÿå°±éå¸¸å¥½ç†è§£äº†

å°† nn.RNN ä¿®æ”¹ä¸º nn.GRU

```python
class Gru_Model(nn.Module):
    def __init__(self, base_model, num_classes, input_size):
        super().__init__()
        self.base_model = base_model
        self.num_classes = num_classes
        self.input_size = input_size
        self.Gru = nn.GRU(input_size=self.input_size,
                          hidden_size=320,
                          num_layers=1,
                          batch_first=True)
        self.fc = nn.Sequential(nn.Dropout(0.5),
                                nn.Linear(320, 80),
                                nn.Linear(80, 20),
                                nn.Linear(20, self.num_classes),
                                nn.Softmax(dim=1))
        for param in base_model.parameters():
            param.requires_grad = (True)
 
    def forward(self, inputs):
        raw_outputs = self.base_model(**inputs)
        tokens = raw_outputs.last_hidden_state
 
        gru_output, _ = self.Gru(tokens)
        outputs = gru_output[:, -1, :]
        outputs = self.fc(outputs)
        return outputs
```

### 5.3 LSTM

å°† nn.RNN ä¿®æ”¹ä¸º nn.LSTM

```python
class Lstm_Model(nn.Module):
    def __init__(self, base_model, num_classes, input_size):
        super().__init__()
        self.base_model = base_model
        self.num_classes = num_classes
        self.input_size = input_size
        self.Lstm = nn.LSTM(input_size=self.input_size,
                            hidden_size=320,
                            num_layers=1,
                            batch_first=True)
        self.fc = nn.Sequential(nn.Dropout(0.5),
                                nn.Linear(320, 80),
                                nn.Linear(80, 20),
                                nn.Linear(20, self.num_classes),
                                nn.Softmax(dim=1))
        for param in base_model.parameters():
            param.requires_grad = (True)
 
    def forward(self, inputs):
        raw_outputs = self.base_model(**inputs)
        tokens = raw_outputs.last_hidden_state
        lstm_output, _ = self.Lstm(tokens)
        outputs = lstm_output[:, -1, :]
        outputs = self.fc(outputs)
        return outputs
```

### 5.4 BILSTM

bilstm ä¸å…¶ä»–å‡ ä¸ªç½‘ç»œæ¨¡å‹ç¨å¾®æœ‰ç‚¹ä¸åŒï¼Œéœ€è¦ä¿®æ”¹çš„åœ°æ–¹æœ‰ä¸‰å¤„

*   å°† nn.RNN ä¿®æ”¹ä¸º nn.LSTM
*   åœ¨ nn.LSTM ä¸­æ·»åŠ  bidirectional=True
*   ä¸€ä¸ª BILSTM æ˜¯ç”±ä¸¤ä¸ª LSTM ç»„åˆè€Œæˆçš„ï¼Œå› æ­¤ FNN è¾“å…¥çš„ç»´åº¦ä¹Ÿè¦ä¹˜ 2ï¼Œå³ nn.Linear(320 * 2, 80)

```python
class BiLstm_Model(nn.Module):
    def __init__(self, base_model, num_classes, input_size):
        super().__init__()
        self.base_model = base_model
        self.num_classes = num_classes
        self.input_size = input_size
        # Open the bidirectional
        self.BiLstm = nn.LSTM(input_size=self.input_size,
                              hidden_size=320,
                              num_layers=1,
                              batch_first=True,
                              bidirectional=True)
        self.fc = nn.Sequential(nn.Dropout(0.5),
                                nn.Linear(320 * 2, 80),
                                nn.Linear(80, 20),
                                nn.Linear(20, self.num_classes),
                                nn.Softmax(dim=1))
        for param in base_model.parameters():
            param.requires_grad = (True)
 
    def forward(self, inputs):
        raw_outputs = self.base_model(**inputs)
        cls_feats = raw_outputs.last_hidden_state
        outputs, _ = self.BiLstm(cls_feats)
        outputs = outputs[:, -1, :]
        outputs = self.fc(outputs)
        return outputs
```

### 5.5 TextCNN

æ—¢ç„¶ RNN å¯ä»¥åšæ–‡æœ¬åˆ†ç±»ï¼Œé‚£ CNN å‘¢ï¼Ÿç­”æ¡ˆå½“ç„¶æ˜¯å¯ä»¥çš„ï¼Œæ—©åœ¨ 2014 å¹´å°±å‡ºç°äº† TextCNNï¼ŒåŸæ¨¡å‹å¦‚ä¸‹

![](https://img-blog.csdnimg.cn/a44bb796d08d4ab7b8aa12c451061bdc.png)

æ¬¸ï¼Œçœ‹èµ·æ¥å¯èƒ½æœ‰ç‚¹æŠ½è±¡ï¼Œçœ‹å¦ä¸€ç¯‡è§£é‡Šè¯¥æ¨¡å‹çš„å›¾å¯èƒ½å¥½ç†è§£å¤šäº†

![](https://img-blog.csdnimg.cn/968c3a29e0ec412ba78cb4bc41d04fc3.png)

*   é¦–å…ˆåŸå¥ä¸å·ç§¯æ ¸åˆ†åˆ«ä¸º [2ï¼Œ768]ã€[3,768]ã€[4,768] ä¸” channels ä¸º 2 çš„ filtet è¿›è¡Œå·ç§¯è¿ç®—å¾—åˆ° 6 ä¸ªä¸€ç»´å‘é‡
*   éšåå°†æ¯ä¸ªä¸€ç»´å‘é‡ä¸­å–å‡ºæœ€å¤§å€¼ï¼Œå°†è¿™ 6 ä¸ªæœ€å¤§å€¼æ‹¼æ¥æˆ [6ï¼Œ2] çš„ Tensor
*   æœ€åè¿›è¡Œå¸¸è§„çš„åˆ†ç±»é¢„æµ‹
*   æ³¨æ„ nn.ModuleList çš„ Pytorch ä»£ç æŠ€å·§ï¼ŒModuleList å¯ä»¥ç†è§£ä¸ºå¯å­˜å‚¨å·ç§¯æ ¸çš„ List

```
class TextCNN_Model(nn.Module):
    def __init__(self, base_model, num_classes):
        super().__init__()
        self.base_model = base_model
        self.num_classes = num_classes
        for param in base_model.parameters():
            param.requires_grad = (True)
 
        # Define the hyperparameters
        self.filter_sizes = [2, 3, 4]
        self.num_filters = 2
        self.encode_layer = 12
 
        # TextCNN
        self.convs = nn.ModuleList(
            [nn.Conv2d(in_channels=1, out_channels=self.num_filters,
                       kernel_size=(K, self.base_model.config.hidden_size)) for K in self.filter_sizes]
        )
        self.block = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(self.num_filters * len(self.filter_sizes), self.num_classes),
            nn.Softmax(dim=1)
        )
 
    def conv_pool(self, tokens, conv):
        tokens = conv(tokens)
        tokens = F.relu(tokens)
        tokens = tokens.squeeze(3)
        tokens = F.max_pool1d(tokens, tokens.size(2))
        out = tokens.squeeze(2)
        return out
 
    def forward(self, inputs):
        raw_outputs = self.base_model(**inputs)
        tokens = raw_outputs.last_hidden_state.unsqueeze(1)
        out = torch.cat([self.conv_pool(tokens, conv) for conv in self.convs],
                        1)
        predicts = self.block(out)
        return predicts
```

### 5.7 FNN

å› ä¸ºæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯ Bert æ¨¡å‹ï¼Œå…¶æ¨¡å‹æœ¬èº«æ˜¯ç”± 12 å±‚ Transformer ç»„æˆï¼Œæ¯å±‚ Transformer åˆç”±å¤æ‚çš„ Attention ç½‘ç»œç»„æˆï¼Œæ‰€ä»¥ Bert æ¨¡å‹æœ¬èº«å°±æ˜¯ä¸€ä¸ªéå¸¸å¥½çš„ç½‘ç»œæ¨¡å‹ï¼Œæ‰€ä»¥å¯èƒ½æˆ‘ä¸å¤ªéœ€è¦åŠ  RNNã€CNN è¿™äº›æ“ä½œï¼Œç›´æ¥ä½¿ç”¨ FNN æˆ–è®¸ä¹Ÿå¯ä»¥å®ç°ä¸é”™çš„æ•ˆæœï¼ˆ7.Result éƒ¨åˆ†çš„æ¶ˆèå®éªŒä¹Ÿè¯å®äº†è¯¥æƒ³æ³•ï¼‰ã€‚

åœ¨è®²è§£è¿™ä¸ªä»£ç ä¹‹å‰ï¼Œä¸å¾—ä¸å†æ¬¡æèµ· Bert çš„è¾“å‡ºäº†ï¼Œè¾“å…¥ä¸€ä¸ªå¥å­ï¼ŒBert çš„è¾“å‡ºæ˜¯

ã€CLSã€‘token1 token 2 token3 token4 ... token n ã€SEPã€‘

token è¡¨ç¤ºæ¯ä¸ªè¾“å…¥å•è¯çš„å‘é‡ ï¼Œã€CLSã€‘è¡¨ç¤ºæ•´ä¸ªå¥å­çš„å‘é‡ï¼Œåš FNN éœ€è¦æ•´ä¸ªå¥å­çš„è¾“å…¥ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦è·å–çš„æ˜¯ã€CLSã€‘ã€‚ã€CLSã€‘æ˜¯éšå±‚ 0 å·ä½çš„æ•°æ®ï¼Œå› æ­¤å…·ä½“è·å–ã€CLSã€‘çš„ä»£ç æ˜¯

```
class Transformer(nn.Module):
    def __init__(self, base_model, num_classes, input_size):
        super().__init__()
        self.base_model = base_model
        self.num_classes = num_classes
        self.input_size = input_size
        self.linear = nn.Linear(base_model.config.hidden_size, num_classes)
        self.dropout = nn.Dropout(0.5)
        self.softmax = nn.Softmax()
        for param in base_model.parameters():
            param.requires_grad = (True)
 
    def forward(self, inputs):
        raw_outputs = self.base_model(**inputs)
        cls_feats = raw_outputs.last_hidden_state[:, 0, :]
        predicts = self.softmax(self.linear(self.dropout(cls_feats)))
        return predicts
```

å®Œæ•´ FNN ç½‘ç»œæ¨¡å—ä»£ç å¦‚ä¸‹Â 

```python
class Transformer_Attention(nn.Module):
    def __init__(self, base_model, num_classes):
        super().__init__()
        self.base_model = base_model
        self.num_classes = num_classes
        for param in base_model.parameters():
            param.requires_grad = (True)
 
        # Self-Attention
        self.key_layer = nn.Linear(self.base_model.config.hidden_size, self.base_model.config.hidden_size)
        self.query_layer = nn.Linear(self.base_model.config.hidden_size, self.base_model.config.hidden_size)
        self.value_layer = nn.Linear(self.base_model.config.hidden_size, self.base_model.config.hidden_size)
        self._norm_fact = 1 / math.sqrt(self.base_model.config.hidden_size)
 
        self.block = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(768, 128),
            nn.Linear(128, 16),
            nn.Linear(16, num_classes),
            nn.Softmax(dim=1)
        )
 
    def forward(self, inputs):
        raw_outputs = self.base_model(**inputs)
        tokens = raw_outputs.last_hidden_state
 
        K = self.key_layer(tokens)
        Q = self.query_layer(tokens)
        V = self.value_layer(tokens)
        attention = nn.Softmax(dim=-1)((torch.bmm(Q, K.permute(0, 2, 1))) * self._norm_fact)
        attention_output = torch.bmm(attention, V)
        attention_output = torch.mean(attention_output, dim=1)
 
        predicts = self.block(attention_output)
        return predicts
```

### 5.8 Self-Attention

RNN å’Œ CNN éƒ½ä»‹ç»è¿‡äº†ï¼Œé‚£ Attention æœºåˆ¶æ€ä¹ˆèƒ½å°‘å‘¢ï¼Ÿ

å…³äº Self-Attention çš„ç»†èŠ‚ï¼Œå¯ä»¥å‚è€ƒ[è¿™ç¯‡æ–‡ç« ](https://blog.csdn.net/ccaoshangfei/article/details/128044362?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522167913774416800186524363%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fblog.%2522%257D&request_id=167913774416800186524363&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~blog~first_rank_ecpm_v1~rank_v31_ecpm-1-128044362-null-null.blog_rank_default&utm_term=Attention&spm=1018.2226.3001.4450 "è¿™ç¯‡æ–‡ç« ")

åœ¨è¿™é‡Œï¼Œä¸ºäº†æ–¹ä¾¿å¯¹ç€å…¬å¼é˜…è¯»ä»£ç ï¼Œæˆ‘å°† Attention çš„å…¬å¼æ”¾ç½®å¦‚ä¸‹

![](https://latex.csdn.net/eq?Attention%28Q%2CK%2CV%29%3DSoftmax%28%5Cfrac%20%7BQ.%7BK%7D%5E%7BT%7D%7D%20%7B%5Csqrt%20%7Bd%7D%7D%29.V)

```
class Transformer_CNN_RNN(nn.Module):
    def __init__(self, base_model, num_classes):
        super().__init__()
        self.base_model = base_model
        self.num_classes = num_classes
        for param in base_model.parameters():
            param.requires_grad = (True)
 
        # Define the hyperparameters
        self.filter_sizes = [3, 4, 5]
        self.num_filters = 100
 
        # TextCNN
        self.convs = nn.ModuleList(
            [nn.Conv2d(in_channels=1, out_channels=self.num_filters,
                       kernel_size=(K, self.base_model.config.hidden_size)) for K in self.filter_sizes]
        )
 
        # LSTM
        self.lstm = nn.LSTM(input_size=self.base_model.config.hidden_size,
                            hidden_size=320,
                            num_layers=1,
                            batch_first=True)
 
        self.block = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(620, 128),
            nn.Linear(128, 16),
            nn.Linear(16, num_classes),
            nn.Softmax(dim=1)
        )
 
    def conv_pool(self, tokens, conv):
        # x -> [batch,1,text_length,768]
        tokens = conv(tokens)  # shape [batch_size, out_channels, x.shape[2] - conv.kernel_size[0] + 1, 1]
        tokens = F.relu(tokens)
        tokens = tokens.squeeze(3)  # shape [batch_size, out_channels, x.shape[2] - conv.kernel_size[0] + 1]
        tokens = F.max_pool1d(tokens, tokens.size(2))  # shape[batch, out_channels, 1]
        out = tokens.squeeze(2)  # shape[batch, out_channels]
        return out
 
    def forward(self, inputs):
        raw_outputs = self.base_model(**inputs)
        cnn_tokens = raw_outputs.last_hidden_state.unsqueeze(1)  # shape [batch_size, 1, max_len, hidden_size]
        cnn_out = torch.cat([self.conv_pool(cnn_tokens, conv) for conv in self.convs],
                            1)  # shape  [batch_size, self.num_filters * len(self.filter_sizes]
        rnn_tokens = raw_outputs.last_hidden_state
        rnn_outputs, _ = self.lstm(rnn_tokens)
        rnn_out = rnn_outputs[:, -1, :]
        # cnn_out --> [batch,300]
        # rnn_out --> [batch,320]
        out = torch.cat((cnn_out, rnn_out), 1)
        predicts = self.block(out)
        return predict
```

### 5.9 TextCNN+LSTM

åœ¨è¿™é‡Œï¼Œæˆ‘å…ˆæŠ›å‡ºä¸€ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬ä¸ºä»€ä¹ˆè¦ç”¨ CNN å’Œ LSTMï¼Œå®ƒç©¶ç«Ÿæœ‰ä»€ä¹ˆå¥½å¤„ï¼Ÿ

TextCNN æ˜¯ä½¿ç”¨äº†å·ç§¯æœºåˆ¶ï¼Œæå–äº†æ–‡æœ¬ä¸­æœ€å…³é”®çš„ä¿¡æ¯ï¼Œä¹Ÿå°±æ˜¯æœ€å…·æœ‰åŒºåˆ†æ€§çš„ä¿¡æ¯ï¼Œæ¯”å¦‚ 5.5 ä¸­çš„ TextCNN å±•ç¤ºå›¾ï¼Œä¸€ä¸ª 7*5 å¦‚æ­¤å¤§çš„çŸ©é˜µï¼Œæœ€ç»ˆç«Ÿç„¶åªç”¨ 6*1 çš„å‘é‡å°±å¯è¡¨å¾äº†

è€Œ LSTM æ°å·§ç›¸åï¼Œå®ƒæ˜¯ä¸æ–­åœ°å…³æ³¨äº†æ—¶é—´åºåˆ—ä¸­æ¯ä¸ªéšè—å±‚çŠ¶æ€ï¼Œç›®çš„æ˜¯æŒ–æ˜å‡ºæ·±è—çš„è¯­ä¹‰ä¿¡æ¯

å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å°† TextCNN å’Œ LSTM ç»“åˆèµ·æ¥ï¼Œå°±åƒä½ è€ƒ CET6 é˜…è¯»ç†è§£ä¸€æ ·ä¸€æ ·ï¼Œæ—¢è¦æŠ“å–æœ€æ˜¾è‘—çš„ä¿¡æ¯å¿«é€Ÿåˆ¤æ–­ï¼Œåˆè¦ç»†è¯»æ–‡ç« å†…å®¹ï¼Œç†è§£å†…å«çš„è¯­ä¹‰ä¿¡æ¯ï¼ŒäºŒè€…ç»“åˆæ¥ä¾¿äºåˆ¤æ–­

è¯´äº†è¿™ä¹ˆå¤šï¼Œçœ‹çœ‹ä»£ç åˆ°åº•å¦‚ä½•å®ç°

```python
class Transformer_CNN_RNN_Attention(nn.Module):
    def __init__(self, base_model, num_classes):
        super().__init__()
        self.base_model = base_model
        self.num_classes = num_classes
        for param in base_model.parameters():
            param.requires_grad = (True)
 
        # Define the hyperparameters
        self.filter_sizes = [3, 4, 5]
        self.num_filters = 100
 
        # TextCNN
        self.convs = nn.ModuleList(
            [nn.Conv2d(in_channels=1, out_channels=self.num_filters,
                       kernel_size=(K, self.base_model.config.hidden_size)) for K in self.filter_sizes]
        )
 
        # LSTM
        self.lstm = nn.LSTM(input_size=self.base_model.config.hidden_size,
                            hidden_size=320,
                            num_layers=1,
                            batch_first=True)
        # Self-Attention
        self.key_layer = nn.Linear(self.base_model.config.hidden_size, self.base_model.config.hidden_size)
        self.query_layer = nn.Linear(self.base_model.config.hidden_size, self.base_model.config.hidden_size)
        self.value_layer = nn.Linear(self.base_model.config.hidden_size, self.base_model.config.hidden_size)
        self._norm_fact = 1 / math.sqrt(self.base_model.config.hidden_size)
 
        self.block = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(620, 128),
            nn.Linear(128, 16),
            nn.Linear(16, num_classes),
            nn.Softmax(dim=1)
        )
 
    def conv_pool(self, tokens, conv):
        # x -> [batch,1,text_length,768]
        tokens = conv(tokens)  # shape [batch_size, out_channels, x.shape[2] - conv.kernel_size[0] + 1, 1]
        tokens = F.relu(tokens)
        tokens = tokens.squeeze(3)  # shape [batch_size, out_channels, x.shape[2] - conv.kernel_size[0] + 1]
        tokens = F.max_pool1d(tokens, tokens.size(2))  # shape[batch, out_channels, 1]
        out = tokens.squeeze(2)  # shape[batch, out_channels]
        return out
 
    def forward(self, inputs):
        raw_outputs = self.base_model(**inputs)
        tokens = raw_outputs.last_hidden_state
        # Self-Attention
        K = self.key_layer(tokens)
        Q = self.query_layer(tokens)
        V = self.value_layer(tokens)
        attention = nn.Softmax(dim=-1)((torch.bmm(Q, K.permute(0, 2, 1))) * self._norm_fact)
        attention_output = torch.bmm(attention, V)
 
        # TextCNN
        cnn_tokens = attention_output.unsqueeze(1)  # shape [batch_size, 1, max_len, hidden_size]
        cnn_out = torch.cat([self.conv_pool(cnn_tokens, conv) for conv in self.convs],
                            1)  # shape  [batch_size, self.num_filters * len(self.filter_sizes]
 
        rnn_tokens = tokens
        rnn_outputs, _ = self.lstm(rnn_tokens)
        rnn_out = rnn_outputs[:, -1, :]
        # cnn_out --> [batch,300]
        # rnn_out --> [batch,320]
        out = torch.cat((cnn_out, rnn_out), 1)
        predicts = self.block(out)
        return predicts
```

### 5.10 Attention+LSTM+TextCNN

æ—¢ç„¶æˆ‘ä»¬çš„ç½‘ç»œæ¨¡å‹åŒæ—¶åŠ äº† LSTM å’Œ TextCNNï¼Œé‚£ Attention æ€ä¹ˆèƒ½å°‘å‘¢ï¼Ÿ

ä» IO è§’åº¦æ¥çœ‹ Self-Attentionï¼Œå®ƒè¾“å…¥æ˜¯ä»€ä¹ˆå°ºå¯¸çš„çŸ©é˜µï¼Œè¾“å‡ºä¹Ÿæ˜¯ä»€ä¹ˆå°ºå¯¸çš„çŸ©é˜µï¼Œå› æ­¤ä¸€ä¸ªç®€å•çš„åšæ³•å°±æ˜¯å°† Bert çš„è¾“å‡ºï¼Œå…ˆæ”¾åˆ° Attention æ¨¡å—ä¸­ï¼Œå°† Attention çš„è¾“å‡ºæ”¾åˆ°åç»­çš„ Lstm å’Œ TexCNN

ä»£ç å¦‚ä¸‹

```
def _train(self, dataloader, criterion, optimizer):
        train_loss, n_correct, n_train = 0, 0, 0
        # Turn on the train mode
        self.Mymodel.train()
        for inputs, targets in tqdm(dataloader, disable=self.args.backend, ascii='>='):
            inputs = {k: v.to(self.args.device) for k, v in inputs.items()}
            targets = targets.to(self.args.device)
            predicts = self.Mymodel(inputs)
            loss = criterion(predicts, targets)
 
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
 
            train_loss += loss.item() * targets.size(0)
            n_correct += (torch.argmax(predicts, dim=1) == targets).sum().item()
            n_train += targets.size(0)
 
        return train_loss / n_train, n_correct / n_train
```

## å…­ã€Train and Test

æœ€åå°±æ˜¯ç¼–å†™å¯¹åº”çš„è®­ç»ƒå‡½æ•°å’Œæµ‹è¯•å‡½æ•°å•¦

å¯èƒ½æœ‰äº›å°ä¼™ä¼´ä¸æ‡‚ tqdm å‡½æ•°ï¼Œå®ƒçš„åŠŸèƒ½å°±æ˜¯èƒ½æ˜¾ç¤ºåŠ¨æ€è¿›åº¦ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤º

![](https://img-blog.csdnimg.cn/c2d6417c247743869d88352985cd20d9.png)

**è®­ç»ƒå‡½æ•°**

æ³¨æ„è¦å¼€å¯è®­ç»ƒæ¨¡å¼ï¼Œå³ self.Mymodel.train()ï¼Œå¦‚æ­¤æœ‰äº›å±‚å¦‚ dropout å±‚å‚æ•°å¯ä»¥è¿›è¡Œæ›´æ–°

```
def _test(self, dataloader, criterion):
        test_loss, n_correct, n_test = 0, 0, 0
        # Turn on the eval mode
        self.Mymodel.eval()
 
        with torch.no_grad():
            for inputs, targets in tqdm(dataloader, disable=self.args.backend, ascii=' >='):
                inputs = {k: v.to(self.args.device) for k, v in inputs.items()}
                targets = targets.to(self.args.device)
                predicts = self.Mymodel(inputs)
                loss = criterion(predicts, targets)
 
                test_loss += loss.item() * targets.size(0)
                n_correct += (torch.argmax(predicts, dim=1) == targets).sum().item()
                n_test += targets.size(0)
 
        return test_loss / n_test, n_correct / n_test
```

**æµ‹è¯•å‡½æ•°**

æ³¨æ„è¦å¼€å¯éªŒè¯æ¨¡å¼ï¼Œå³ self.Mymodel.eval()ï¼Œå¦‚æ­¤æœ‰äº›å±‚å¦‚ dropout å‚æ•°ä¸ä¼šæ›´æ–°äº†

```
# Get the best_loss and the best_acc
best_loss, best_acc = 0, 0
for epoch in range(self.args.num_epoch):
    train_loss, train_acc = self._train(train_dataloader, criterion, optimizer)
    test_loss, test_acc = self._test(test_dataloader, criterion)
    if test_acc > best_acc or (test_acc == best_acc and test_loss < best_loss):
          best_acc, best_loss = test_acc, test_loss
```

Â æœ€ååœ¨ run å‡½æ•°ä¸­è¿›è¡Œå¤šæ¬¡è®­ç»ƒå’Œè·å–æœ€ä½³è®­ç»ƒå‡†ç¡®ç‡

```
def _train(self, dataloader, criterion, optimizer):
        train_loss, n_correct, n_train = 0, 0, 0
 # Confusion matrix
        TP, TN, FP, FN = 0, 0, 0, 0
 # Turn on the train mode
        self.Mymodel.train()
        for inputs, targets in tqdm(dataloader, disable=self.args.backend, ascii='>='):
            inputs = {k: v.to(self.args.device) for k, v in inputs.items()}
            targets = targets.to(self.args.device)
            predicts = self.Mymodel(inputs)
            loss = criterion(predicts, targets)
 
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
 
            train_loss += loss.item() * targets.size(0)
            n_correct += (torch.argmax(predicts, dim=1) == targets).sum().item()
            n_train += targets.size(0)
 
            ground_truth = targets
            predictions = torch.argmax(predicts, dim=1)
            TP += torch.logical_and(predictions.bool(), ground_truth.bool()).sum().item()
            FP += torch.logical_and(predictions.bool(), ~ground_truth.bool()).sum().item()
            FN += torch.logical_and(~predictions.bool(), ground_truth.bool()).sum().item()
            TN += torch.logical_and(~predictions.bool(), ~ground_truth.bool()).sum().item()
 
        precision = TP / (TP + FP)
        recall = TP / (TP + FN)
        specificity = TN / (TN + FP)
        f1_score = 2 * precision * recall / (precision + recall)
 
        return train_loss / n_train, n_correct / n_train, precision, recall, specificity, f1_score
```

## **ä¸ƒã€Result**

åˆ°äº†æœ€å¿«ä¹çš„ç‚¼ä¸¹æ—¶é—´ï¼Œçœ‹çœ‹æœ€ç»ˆçš„æ•ˆæœæ€ä¹ˆæ ·

![](https://img-blog.csdnimg.cn/direct/a76831b3054a481e8bd5202498af48ae.png)

åˆ†æ

*   æ€»ä½“è¡¨ç°çœ‹ï¼ŒALT æ•ˆæœæœ€å¥½ï¼Œç»ˆç©¶æ˜¯ç¼åˆæ€ªèµ¢äº†
*   ä» PLM è§’åº¦çœ‹ï¼ŒRoberta è¾ƒ Bert å¥½ï¼Œä¸æ„§ä¸ºå‡çº§ç‰ˆ Bert
*   FNN æ•ˆæœä¹Ÿä¸é”™ï¼Œè¿™æ˜¯ç”±äº PLM æœ¬èº«æºå¸¦äº†å¤§é‡çš„å‚æ•°ï¼Œæ¨¡å‹å·²ç»è¶³å¤Ÿå¤æ‚äº†
*   çº¯ TextCNN çš„æ•ˆæœæœ‰ç‚¹é¸¡è‚‹ï¼Œå› ä¸º CNN ä¸»è¦å…³æ³¨å±€éƒ¨è€Œ RNN å…³æ³¨å…¨å±€ã€‚

## å…«ã€Conclusion

ç›®å‰è¯¥æ•°æ®é›†çš„ SOTA æ˜¯ä½¿ç”¨ XLNet æ¨¡å‹è·‘çš„ 96.21%ï¼Œæœ¬æ¨¡å‹åªæ˜¯ç”¨äº† 10% çš„æ•°æ®é›† + ç®€å•çš„ç½‘ç»œæ¶æ„ + æœªè°ƒå‚å°±å¯ä»¥è¾¾åˆ° 93% çš„å‡†ç¡®ç‡ï¼Œæ•ˆæœè¿˜æ˜¯ä¸é”™çš„ğŸ˜ğŸ˜ğŸ˜

![](https://img-blog.csdnimg.cn/2e94c1ecadb843db9b774c7155c4f472.png)

ç™½å«–æ—¶ï¼Œéº»çƒ¦å¤§ä½¬ä»¬åŠ¨åŠ¨é¼ æ ‡ç»™ä¸ª starï¼Œè¿™å¯¹æˆ‘å¾ˆé‡è¦ğŸ¥°ğŸ¥°ğŸ¥°

## ä¹ã€Reference

[1]Â  Yoon Kim. 2014.Â [Convolutional Neural Networks for Sentence Classification](https://aclanthology.org/D14-1181 "Convolutional Neural Networks for Sentence Classification"). InÂ _Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 1746â€“1751, Doha, Qatar. Association for Computational Linguistics.

[2]Â Vaswani, A. , Â Shazeer, N. , Â Parmar, N. , Â Uszkoreit, J. , Â Jones, L. , & Â Gomez, A. N. , et al. (2017). Attention is all you need. arXiv.

## åã€Another questionï¼ˆ2023-11ï¼‰

æ±‡æ€»ä¸€äº›è¯„è®ºåŒºæˆ–ç§ä¿¡çš„ä¸€äº›é—®é¢˜

### Question 1ï¼šHow to add precisionï¼Œrecallï¼Œspecificity and f1_scall

åœ¨æ–‡æœ¬åˆ†ç±»ä¸­ä¸€èˆ¬æ˜¯åªä½¿ç”¨ Accuracy ä½œä¸ºè¯„åˆ¤æ ‡å‡†ï¼Œå› æ­¤å…¶ä»–çš„æ ‡å‡†å°±æ²¡æœ‰æ”¾åˆ°å¼€æºä»£ç ä¸­äº†ï¼Œè‹¥æƒ³å®ç°è¿™äº›åŠŸèƒ½ï¼Œåœ¨ train()ã€test() ä¸­è¿›è¡Œä¿®æ”¹ä»£ç å³å¯ï¼Œåœ¨è¿™é‡Œæˆ‘ä»¥ train() å‡½æ•°ä¸ºä¾‹

ç°æ”¾ä¸Šä¿®æ”¹åå®Œæ•´çš„ train() ä»£ç 

```python
def forward(self, inputs):
        raw_outputs = self.base_model(**inputs)
        tokens = raw_outputs.last_hidden_state
        lstm_output, _ = self.Lstm(tokens)
 # Layer Normalization
        norm_lstm = nn.LayerNorm([lstm_output.shape[1], lstm_output.shape[2]], eps=1e-8).cuda()
        ln_lstm = norm_lstm(lstm_output)
        
        outputs = lstm_output[:, -1, :]
        outputs = self.fc(outputs)
        return outputs
```

targets å°±æ˜¯çœŸå®çš„æ ‡ç­¾ï¼Œé‚£é¢„æµ‹ç»“æœå‘¢ï¼Œæ˜¯ predicts å—ï¼Ÿå½“ç„¶ä¸æ˜¯ï¼Œå› ä¸º predicts ä¸­æ˜¯é¢„æµ‹æ¯æ¡å¥å­çš„æƒ…æ„Ÿç±»åˆ«æ˜¯ positive å’Œ negative åˆ†åˆ«æ¦‚ç‡æ˜¯å¤šå°‘ï¼Œæ¯”å¦‚ä¸€æ¡å¥å­çš„é¢„æµ‹ç»“æœæ˜¯ã€0.2,0.8ã€‘ï¼Œé‚£ä¹ˆæˆ‘ä»¬éœ€è¦å°†è¯¥å¥çš„åˆ¤å®šç»“æœä¸º 0.8ï¼Œä¹Ÿå°±æ˜¯ä¸‹æ ‡ 1ï¼Œæ‰€ä»¥ torch.argmax(predicts,dim=1) æ‰æ˜¯çœŸæ­£çš„é¢„æµ‹ç»“æœã€‚

æœ‰äº†è¿™ä¸¤ä¸ªä¹‹åï¼Œæˆ‘ä»¬ä¾¿å¯ä»¥è®¡ç®—æ··æ·†çŸ©é˜µï¼Œ**è¿™é‡Œæˆ‘ä½¿ç”¨äº† logical_and å’Œ bool() å‡½æ•°çš„æŠ€å·§å»å®ç°å®ƒï¼Œè€Œæ²¡æœ‰ç›´æ¥ç”¨å„ç§åº“å‡½æ•°ï¼Œç›®çš„æ˜¯ä¸ºäº†å±•ç¤ºæˆ‘æ˜¯å¦‚ä½•å¤„ç†æ•°æ®çš„**ï¼Œè¿™æ ·å¦‚æœå¤§å®¶æƒ³è¦æœ‰å…¶ä»–è¯„åˆ¤æ ‡å‡†ä¹Ÿå¯ä»¥**ç”±æ­¤å€Ÿé‰´**ã€‚

### Question 2ï¼šWhy run so slowly

åœ¨ config.pyï¼Œè®¾ç½®çš„é»˜è®¤ç¯å¢ƒä¸º CPUï¼Œä»¥åŠ train_batch_size å’Œ test_batch_size éƒ½è®¾ç½®è¾ƒå°ï¼Œç›®çš„æ˜¯å…ˆè·‘é€šè¯¥ç¨‹åºï¼ŒéšåæŒ‰è‡ªå·±èµ„æºé…ç½®æ¥æ‰©å¤§ batch_size ä»¥åŠå°† cpu æ”¹æˆ cudaÂ 

![](https://img-blog.csdnimg.cn/f354e0f9f4e14cf1a0d1b9be18e8d4f2.png)

### Question 3ï¼šHow to improve the acc on my dataset

å¯èƒ½å¾ˆå¤šå°ä¼™ä¼´åœ¨ç”¨äº†è‡ªå·±çš„æ•°æ®é›†åå‘ç°å‡†ç¡®ç‡å¯èƒ½åœåœ¨ 80%ï¼Œæˆ‘ä¼°è®¡åŸå› å¯èƒ½æ˜¯ï¼šæ•°æ®é›†è´¨é‡ï¼ˆå¯è¿›ä¸€æ­¥æ•°æ®æ¸…ç†ï¼‰ï¼Œå‚æ•°è°ƒä¼˜é—®é¢˜

æ­¤å¤–ï¼Œä¹Ÿå¯ä»¥è€ƒè™‘èå…¥æ›´å¤šçš„çš„æ¨¡å—ï¼Œæ¯”å¦‚ Prompt learningã€Contrastive learningï¼ŒCapsuleNetï¼ŒDual learningã€‚å†æˆ–è€…é‡‡ç”¨æ€§èƒ½æ›´ä¼˜è´¨çš„é¢„è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹ï¼Œæ¯”å¦‚ Roberta-Large

ä¹Ÿå¯ä»¥å°è¯•æœ€ç®€å•çš„æ¨¡å—ï¼Œæ¯”å¦‚ Layer Normalizationã€Batch Normalizationã€æ–°çš„æ¿€æ´»å‡½æ•°ç­‰ç­‰

åœ¨è¿™é‡Œæˆ‘å±•ç¤º Layer Normalization çš„æŠ€å·§æ¨¡å—

ä»¥ä¸‹å–è‡ª LSTM çš„ç½‘ç»œéƒ¨åˆ†ï¼Œå¤§å®¶éœ€è¦å…³æ³¨çš„æ˜¯**#Layer Normalization ä¸‹çš„ä¸¤è¡Œä»£ç **

```
def forward(self, inputs):
        raw_outputs = self.base_model(**inputs)
        tokens = raw_outputs.last_hidden_state
        lstm_output, _ = self.Lstm(tokens)
 # Layer Normalization
        norm_lstm = nn.LayerNorm([lstm_output.shape[1], lstm_output.shape[2]], eps=1e-8).cuda()
        ln_lstm = norm_lstm(lstm_output)
        
        outputs = lstm_output[:, -1, :]
        outputs = self.fc(outputs)
        return outputs
```

### Â Question 4: How to adapt to your own datasetï¼Ÿ

æƒ³è¦é€‚é…è‡ªå·±æ•°æ®é›†ï¼Œæ¯”å¦‚ä¸€ä¸ªæ–°é—»ä¸»é¢˜ 6 åˆ†ç±»ï¼Œä»£ç å¦‚ä½•ä¿®æ”¹å‘¢ï¼Ÿ

ä¸€ä¸ªå¿«é€Ÿåšæ³•æ˜¯é¦–å…ˆä¿è¯æ•°æ®é›†æ ¼å¼ä¸ IMDB æ ¼å¼ä¸€è‡´

1ï¼‰ä¿®æ”¹ config.py æ–‡ä»¶ä¸­ num_classes ä¸º 6

2ï¼‰ä¿®æ”¹ data.py æ–‡ä»¶ä¸­ max_lengthï¼ˆæ–‡ä¸­å·²è§£é‡Šè¯¥å˜é‡ï¼‰

å…³æ³¨æœ¬åšå®¢ï¼ŒåæœŸæ¨å‡ºèåˆ**å¯¹å¶ç†è®ºã€æç¤ºå­¦ä¹ ã€å¯¹æ¯”å­¦ä¹ ã€æ‰©æ•£æ¨¡å‹**æ¨¡å—çš„æ–‡ç« ï¼Œå¸Œæœ›å¤§å®¶å¤šå¤šå…³æ³¨ã€‚ï¼ˆæœ‰åšè¿™å—å†…å®¹çš„å°ä¼™ä¼´æ¬¢è¿äº¤æµï¼ï¼‰

ç¥å¤§å®¶éƒ½èƒ½ç‚¼ä¸¹é¡ºåˆ©ï¼Œæ—©æ—¥ acceptï¼ï¼ï¼âœŒğŸ»âœŒğŸ»âœŒğŸ»