# 图形学基础

## 2 一份模型文件中大概会记录哪些顶点数据？​

- “顶点位置”、“顶点法线”、“顶点色”、“UV” 、顶点索引
- 不同格式的数据存储内容存在一定差异，比如 FBX 会比 OBJ 多一些绑定蒙皮、动画和多套 UV 的信息。​

## 3 什么是切线空间？有哪些数据会使用切线空间存储？举几个例子​

切线空间（Tangent Space）是一个相对于模型表面的局部坐标系，用于一些模型表面基于平面的数据，比如切线、副切线、法线的方向。
  

## 6 透明是怎么做的？Alphatest，Alphablend 底层是怎么实现的?
通常使用两种方法来实现透明效果：Alphatest，Alphablend

1. **透明度测试 (Alpha Test)**：无法得到半透明，要么完全透明，要么完全不透明
    - **只要一个片元的透明度不满足条件（通常是小于某个阙值），那么它对应的片元就会被舍弃。** 被舍弃的片元将不会再进行任何处理，也不会对颜色缓冲产生任何影响；
    - **否则，就会按照普通的不透明物体的处理方式来处理它，即进行深度测试、深度写入等。** 
    - 也就是说，透明度测试是<mark style="background: #FF5582A6;">不需要关闭深度写入</mark>的，它和其他不透明物体最大的不同就是它会根据透明度来舍弃些片元。**虽然简单，但是它产生的效果也很极端，要么完全透明，即看不到，要么完全不透明，就像不透明物体那样。**
2. **透明度混合 (Alpha Blending)**： 这种方法可以得到真正的半透明效果
    - 它会使用当前片元的透明度作为混合因子，与已经存储在颜色缓冲中的颜色值进行混合，得到新的颜色。
    - 透明度混合需要**关闭深度写入，破坏了深度缓冲机制，这使得我们要非常小心物体的渲染顺序**。需要注意的是，透明度混合<mark style="background: #FF5582A6;">只关闭了深度写入，但没有关闭深度测试</mark>。
    - 当使用透明度混合渲染一个片元时，会进行深度测试，测试通过才会进行混合操作。这一点决定了当一个不透明物体出现在一个透明物体前，我们先渲染不透明物体并进行了深度写入，后面的透明物体无法通过深度测试所以被遮挡。
    - **深度缓冲中的值其实是像素级别的，即每个像素有一个深度值。但是由于我们关闭了深度写入，无法对模型进行像素级别的排序**。当模型网格之间有互相交叉的结构时，往往会得到错误的半透明效果（解决办法 Pre-Z）

## 5 如何使用法线贴图的 RG 通道计算出 B 通道？​

1. 采样贴图得到 RG 通道的值，一般是切线空间贴图，大小范围 $(0,1)$
2. 执行 UnpackNormal 操作，RG 通道分别**乘 2 减 1**，得到 XY 分量范围在 $(-1,1)$
3. 计算 Z 分量的值，根据**勾股定理**：$​Z = sqrt(1 - X^2 - Y^2)$
4. 将 Z 分量加 1 除 2 重新映射回法线贴图的 B 通道中。

## 8. 什么是 ao？游戏流程中有哪些实现 ao 的方式？​

AO（Ambient Occlusion）​描述在光线从光源照射到物体表面时，由于物体的凹凸不平或者其他物体的遮挡，导致一部分光线无法直接照射到物体表面而被阻挡的现象。

实时渲染中 AO 的实现方式大概有 SSAO、GTAO、RTXAO（实时、预烘焙）等​

## 9. 浮点数全精度和半精度有什么区别？错误选择不合适的精度会造成什么后果？举个例子​

**浮点数全精度和半精度之间的区别在于它们所使用的位数和表示范围**。全精度浮点数是 32 位的，范围不同标准都有所不同，大致是正负 3.402823466×10^32，半精度浮点数是 16 位的，范围大约是正负 65,504​。

ii. 错误选择全精度半精度会导致下面问题​

1. 使用全精度代替半精度：一般不会有效果问题，但是在性能上会变差​

1. 使用半精度代替全精度：由于精度不够可能会出现色带、马赛克。数值溢出可能会导致不可控的材质表现（一般都奇奇怪怪的，比如模型莫名其妙的消失、过曝等）

## 10 什么是指令数？材质制作中通常哪些操作的指令数比较高​


- 指令数是指 GPU 运行顶点、片元着色器时需要执行的命令数量，指令数越多消耗越高。

- 对于材质制作来说，指令数比较多的操作有：三角函数、平方和开方、除法、For 循环、噪声等​

## 11 什么是 drawcall？drawcall 数量一般和什么有关？简单举几个例子​

drawcall（绘制调用）是指CPU调用图形   API，以命令GPU进行渲染操作。



1. drawcall 数量一般与以下几个因素有关：​
    - 对象数量：每个对象都需要至少一个 drawcall 来进行渲染。如果场景中有大量的独立对象，那么 drawcall 的数量就会增加。​
    - 材质数量：每个材质都需要进行一次 drawcall。如果场景中的对象使用了不同的材质，那么 drawcall 的数量会随着材质数量的增加而增加。
    - 纹理贴图：每个纹理贴图都需要进行一次 drawcall。如果对象使用了多个纹理贴图，那么 drawcall 的数量会增加。​

举几个例子来说明 drawcall 数量的关系：​

1. 场景中有 100 个独立的立方体对象，每个对象使用相同的材质和纹理贴图。在这种情况下，绘制调用数量将是 100，因为每个对象都需要一个 drawcall。​
2. 场景中有 100 个独立的立方体对象，每个对象使用不同的材质和纹理贴图。在这种情况下，绘制调用数量将大于 100，因为每个对象的材质和纹理贴图不同，需要额外的 drawcall。​

需要注意的是，过多的 drawcall 数量可能会对性能产生负面影响，因为在每个 drawcall 之间切换上下文会引入额外的开销。优化 drawcall 数量是提高图形性能的一项重要任务，**可以通过合并对象、合并材质和纹理等方式来减少 drawcall 的数量**。

## 12 什么是 overdraw？什么操作会增加 overdraw 的数量？简单举几个例子​

**Overdraw（过度绘制）是指在渲染过程中多次绘制相同像素区域的情况**。当多个物体或图层在屏幕上重叠时，如果它们都完全或部分地覆盖相同的像素，就会导致过度绘制。​

增加 overdraw 的数量的一些操作和情况包括：​

1. 不可见的物体：当不可见的物体被渲染时，会导致不必要的 overdraw。例如，当一个物体完全被其他物体遮挡时，但仍然被渲染并覆盖了相应的像素区域。​
2. 透明度和混合：透明度和混合效果也可能增加 overdraw。当多个半透明物体重叠时，它们的颜色会叠加在一起，导致对相同像素区域的多次绘制。​
3. 多层叠加的图层：在图形合成过程中，多个图层的叠加也可能导致 overdraw。例如，在使用图层混合（layer blending）或渲染半透明效果时，多个图层之间的重叠会增加 overdraw。​

以下是几个简单的例子，说明操作会增加 overdraw 的数量：​

在一个游戏中，角色身后有一个墙壁，但墙壁的渲染仍然发生。即使墙壁对于玩家来说是不可见的，但由于渲染管道中的顺序，仍会发生对墙壁的渲染，导致 overdraw 的发生。​

减少 overdraw 的方法通常包括使用深度测试（depth testing）和剔除（culling）来排除不可见物体的渲染，优化渲染顺序以减少重复渲染，以及使用合理的材质和渲染技术来最小化 overdraw 的发生。这些优化可以提高图形渲染性能并减少资源的浪费。




## 14 实时渲染主流抗锯齿方法有哪些？至少举三个例子​

SSAA (超采样抗锯齿)：先渲染出一张 N 倍屏幕分辨率的图像，然后再通过降采样的方式缩小到屏幕分辨率。

MSAA (多重采样抗锯齿)：在光栅化阶段，对一个像素布置多个采样点，采样点越多越准确，性能消耗也越高。

TAA (时间抗锯齿) 利用前一帧和当前帧之间的图像信息进行插值和平滑处理，从而实现抗锯齿的效果。

FXAA (快速近似抗锯齿): 基于后处理的抗锯齿，通过边缘检测并对边缘应用模糊
****
基于后处理和基于屏幕空间有哪些区别啊？后处理是基于图像的，屏幕空间是基于 Gbuffer 的（反正要拿到法线深度啥的）​


## DLSS
DLSS (深度学习超级采样) , 作用是通过降低游戏内的渲染分别率，同时再通过 AI 输出高分辨率图像。以此来提升帧数，并尽量减少画面损失




  
## 什么样的内容写在顶点着色器里？什么样的内容写在片元着色器里？
顶点着色器：
1. 处理输入的顶点数据，如顶点位置、法线、uv 等
2. 对顶点进行空间变换、位置变换
3. 顶点动画

片元着色器：
1. 光照、阴影计算
2. 纹理采样
3. 其他各种效果几乎都是在片元着色器完成
4. 当光照模型中有非线性的计算（如计算高光反射），逐顶点光照会出问题，只能选择逐像素光照。

总的来说，顶点着色器主要负责处理顶点级别的数据和计算，而片元着色器则负责处理像素级别的数据和计算。它们共同工作以完成图形渲染管线中的渲染过程。


## 顶点着色器和片元着色器性能比较？

1. **计算负载**：
    - 由于模型顶点数目往往远小于像素数目，片元着色器的计算比顶点着色器更复杂，因此顶点着色器的计算量往往小于片元着色器
2. 内存带宽：
    - 片元着色器通常对内存带宽的需求更高，因为它们需要从纹理和帧缓冲区等存储器中读取大量数据。

## 法线是什么？为什么法线贴图会偏蓝？
法线是垂直于平面的直线

偏蓝的是切线空间的法线贴图，这种法线纹理其实存储了**每个点在各自的切线空间中的法线偏移（扰动）方向。**

如果一个点的法线方向不变，那么在其切线空间中，新的法线方向就是 z 轴方向。即值为（0,0,1），映射到颜色即（0.5,0.5,1)浅蓝色。蓝色说明顶点的大部分 shader 法线和模型本身法线一样，无需改变.

![[03 光照向量#读取法线贴图数据]]

# 光照模型


## 基础光照模型，Phong 模型和 Blinn-Phong 模型区别？
![[Pasted image 20240304233905.png]]
Phong 光照模型和 BlinnPhong 光照模型主要区别在于计算高光的方式不同。

**phong**：
首先计算反射向量 $r$：
$$r=2(l\cdot n)n-l$$
$$
\mathbf{c}_{specular}=(c_{light}\cdot\mathbf{m}_{specular})\max(0,{\mathbf{v}}\cdot\mathbf{r})^{m_{gloss}}
$$

**BlinnPhong：**
引入半程向量，
计算半程向量：
$$
h=\frac{v+l}{||v+l||}
$$

使用们 $n$ 和 $h$ 之间的夹角进行计算：
$$
c_{specular}=\left(\mathbf{c}_{light}\cdot\mathbf{m}_{specular}\right)\max\left(0,{n}\cdot{h}\right)^{m_{gloss}}
$$


**外观对比：** BlinnPhong 高光更平滑 ![[Pasted image 20240304233735.png]]


**性能对比：**
- 在硬件实现时, 如果摄像机和光源距离模型足够远，此时可以认为 $v$ 和 $l$ 都是定值, 因此 $h$ 将是一个常量。即入射方向和视线方向保持不变，半角向量可以只计算一次并重用。此外，**半角向量的计算比反射方向的计算量更少**。这时 BlinnPhong 模型性能优于 Phong 模型。 
- 但是，当 $v$ 和 $l$  不是定值时, Phong 模型计算量更少一些。





# 色彩相关
## 18 对色彩空间的理解？

**色彩模型**是用一定规则来描述颜色的方法。例如：RGB 模型用规定红、绿、蓝 3 个分量描述颜色，然而并没有确定红色、绿色、蓝色对应的波长、亮度等信息

这就需要引入**色彩空间**，以确切地描述色彩模型的具体信息和含义。

**一个色彩模型下可以有多个不同的色彩空间**

## 19 HDR 是什么？
HDR 即高动态范围，在传统的 8 位图像中，每个通道的亮度范围是 0 到 255，亮度和颜色的细节可能会在亮度范围有限的情况下丢失或过曝。
HDR 图像会使用至少 16 位整数或浮点数来表示每个通道的颜色值，这样可以提供更多的亮度级别，从而更好地保留亮度和颜色的细节。
HDR 通常与后处理技术结合使用
# 15 渲染管线相关
## ⭐介绍下渲染管线
在计算机图形学中，我们使用渲染管线来实现将 3D 场景转换成 2D 图像的过程。如果给出一台具有确定位置和朝向的摄像机以及某个场景的几何描述，那么渲染管线则是以摄像机为视角进行观察，并据此生成给定 3D 场景对应的 2D 图像的一整套处理步骤。

实时渲染管线一般按功能性阶段分为四个阶段：**应用程序阶段 (Application)**、**几何处理阶段 (Geometry Processing)**、**光栅化阶段 (Rasterization)** 和**像素处理阶段 (Pixel Processing)**。
![](1683366278896.png)
**一、应用程序阶段**会从显存中读取几何数据 (顶点和索引)，再将它装配为**几何图元**传递给几何处理阶段。

**二、几何处理阶段**可以细分多个子阶段：
1. **顶点着色器阶段 (Vertex Shading)**，
    1. 计算顶点位置，通过 MV 矩阵从模型空间变换到观察空间
    2. 传递插值的数据，如法线和纹理坐标
2. **可选阶段**
    1. **曲面细分阶段**（tessellation Stage），利用镶嵌化处理技术对网格中三角形进行**细分 (subdivide)**，以此来增加物体表面上的三角形数量。
    2. **几何着色阶段**（geometry shading），将输入的单个图元，通过配置输出一个或多个图元
    3. **流输出阶段**（stream output）。将顶点数据输出至内存并可再次读回
3. **投影阶段 (Projection)**，透视投影或正交投影，
4. **裁剪阶段 (Clipping)** 剔除掉不在视椎体范围内的顶点，将顶点变换到齐次裁剪空间
5. **屏幕映射阶段 (Screen Mapping)**，将顶点变换到屏幕空间

**三、光栅化阶段：**

- 光栅化阶段分为两个子阶段：**三角形设置**和**三角形遍历**。
    - 三角形设置阶段，计算三角形网格信息，例如三角形顶点坐标和边界表达式。
    - 三角形遍历阶段，检查像素是否在三角形内，如果在的话就会生成一个**片元**

**四、像素处理阶段**
- 像素处理阶段可以分为两个子阶段：**像素着色阶段** 和**合并阶段**。
    - 像素着色阶段，使用光栅化阶段传递的插值后的数据以及纹理计算像素颜色，将计算的颜色传递给合并阶段。
    - 合并阶段，首先通过模板测试，深度测试等一系列测试决定每个片元的可见性，如果一个片元通过了所有的测试，就需要把这个片元的颜色值和已经存储在颜色缓冲区中的颜色进行混合。
        - 合并阶段流程：**像素所有权测试→裁剪测试→透明度测试→模板测试→深度测试→透明度混合
## 前向渲染和延迟渲染的区别
前向渲染会遍历计算每个模型进行光照计算，然后进行深度测试并输出渲染结果。

延迟渲染是将**着色（Shading）** 延迟到深度计算之后进行处理的一种渲染方法。首先会将场景中所有物体的片元信息通过 MRT 技术绘制到 GBuffer 中，最后根据 gbuffer 中的信息进行光照计算。

延迟渲染管线和前向渲染管线的差异如下​
- 后处理方式方面：前向渲染需要额外渲染深度图，而延迟渲染可以直接使用已有的深度图。​
- 着色计算方面：前向渲染可以对不同物体使用不同的光照模型，​延迟渲染因为是最后统一计算光照的，所以只能用一个光照模型（如果需要其他光照模型，只能切换 pass）
- 抗锯齿方面：延迟渲染对 MSAA 算法不友好，无法对已降采样的片元增加采样点实现抗锯齿。、
- 性能方面：前向渲染管线在多光源场景下性能较差，延迟渲染的 Gbuffer 占用较多显存带宽对移动端不友好。

## 前向渲染和延迟渲染的优劣

**前向渲染**
- **优点**
    -   支持半透明渲染
    -   支持使用多个光照 pass
    -   支持自定义光照计算方式
- **缺点**
    -  先光照计算，再深度测试，会渲染不可见像素，产生大量 overdraw
    -  光源数量对计算复杂度影响巨大
    -  访问深度等数据需要额外计算（需要再渲染一张深度图）

**延迟渲染**
- **优点**
    -  大量光照场景的情况下，优势明显
    -  只渲染可见像素，节省计算量
    -  对后处理支持良好（例如深度信息：直接拿 G-buffer 中的就行）
    -  用更少的 shader（所有的物体光照模型都一样，很多东西不用再定义了）

- **缺点**
    -   对 MSAA 支持不友好，无法对已降采样的片元增加采样点实现抗锯齿。
    -   对于透明渲染不友好，因为透明物体和不透明物体渲染方式不一样，透明物体不能在 Gbuffer 阶段画，得需要单独的 Pass 来绘制。
    -   GBuffer 占用大量的显存带宽
    -   只能用一个光照模型（如果需要其他光照模型，只能切换 pass）

# 16 深度测试相关
## 渲染管线中的深度测试是什么？在什么情况下物体可以不需要进行深度测试

深度测试是一种用于确定像素是否应该被绘制的技术。深度测试通过比较当前像素的深度值与已存储在深度缓冲区中的对应位置的深度值来进行判断。​

有些需求是物体被遮挡后也要渲染出来，比如 UI 和后处理就不需要进行深度测试

## Early-Z
像素处理阶段可以分为两个子阶段：**像素着色阶段** 和**合并阶段**。
- 像素着色阶段，使用光栅化阶段传递的插值后的数据以及纹理计算像素颜色，将计算的颜色传递给合并阶段。
- 合并阶段，首先通过模板测试，深度测试决定每个片元的可见性，如果一个片元通过了所有的测试，就需要把这个片元的颜色值和已经存储在颜色缓冲区中的颜色进行混合。

传统方式下会先计算光照再进行深度测试，产生大量 overdraw。

原理：**在光栅化阶段（三角形遍历阶段）之后、像素处理阶段之前**，加入一个 Early-Z 阶段，提前进行深度测试，减少了 overdraw
## Pre-Z
配合 Early-Z 使用，将场景做两个 pass 的绘制。
- 第一个 pass 仅写入深度，不做任何复杂的片元计算，不输出任何颜色（这样后面即使丢弃了片元，也没有很大开销）。
- 第二个 pass**关闭深度写入**，并将**深度比较函数设为“相等”。

## 缓解 Z-fighting 的方法
[[06 深度测试#Z-Fight 深度冲突]]

# 纹理相关
## 1 纹理映射的两种方式
1. 投影：对每个光栅化的屏幕坐标算出它的 uv 坐标 (利用三角形顶点重心坐标插值)，再利用这个 uv 坐标采样纹理。
![[1684548373611.png]]

1. UV Mapping：在实时渲染中，通常是将 uv 坐标保存在模型顶点信息中，在三角形内使用时，通过插值的方式得到每个片元具体的 uv 坐标，再从纹理中采样获得对应的值。
![[1684548373674.png]]
## 2 什么是 uv？uv 是以什么方式采样图片的？unity 和 ue 的 uv 采样有什么不一样？​

- UV 是一套通过二维坐标系映射到模型三维表面的解决方案，通常用于贴图采样空间。​
- UV 通过坐标映射将图片不同位置的像素映射到模型表面。​
- Unity 的 UV 是 OpenGL 坐标系下的，uv 左下角为原点。UE4 的 UV 是 DirectX 坐标系下的，uv 左上角为原点。​

## 3. 纹理分辨率过低引发的问题
**产生走样（很好理解，贴图分辨率低就会有锯齿）**


解决办法：双线性插值，可以考虑四个纹素的颜色
![[Pasted image 20230520102532.png]]

## 纹理分辨率过高引发的问题
**导致远处图像失真（摩尔纹）**


当三角形面距离相机较远时，1 个像素点就要代表原来 10 个像素点的颜色信息，导致失真
![[Pasted image 20230520103718.png]]

解决办法： mipmap（多级渐远纹理）

## mipmap 了解吗？GPU 是怎么决定采第几层的 mipmap？
由于透视投影，不同的屏幕像素所对应的纹理覆盖面积（footprint） 大小是不一样大小的，通过将纹理分成不同等级的精度，近处采样精度大的，远处采样精度小的。
![[1684499674924.png]]
>mipmap 比会增加原本图像 1/3 的额外存储量

level 0 代表的是原始 texture，也是精度最高的纹理，随着 level 的提升，每提升一级将 4 个相邻像素点求均值合为一个像素点。

**GPU 中的计算方法**：在屏幕空间中计算出当前像素点与右方像素点和上方像素点在纹理空间的距离，二者取最大值，再取对数即可得到 D (D 表示 Level 等级)
![[Pasted image 20230520111345.png]]

结果远处仍有模糊（Overblur），这就需要通过各向异性过滤来处理。
![[1684499675124.png]]
## 各向异性过滤

**产生 Overblur 的原因是因为，所采用的不同 level 的 Mipmap 默认的都是屏幕空间正方形区域的 Range Query**。但事实情况如图：在纹理空间对应各种不规则图形
![[1684499675236.png]]

有的所需要的是仅仅是水平方向的高 level，有的需要的仅仅是竖直方向上的高 level，因此这也就启发了各向异性的过滤:

![[1684499675290.png]]
>额外的 3 倍显存开销
>从左往右看，宽度从原长逐渐缩小逼近 0
>从上往下看，高度从原长逐渐缩小逼近 0

**各向异性过滤只能解决水平或竖直的不同大小的矩形覆盖面积（footprint），并不能解决斜向的覆盖面积（footprint）。**
![[Pasted image 20230520112744.png|323]]

解决方法：EWA 过滤
![[Pasted image 20230520112852.png|400]]
EWA 过滤就是把斜着的图形拆成很多个圆形去覆盖不规则形状，每一次就查询一个圆形，然后多次查询，自然就可以覆盖这个不规则的形状，得到好的结果。缺点是开销较大
## 3 贴图压缩有哪几种方式？简单讲解其中一种的原理​

贴图压缩是一种用于减小纹理贴图文件大小的技术，以节省存储空间和提高渲染性能。以下是几种常见的贴图压缩方式：​

1. DXT（也称为 S3TC）压缩：DXT 是一种基于固定颜色调色板和压缩算法的纹理压缩格式。它将纹理图像分为 4x4 像素块，并为每个块选择一个颜色调色板和一个压缩算法。DXT 压缩算法使用颜色查找表和差值技术来减少颜色精度，并使用基于块的压缩算法进一步减小数据量。由于使用了固定调色板和压缩算法，DXT 压缩可以提供较高的压缩比例，但可能会引入一些压缩损失。​
2. ETC（Ericsson Texture Compression）压缩：ETC 是一种基于块的纹理压缩格式，常用于移动设备和游戏开发。它使用基于 YCoCg 颜色空间的压缩算法，将纹理图像分成 4x4 像素块，并对每个块进行压缩。ETC 压缩算法在保持良好视觉质量的同时，采用了一系列的压缩技术，如颜色查找表、变换和差值来减小数据量。​
3. ASTC（Adaptive Scalable Texture Compression）压缩：ASTC 是一种可扩展的纹理压缩格式，支持从低到高的压缩比例和质量范围。它使用基于块的压缩算法，可以根据纹理的特征和要求来自适应地选择压缩模式和位数。ASTC 可以提供更高的图像质量和更高的压缩比，同时还支持透明度压缩和 HDR（高动态范围）图像的压缩。​

其中，以 DXT 压缩为例进行简单讲解。DXT 压缩使用基于块的压缩方法，将纹理图像划分为 4x4 像素块。对于每个块，DXT 压缩算法会选择一个适合的颜色调色板，并根据该调色板对每个像素进行颜色压缩。​

DXT 压缩使用颜色查找表（color lookup table）来减小颜色精度。通过将颜色值映射到调色板上最接近的颜色，可以使用较少的位数来表示颜色。这样可以减小纹理数据的大小，但可能会引入一些

ETC 和 ASTC 的区别： ASTC 是自适应压缩​


# 7. 阴影
## shadow map 的实现原理​
传统实现分两步：
- 1. 从光源视角生成场景的 DepthTexture，作为shadowmap
- 2. 将摄像机空间可见的位置转换到光源空间，将观察点在光源空间的深度与 DepthTexture 比较，如果深度>DepthTexture 中的深度，则认为在阴影中。

Unity 中实现了基于屏幕空间的 shadow mapping
![[09 阴影#Unity 中的屏幕空间阴影映射]]


## 平行光的 shadowmap 和点光源的 shadowmap 的区别？
平行光因为方向是单向，所以可以使用一张 2D 的纹理作为 shadowmap。

而点光源方向是向球形的，做法和传统的 shadowmap 有区别，采用了 Omnidirectional Shadow Maps（OSM, 全方向的阴影贴图）技术，使用了 cubemap 纹理。


## 阴影走样及其解决办法
[[09 阴影#自阴影/阴影痤疮]]

[[09 阴影#阴影走样]]


# ⭐ PBR 相关

## 解释下 PBR？
**基于物理的渲染**（Physically Based Rendering） 是指使用基于物理原理和微表面理论建模的着色模型，以及使用从现实中测量的表面参数来准确表示真实世界材质的渲染理念。

**三大组成部分：**
1. **基于物理的材质**（Material）
2. 基于物理的光照（Lighting）
3. 基于物理的摄像机（Camera）

基于物理的材质需要满足三大条件：
*   基于微表面理论的表面模型
*   能量守恒
*   使用基于物理的双向反射分布函数 BRDF

## PBR 材质有哪些贴图？
金属流和高光流用的贴图不同。如图：
- 金属流：BaseColor、roughness、metallic、AO、normal、Height
- 高光流：Diffuse、glossiness、specular、AO、normal、Height
![[Pasted image 20240304211806.png]]


金属流和高光流

![[Pasted image 20240304211734.png]]
## 什么是 brdf，简单介绍一下 brdf 方程
答题思路：理解 BRDF 应该就很好作答，主要回答出 DFG 和三个函数的意义就行​

双向反射分布函数是用来描述光线从不同方向入射后，其反射光线的分布情况。具体来说，BRDF 为朝某个方向发出反射光辐射率 radiance 与入射光辐照度 irrandiance 的比值。
- 辐照度：通过单位面积的辐射通量
- 辐射率：通过单位面积单位立体角的辐射通量（半球向量所有立体角上的辐射率相加等于辐照度）


**实时**渲染管线通常使用是 **Cook-Torrance BRDF**。Cook-Torrance BRDF 分为漫反射和高光反射两个部分：
$$f_r = k_d f_{lambert} + k_s f_{cook-torrance}$$

- $k_d$ ：漫反射比例
- $k_s$ ：高光反射比例
- $f_{lambert}$ ：漫反射部分是一个恒等式，表示了一个立体角方向的漫反射颜色
$$f_{lambert} = \frac{c}{\pi}$$
其中 $c$ 代表的是反射率 Albedo 或漫反射颜色颜色
（**为什么要除以** $\pi$ ？因为 $\cos\theta$ 在半球上的积分等于 $\pi$： $\int_{2\pi}cos\theta_od\omega_o=\pi$ ，如果不除 $\pi$ 表示的是反射到半球方向的总能量，而我们眼睛看到的是一个立体角方向，所以需要除π。**）

- BRDF 的高光反射部分更复杂：
$$f_{cook-torrance} = \frac{DFG}{4(\omega_o \cdot n)(\omega_i \cdot n)}$$

Cook-Torrance 镜面反射 BRDF 由 3 个函数（$D$，$F$，$G$）和一个标准化因子构成。$D$，$F$，$G$ 符号各自近似模拟了特定部分的表面反射属性：

*   **$D$ (Normal Distribution Function，NDF)**：法线分布函数，描述的是微表面的法线方向与半角向量对齐的概率，如果对齐那么认为该反射光可以被看到，否则没有。这是用来估算微表面的主要函数。
*   **$F$ (Fresnel equation)**：菲涅尔方程，描述的是在不同的表面角下表面反射的光线所占的比率。
*   **$G$ (Geometry function)**：几何函数，描述了微表面自成阴影的属性。当一个平面相对比较粗糙的时候，平面表面上的微表面有可能挡住其他的微表面从而减少表面所反射的光线。

##  PBR 实现
[[02 PBR理论#3 PBR 实现]]

![[Pasted image 20221102144938.png]]

# 3D 数学
## 平面点到直线的距离

已知直线 $l$ 的方程为 $ax+by+c=0$，平面上任意一点 $(x_0,y_0)$ 到该直线的距离 $d$ 的公式为：

$$d=\frac{|ax_0+by_0+c|}{\sqrt{a^2+b^2}} $$
## 空间中点到直线的距离
![[v2-127adea7890e30e2c3afade4aa70c9b6_1440w.webp]]

## 空间中点到面的距离

已知平面 $p$ 的方程为 $ax+by+cz+d=0$，空间任意一点 $(x_0,y_0,z_0)$ 到该平面的距离 $d$ 的公式为：

$$d=\frac{|ax_0+by_0+cz_0+d|}{\sqrt{a^2+b^2+c^2}} $$

## 空间中两条直线的距离
![[v2-19d31c56eab31439c88dd2b3a2088fdc_1440w.webp]]
平行六面体的体积公式：
1. 体积是底面积 A 与高 h 的乘积： $V=Ah$
2. 以 abc 向量为棱的平行六面体的体积为：（注意是点乘和叉乘）
$$V=|\mathbf{a}\cdot (\mathbf{b}\times\mathbf{c})|=|\mathbf{b}\cdot (\mathbf{c}\times\mathbf{a})|=|\mathbf{c}\cdot (\mathbf{a}\times\mathbf{b})|$$


向量 $AS_1,AB,BS_2$ 是所组成的平行六面体的三条棱，我们要求的距离是垂线 d
即
$$|\mathbf{AS_1}\cdot (\mathbf{BS_2}\times\mathbf{AB})|=|AS_{1}\times BS_2|h$$
这样就可以求出h

## 求交点
### 射线和球面求交
![[v2-ef68c883078f755c5c14d33185b7776f_720w.webp|500]]
计算交点 K
由图可知，焦点 K 满足 ${\vec{SK}}\cdot{\vec{SK}}=R^{2}$

$K$ 在射线上可以表示为： $K=O+t\vec{D}$

带入得：$(O+t\vec{D}-S)\cdot (O+t\vec{D}-S)=R^2$

展开后是关于 t 得二次方程：
$$
t^{2}\vec{D}\cdot\vec{D}+2t\vec{D}\cdot(O-S)+(O-S)\cdot(O-S)-R^2=0
$$
根据 $t=\frac{-b\pm\sqrt{b^{2}-4ac}}{2a}$ 得 t 即可表示出 K

### 射线与平面求交

假设原点沿着平面法线方向投影到平面上的点为 P0，P（x，y，z）为平面上任意一点， N（A，B，C）为平面法线，因为平面上的任意向量和平面法线都是垂直关系，所以可以得到
$$(p-p_{0})\cdot N=0$$
因为点积满足加法分配律所以可得
$$
p\cdot N-p_0\cdot N=0
$$

设射线方程为 $f(t)=O+nt$

O 为射线原点，n 为射线方向单位向量。假设射线与平面交于 P 点，联立射线方程和平面方程可得
$$
(O+n*t)\cdot N-p_0\cdot N=0
$$

解出
$$
t=\frac{p_0\cdot N-O\cdot N}{n\cdot N}=\frac{(p_0-O)\cdot N}{n\cdot N}
$$

### 射线与三角形求交
假设射线方程为 $f(t)=O+dt$

其中 O 是射线原点，d 是射线的方向

对于三角形，假设已知三角形 ABC 的三个顶点分别为 V0、V1、V2。

设 v、u 分别是 V1、V2 的权重， 1-u-v 是 V0 的权重，所以三角形中任一点可以表示为（根据重心坐标插值）：
$$g(u,v)=(1-u-v)V_0+uV_1+vV_2$$
其中

$$u\geq0,v\geq0,1-u-v\geq 0$$

假设射线和三角形相交于 P 点（P=O+Dt），所以联立两方程可得
$$
O+D*t=(1-u-v)*V_0+u*V_1+v*V_2
$$
解出 t 即可
### 射线与 AABB 求交
![[03 光追理论#轴对齐包围盒 (AABB)]]


### 射线与球形包围盒相交

同理，计算 $t_离开$ 和 $t_进入$ 的关系

**给定一堆顶点数据代表物体，球形的包围盒该怎么算出来?**
根据所有顶点的 x，y，z 坐标的均值确定包围球的球心，
再由球心与三个 xyz 最大值坐标所确定的点间的距离确定半径 r。
### 射线和某个物体如何求交？
先判断包围盒是否求交，之后和每个三角形判断是否相交


## 齐次坐标 w的作用

![[3D数学#^qctphb]]

## 欧拉角
[[01 三维旋转#欧拉角]]

**一共有 3 种欧拉角：俯仰角 (Pitch)、偏航角 (Yaw)和滚转角 (Roll)**
inspector 界面上显示的 Rotation 的 XYZ 值都是欧拉角
Untiy 欧拉角常用顺规：YXZ（Yaw-pitch-Roll）

**使用欧拉角的两个缺点：**
1. 同一旋转表示不唯一，即欧拉角绕一个轴旋转 90° 和 450°结果是一样的
2. 顺规中间轴达到 90 度时会产生万向节死锁
**使用四元数可以解决这两个问题，四元数的旋转转换为欧拉角后可以发现对应的欧拉角范围为（-180~180），不会出现欧拉角的缺点一。**

## 什么是万向节死锁？​

万向节死锁是指在使用欧拉角进行旋转时可能出现的一种现象，比如坐标系的顺规为 xyz ，在绕顺规的中间坐标轴（即 y 轴）旋转 90° 时，另外两个旋转轴发生重合，失去了一个自由度，xz 无法单独旋转，因为它们共享相同的旋转轴 y。​

万向节死锁无法避免，**游戏开发中可以根据需要将最不可能发生 90 度旋转的轴放在顺规的中间次序**。四元数可以解决这一问题

## 四元数
[[01 三维旋转#四元数]]
**四元数构成**
一个四元数包含一个标量和一个 3D 向量 $[ v,w]$
其中 $v$  为 3D 向量, $w$ 为标量，即 $[(x, y, z),w]$

**对于给定的任意一个四元数: 表示 3D 空间中的一个旋转量**

> [!NOTE] 轴-角对
> 在 3D 空间中，任意旋转都可以表示绕着某个轴旋转一个旋转角得到
> 注意: 该轴是**局部空间**中的**任意一个轴**

对于给定旋转，假设为绕着 $n$ 轴，旋转 $β$ 度，$n$ 轴为 $(x, y, z)$ 那么可以构成四元数为
四元数 $Q= [\sin (β/2)*n,\cos (β/2)]$
四元数 $Q= [ \sin (β/2) *x, \sin (β/2) *y, \sin (β/2) *z,\cos (β/2)]$
**四元数 $Q$ 则表示绕着轴 $n$，旋转 $β$ 度的旋转量


# GAMES101:
## 齐次裁剪为什么要在透视除法前？
[[02 空间变换#^xu4mwq]] 提到透视投影后，裁剪空间的 $w$ 值=观察空间深度值 $z$。如果观察空间深度值为 $z=0$，那么 $w=0$，透视除法除以 $w$ 就会发生除零错误。所以在透视除法之前必然要把这样的点裁剪掉，也就是透视除法之前做裁剪。

# unityshader

## shader 的代码结构是怎样的？
```c
//1. Shader 程序的第一行代码用来声明该 Shader 的名称以及所在路径。
Shader "Custom/SimpleColor"
{
    //2. 属性值（数值、颜色和向量、纹理贴图）
    Properties
    {
        _MainTex ("MainTex", 2D) = "white" {}
        _BaseColor("BaseColor", Color) = (1,1,1,1)
    }
    
    //3. 可以编写多个子着色器（SubShader），但至少需要一个。
    //在应用程序运行过程中，GPU 会先检测第一个子着色器能否正常运行，如果不能正常运行就会再检测第二个，以此类推。
    SubShader
    {
        //4. 通过Tag确定什么时候以及如何对物体进行渲染
        Tags
        {
            "RenderPipeline" = "UniversalPipeline"
            "RenderType"="Opaque"
        }

        //5. 着色器代码块HLSLINCLUDE和ENDHLSL：用来添加着色器程序
        HLSLINCLUDE
        #include "Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl"

        //6. CBUFFER_START 和 CBUFFER_END 宏定义用于创建一个常量缓冲区块，并指定其中包含的数据成员
        CBUFFER_START(UnityPerMaterial)
        float4 _BaseColor;
        float4 _MainTex_ST;
        CBUFFER_END

        TEXTURE2D(_MainTex);
        SAMPLER(sampler_MainTex);

        //7. 顶点着色器输入结构体
        struct Attributes
        {
            float4 positionOS : POSITION;
            float2 uv : TEXCOORD0;
        };
        
        //8. 顶点着色器输出结构体
        struct Varyings
        {
            float4 positionCS : SV_POSITION;
            float2 uv : TEXCOORD0;

        };
        ENDHLSL

        //9. 每个子着色器中还会包含一个甚至多个 Pass
        Pass
        {
            //10. 每个Pass能单独指定其Tag
            Tags
            {
                "LightMode" = "UniversalForward"
            }

            //11. 着色器代码块HLSLPROGRAM和ENDHLSL：用来添加着色器程序
            HLSLPROGRAM
            #pragma vertex vert
            #pragma fragment frag

            //12. 顶点着色器
            Varyings vert(Attributes i)
            {
                Varyings o = (Varyings)0;

                o.positionCS = TransformObjectToHClip(i.positionOS.xyz);
                o.uv =i.uv.xy * _MainTex_ST.xy + _MainTex_ST.zw;
                
                return o;
            }

            //13. 片元着色器
            float4 frag(Varyings i) : SV_Target
            {
                float4 MainTex = SAMPLE_TEXTURE2D(_MainTex, sampler_MainTex, i.uv);
                float4 finalColor = MainTex * _BaseColor;
                return finalColor;
            }
            ENDHLSL
        }
    }
    
    //假如当前 GPU 的硬件版本太旧，以至于所有的子着色器都无法正常运行时，则执行最后的回退（Fallback）命令，运行指定的一个基础着色器。
    FallBack "Packages/com.unity.render-pipelines.universal/FallbackError"
}
```
## shader 中为什么要避免使用 if 语句
会打断 GPU 的并行化：for if 使 GPU 中的硬件单元不能只单纯的去做计算，还需要去做逻辑判断，导致不能对大规模数据的通过相同的逻辑结构做并行计算。GPU 不得不根据 for if 这种跳转语句构建多个流水线阶段，而在流水线之间需要根据临时计算的值，来做跳转。

## 从深度重建世界坐标
根据屏幕空间深度图重建世界坐标

**方法一：通过 VP 逆矩阵**

1. 获取 VP 逆矩阵
2. 用屏幕 uv 采样深度纹理得到像素的非线性深度
3. 计算 NDC 空间坐标 
$$
\begin{array}{lcr}P_{ndc}.x=2*u-1\\ P_{ndc}.y=2*v-1\\ P_{ndc}.z=2*NonlinearDepth-1\\ P_{ndc}.w=1.0\end{array}
$$
4. 根据下式计算世界坐标 $$P_{world}=\frac{M^{-1}P_{ndc}}{{(M^{-1}P_{ndc}).w}}$$
其中 $M^{-1}$ 是 VP 逆矩阵


shader 写法：
```c
//1 脚本获取VP逆矩阵
Matrix4x4 ViewProjectionMatrix = renderingData.cameraData.camera.projectionMatrix * renderingData.cameraData.camera.worldToCameraMatrix;  

Matrix4x4 ViewProjectionInverseMatrix = currentViewProjectionMatrix.inverse;  
m_blitMaterial.SetMatrix("_ViewProjectionInverseMatrix", ViewProjectionInverseMatrix);

//2 片元着色器中计算
//获取屏幕空间UV
float2 ScreenUV = GetNormalizedScreenSpaceUV(i.positionCS);

//用屏幕UV采样屏幕深度纹理得到像素的非线性深度
float depth = SAMPLE_DEPTH_TEXTURE(_CameraDepthTexture, sampler_CameraDepthTexture, ScreenUV).r;

// NDC空间坐标
float4 currentPosNDC = float4(ScreenUV.x * 2 - 1, ScreenUV.y * 2 - 1, depth * 2 - 1, 1);

//得到世界空间坐标
float4 D = mul(_ViewProjectionInverseMatrix, currentPosNDC);
float4 currentPosWS = D / D.w;
```

**方法二：使用摄像机构建**
使用 VP 逆矩阵的方法需要在片元着色器中进行矩阵乘法，通常会影响性能。

## 图像模糊算法
### 高斯模糊
根据高斯方程构建高斯卷积核：
$\frac1{256}\cdot\begin{bmatrix} 1&4&6&4&1\\ 4&16&24&16&4\\ 6&24&36&24&6\\ 4&16&24&16&4\\ 1&4&6&4&1 \end{bmatrix}$

图像中的每个像素被乘以高斯核，然后将所有这些值相加，得到输出图像中此处的值。

 - @ 高斯核特性：
1. **高斯核满足线性可分**（Linearly separable）：
$$二维高斯核变换=水平方向一维高斯核变换+竖直方向一维高斯核变$$
**这样只需要 $O(n\times M\times N)+O(m\times M\times N)$ 的计算复杂度，而原先的计算复杂度为 ${\displaystyle O(m\times n\times M\times N)}$** ，其中 $M, N$ 是需要进行滤波的图像的维数，$m、n$ 是滤波器的维数。

以下为一个高斯核的线性分解过程：

$\frac1{256}\cdot\begin{bmatrix} 1&4&6&4&1\\ 4&16&24&16&4\\ 6&24&36&24&6\\ 4&16&24&16&4\\ 1&4&6&4&1 \end{bmatrix} = \frac1{256}\cdot\begin{bmatrix} 1\\4\\6\\4\\1 \end{bmatrix}\cdot\begin{bmatrix} 1&4&6&4&1 \end{bmatrix}$

2. **高斯核具有对称性**，因此两个一维高斯核中包含了很多重复的权重。**对于一个大小为 5 的一维高斯核，我们实际只需要记录 3 个权重值即可。**

**实现思路：** 
1. shader 部分分两个 pass
    1. 第一个 Pass 使用使用竖直方向的一维高斯核对图像进行滤波。**具体来说就是在顶点着色器将当前顶点的 uv，和上面两个和下面两个偏移的 uv 共个 uv 坐标存入 uv 数组。在片元着色器中对这个 uv 分别采样，并分别乘以不同的权重，最终将颜色相加。**
    2. 第二个 Pass 再使用再使用水平方向的一维高斯核对图像进行滤波。具体操作和步骤一相同。
2. 在 RenderPass 中用乒乓 RT 相互 blit 的方法，多次迭代。

```c++
Shader "URPPostProcessing/Blur/GaussianBlur"
{
    Properties {}

    SubShader
    {
        Tags
        {
            "RenderPipeline" = "UniversalPipeline"
            "RenderType"="Opaque"
        }

        LOD 100
        ZWrite Off Cull Off

        HLSLINCLUDE
        #include "Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl"

        CBUFFER_START(UnityPerMaterial)

        CBUFFER_END

        TEXTURE2D_X(_BlitTexture);
        SAMPLER(sampler_BlitTexture);
        float _BlurOffset;
        float4 _BlitTexture_TexelSize;

        struct Attributes
        {
            uint vertexID : SV_VertexID;
        };

        struct Varyings
        {
            float4 positionCS : SV_POSITION;
            float2 uv[5] : TEXCOORD0;
        };
        ENDHLSL

        Pass
        {
            Name "GaussianBlurY"
            Tags
            {
                "LightMode" = "UniversalForward"
            }

            HLSLPROGRAM
            #pragma vertex vert
            #pragma fragment frag

            Varyings vert(Attributes i)
            {
                Varyings o = (Varyings)0;

                o.positionCS = GetFullScreenTriangleVertexPosition(i.vertexID);

                //一个5x5的二维高斯核可以拆分为两个大小为5的一维高斯核
                //因此我们只需要计算5个纹理坐标即可

                //当前采样纹理
                float2 uv = GetFullScreenTriangleTexCoord(i.vertexID);
                o.uv[0] = uv;

                //邻域采样纹理，_BlurOffset控制采样距离
                o.uv[1] = uv + float2(0.0, _BlitTexture_TexelSize.y * 1.0) * _BlurOffset; //上1
                o.uv[2] = uv + float2(0.0, _BlitTexture_TexelSize.y * -1.0) * _BlurOffset; //下1
                o.uv[3] = uv + float2(0.0, _BlitTexture_TexelSize.y * 2.0) * _BlurOffset; //上2
                o.uv[4] = uv + float2(0.0, _BlitTexture_TexelSize.y * -2.0) * _BlurOffset; //下2

                return o;
            }

            float4 frag(Varyings i) : SV_Target
            {
                //采样并乘高斯核权重
                float4 sum = SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_BlitTexture, i.uv[0]) * 0.4026;
                sum += SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_BlitTexture, i.uv[1]) * 0.2442;
                sum += SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_BlitTexture, i.uv[2]) * 0.2442;
                sum += SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_BlitTexture, i.uv[3]) * 0.0545;
                sum += SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_BlitTexture, i.uv[4]) * 0.0545;

                return sum;
            }
            ENDHLSL
        }

        Pass
        {
            Name "GaussianBlurX"
            Tags
            {
                "LightMode" = "UniversalForward"
            }

            HLSLPROGRAM
            #pragma vertex vert
            #pragma fragment frag

            Varyings vert(Attributes i)
            {
                Varyings o = (Varyings)0;
                o.positionCS = GetFullScreenTriangleVertexPosition(i.vertexID);
                float2 uv = GetFullScreenTriangleTexCoord(i.vertexID);
                o.uv[0] = uv;
                o.uv[1] = uv + float2(_BlitTexture_TexelSize.x * 1.0, 0.0) * _BlurOffset; //左一
                o.uv[2] = uv + float2(_BlitTexture_TexelSize.x * -1.0, 0.0) * _BlurOffset; //右1
                o.uv[3] = uv + float2(_BlitTexture_TexelSize.x * 2.0, 0.0) * _BlurOffset; //左2
                o.uv[4] = uv + float2(_BlitTexture_TexelSize.x * -2.0, 0.0) * _BlurOffset; //右2

                return o;
            }

            float4 frag(Varyings i) : SV_Target
            {
                float4 sum = SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_BlitTexture, i.uv[0]) * 0.4026;
                sum += SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_BlitTexture, i.uv[1]) * 0.2442;
                sum += SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_BlitTexture, i.uv[2]) * 0.2442;
                sum += SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_BlitTexture, i.uv[3]) * 0.0545;
                sum += SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_BlitTexture, i.uv[4]) * 0.0545;

                return sum;
            }
            ENDHLSL
        }
    }
    Fallback Off
}
```

### 其他算法实现思路
- 方框模糊（box blur）: 图像中的每个像素具有的值等于其邻近的像素的输入图像中的平均值，同样采用低通滤波，3 x 3 的 box blur 的滤波核如下：

$${\displaystyle {\frac {1}{9}}{\begin{bmatrix}1&1&1\\1&1&1\\1&1&1\end{bmatrix}}} $$
- Kawase 模糊（Kawase Blur）：对距离当前像素<mark style="background: #FF5582A6;">越来越远</mark>的地方对四个角进行采样，且在两个大小相等的纹理之间进行乒乓式的 blit。**创新点在于，采用了随迭代次数移动的采样位置**。
![[ab5d93ebb5e83d884f816efdcdf41b6b_MD5.png]]

- 双重模糊（Dual Blur）：**相较于 Kawase Blur 在两个大小相等的纹理之间进行乒乓 blit 的的思路，Dual Kawase Blur 的核心思路在于 <mark style="background: #FF5582A6;">blit 过程中进行降采样和升采样</mark>, 即对 RT 进行了降采样以及升采样。** 有更好的性能
![[4cf428fa94dc86dddc4c25bfd85980c4_MD5.jpg]]
- 径向模糊
1. 首先选取一个径向轴心（Radial Center）
2. 然后将每一个采样点的 uv 基于此径向轴心进行偏移（offset），并进行一定次数的迭代采样
3. 最终将采样得到的 RGB 值累加，并除以迭代次数。

### 运动模糊
- 累计缓冲
保存上一帧的渲染结果，不断把当前的 RT 叠加到之前的 RT，从而产生一种运动轨迹的视觉效果。


- 速度缓冲
从深度图重建世界坐标，当得到世界空间中的顶点坐标后，我们使用前一帧的 VP 矩阵对其进行变换，得到该位置在前一帧中的 NDC 坐标。然后，我们计算前一帧和当前帧的位置差，生成该像素的速度。
对 uv 采样时根据像素速度做一定的偏移

## 什么是 bloom？bloom 大致的实现思路是什么样的？​

Bloom 是一种屏幕后处理效果，使亮度较高的区域向周围扩散，产生柔和的光晕效果。

Bloom 由四个 Pass 实现​
1. Pass1：提取图像的高亮区域（使用 luminance 亮度公式）
2. Pass2，Pass3：分别实现实现竖直方向和水平方向的高斯模糊。
3. Pass4：将经过模糊处理的高亮区域与原始图像进行加法混合。

用径向模糊代替高斯模糊，模拟光线往某个方向扩散的效果，就能实现 GodRay 效果。

## Glitch 故障效果
**实现思路：
1. RGB 三通道采用不同的 uv 偏移值进行分别采样。
2. 一般而言，会在 RGB 三个颜色通道中，选取一**个通道采用原始 uv 值，另外两个通道进行 uv 抖动后再进行采样**。

可以用噪声函数实现抖动

## 全局雾效
基于屏幕后处理的全局雾效
关键：根据深度图来重建每个像素在世界空间下的位置。

在简单的雾效实现中，我们需要计算一个**雾效系数**$f$，作为混合原始颜色和雾的颜色的**混合系数**;
当给定距离 $z$ 后，$f$ 的计算公式分别如下:

- **Linear**：$d_{min}$ 和 $d_{max}$ 分别表示受雾影响的最小距离和最大距离
$$
f=\frac{d_{max}-\mid z\mid}{d_{max}-d_{min}}
$$
- **Exponential**：$d$ 是控制雾的浓度的参数
$$
f=e^{-d\cdot|z|}
$$
- **Exponential Squared**：$d$ 是控制雾的浓度的参数
$$
f=e^{-(d-|z|)^2}
$$

以**基于高度的线性雾**为例
当给定一点在**世界空间下的高度 $y$** 后，$f$ 的计算公式为：
$$
f=\frac{H_{end}-y}{H_{end}-H_{start}}
$$
$H_{far}$, 和 $H_{end}$ 分别表示受雾影响的起始高度和终止高度

很简单，我们要做的就是从深度重建世界坐标，取 y 坐标带入公式即可。Hstart 和 Hend 为我们在外部控制的参数。
```cs h:16,19
//1 用深度纹理和屏幕空间uv重建像素的世界空间位置
//屏幕空间uv
float2 ScreenUV = i.positionCS.xy / _ScaledScreenParams.xy;
//从深度纹理中采样深度
#if UNITY_REVERSED_Z
// 具有 REVERSED_Z 的平台（如 D3D）的情况。
//返回[1,0]的深度值
real depth = SampleSceneDepth(ScreenUV);
#else
// 没有 REVERSED_Z 的平台（如 OpenGL）的情况。
// 调整 Z 以匹配 OpenGL 的 NDC([-1, 1])
real depth = lerp(UNITY_NEAR_CLIP_VALUE, 1, SampleSceneDepth(ScreenUV));
#endif

// 重建世界空间位置
float3 rebuildPosWS = ComputeWorldSpacePosition(ScreenUV, depth, UNITY_MATRIX_I_VP);

//2 计算雾
float fogDensity = (_FogEnd-rebuildPosWS.y)/(_FogEnd-_FogStart);
fogDensity = saturate(fogDensity*_FogDensity);
float4 color = SAMPLE_TEXTURE2D(_BlitTexture, sampler_BlitTexture, i.uv);
color.rgb = lerp(color.rgb, _FogColor, fogDensity);
```

**基于深度的线性雾**，需要将深度图中的非线性深度转换成线性深度，然后带入公式即可
```cs h:9,12
//采样深度图，转换为线性深度
float2 ScreenUV = i.positionCS.xy / _ScaledScreenParams.xy;
#if UNITY_REVERSED_Z
float depth = SampleSceneDepth(ScreenUV);
#else
float depth = lerp(UNITY_NEAR_CLIP_VALUE, 1, SampleSceneDepth(ScreenUV));
#endif

float linearDepth = LinearEyeDepth(depth, _ZBufferParams);

//计算雾的密度
float fogDensity = (linearDepth-_FogStart)/(_FogEnd-_FogStart);
fogDensity = saturate(fogDensity*_FogDensity);
float4 color = SAMPLE_TEXTURE2D(_BlitTexture, sampler_BlitTexture, i.uv);
color.rgb = lerp(color.rgb, _FogColor, fogDensity);

return color;
```
## 描边
**1 基于观察方向和表面法线**
通过观察方向 $V$ 和表面法线 $N$ 点乘结果来得到轮廓线信息。简单快速，但局限性大。
```c++
float NdotV = dot (N, V);
float color = step (_Edge, NdotV); //_Edge 越大线越粗，_Edge 大于 NdotV 返回0
```

**2 模板测试**
pass1：条件设置为 stencil 为 0 则通过，并将模板缓冲区+1
pass2：每个顶点沿法线外扩，然后进行同样的模板测试，只有外扩的部分模板缓冲区的值为 0 可以通过（渲染成描边颜色），内部部分不通过则不进行渲染。
缺陷：两个模型重叠部分没有描边，无解
![[Pasted image 20230726153538.png]]

**3 剔除**
   - 核心是两个 Pass：
      - 第一个 Pass 剔除正面，只渲染背面。进行顶点外扩。
      - 第二个 Pass 正常渲染正面。  

**4 后处理边缘检测**
- 使用高通滤波提取屏幕图像边缘，比如使用 Soble 算子
直接利用颜色信息进行边缘检测的方法会产生很多我们不希望得到的描边，如模型的纹理和阴影等位置也被描边。

- 优化：考虑颜色、法线向量和/或深度不连续性。在**深度法线纹理**上进行边缘检测，取对角方向的深度或法线，比较它们之间的差值，如果超过某个阈值（参数控制），就认为他们之间存在一条边。


## 水体渲染
[[卡通水]]
[[FFT代码实现]]

FFT 只要能讲述过程就可以：
[[FFT理论#4 公式的计算流程]]  

为什么使用 compute shader？
蝶形网络单个阶段的计算可以并行执行
# 软光栅项目
## 画线算法
### DDA 算法
digital differential analyzer

已知两点 $(x_1,y_1)$ 和 $(x_2,y_2)$
计算出步数：$step = abs(x2-x1)$
计算出步长（x, y 每走一步走多远）： $dlx=(x_2-x_1)/step,dly=(y_2-y_1)/step$

然后遍历步数，不断绘制像素点即可连成一条直线。
```c++
void DDAline(int x1, int y1, int x2, int y2, TGAImage& image, TGAColor color)
{
    float x = x1;
    float y = y1;
    float step = std::abs(x2 - x1); //步数,即绘制次数
    float dlx = (x2 - x1) / step;   //x步长
    float dly = (y2 - y1) / step;   //y步长

    for(int i = 1;i < step;i++)
    {
        image.set(x, y, color);
        x = x + dlx;
        y = y + dly;
    }
}
```

上述算法使用 $x$ 坐标的变化作为步长，但是当直线斜率绝对值大于 1 时，绘制出的值线会断掉（因为这时步长 dly 大于 dlx, 即 y 变化的比 x 快）。
解决方法也很简单，**绘制这种比较「陡峭」的直线时（斜率绝对值大于 1），以 y 的变化为基准，而不是以 x**，这样就可以避免上面直线不连续情况。
最后的直线算法就是这样：
```c++
void DDAline(int x1, int y1, int x2, int y2, TGAImage& image, TGAColor color)
{
    float x = x1;
    float y = y1;
    float dx = x2 - x1;
    float dy = y2 - y1;
    float step;
    float dlx, dly;
    // 根据 dx 和 dy 的长度决定基准
    if(std::abs(dx)>=std::abs(dy))
    {
        step = std::abs(dx);
    }
    else
    {
        step = std::abs(dy);
    }

    dlx = dx / step;   //x步长
    dly = dy / step;   //y步长

    for(int i = 1;i < step;i++)
    {
        image.set(x, y, color);
        x = x + dlx;
        y = y + dly;
    }
}
```

这就是 DDA 算法，但是它还有个很耗性能的问题：计算过程中涉及大量的**浮点运算**。Bresenham 画线算法就解决了这一问题，它的计算中不存在任何浮点数，甚至不存在整数的乘除法，只存在整数的加减位移运算。
### Bresenham 画线算法
Bresenham 画线算法的思想很简单，我们接下来只讨论在第一象限，斜率存在且大于 0，起始点小于终止点的情况，其余情况可通过这种情况稍加变化而来。

若要绘制一条线，首先要提供这条线的起点 $(x_1,y_1)$ 和终点 $(x_2,y_2)$，这两个点可以直接绘制出来，接着绘制两点之前其它的点。由两点可知，这条直线的斜率是 $k=\frac{y_2-y_1}{x2-x_1}$（看到这里出现的浮点数与乘除法不要着急，后续会化简让它消失）。

斜率 $k$ 的值意味着，$x=x+1$ 的时候，$y=y+k$。那么这个增加后的 $y$，它肯定与某单个像素的纵轴有一个交点，接下来让这个交点与这单个像素纵轴的中点（0.5）进行比较，大于等于中点，就规定这个点的实际绘制 $y$ 值加 1，否则不增加。

反复执行上一段的逻辑，直到绘制到终点，整条线绘制完毕。
![[v2-a3cd8fcefc14c0f0b97fa0f7aeb0b846_720w.webp]]
如上图所示，设起始点 $(x_1,y_1)$ 和终点 $(x_2,y_2)$，$y$ 的累计变化量为 $delta$，中点的累计变化量为 $middle$。

伪代码如下：
```c++
int nowX = x1;
int nowY = y1;
int k = (y2 - y1) / (x2 - x1);
int delta = 0;
int middle = 0.5;

drawPoint(x1, y1);

while(nowX != x2) {
    nowX += 1;
    delta += k;

    if(delta >= middle) {
        nowY += 1;
        middle += 1;
    }

    drawPoint(nowX, nowY);
}
```

接下来化简掉浮点数与乘除法，比如让 `k` 乘上它的分子，然后所有用到 `k` 的地方也统一变化。
```c++
int nowX = x1;
int nowY = y1;
int k = y2 - y1;
int delta = 0;
int middle = 0.5 * (x2 - x1);

drawPoint(x1, y1);

while(nowX != x2) {
    nowX += 1;
    delta += k;

    if(delta >= middle) {
        nowY += 1;
        middle += x2 - x1;
    }

    drawPoint(nowX, nowY);
}
```

然后再消掉 `0.5`。

```c++
int nowX = x1;
int nowY = y1;
int k = y2 - y1;
int delta = 0;
int middle = x2 - x1;

drawPoint(x1, y1);

while(nowX != x2) {
    nowX += 1;
    delta += 2 * k;

    if(delta >= middle) {
        nowY += 1;
        middle += 2 * (x2 - x1);
    }

    drawPoint(nowX, nowY);
}
```

接着变乘法为移位运算，然后重构代码。
```c++
int nowX = x1;
int nowY = y1;
int dx = x2 - x1;
int dy = y2 - y1;
int k1 = dx << 1;
int k2 = dy << 1;
int delta = 0;

drawPoint(x1, y1);

while(nowX != x2) {
    nowX += 1;
    delta += k2;

    if(delta >= dx) {
        nowY += 1;
        dx += k1;
    }

    drawPoint(nowX, nowY);
}
```

这样就可以了。但是现在只是在第一象限，斜率存在且大于 0，起始点小于终止点的情况，但是实际中还有其它情况。这个其实很简单，将 `x`、`y` 根据情况调转、对称，起始点和终止点也根据情况调转，步进的值也根据情况取反，就可以用这一个模型涵盖所有情况了，斜率不存在也可以变化为斜率为 0 的情况，改进后的代码如下。
```c++
public void glDrawLine(int x1, int y1, int x2, int y2) {
    int nowX = x1;
    int nowY = y1;

    int delta = 0;
    int dx = x2 - x1;
    int dy = y2 - y1;
    int k1 = dx << 1;
    int k2 = dy << 1;
    int xStep = 1;
    int yStep = 1;

    if (dx < 0) {
        dx = -dx;
        k1 = -k1;
        xStep = -xStep;
    }

    if (dy < 0) {
        dy = -dy;
        k2 = -k2;
        yStep = -yStep;
    }

    glDrawPoint(nowX, nowY);
    if (dx < dy) {
        while (nowY != y2) {
            nowY += yStep;
            delta += k1;
            if (delta >= dy) {
                nowX += xStep;
                dy += k2;
            }
            glDrawPoint(nowX, nowY);
        }
    } else {
        while (nowX != x2) {
            nowX += xStep;
            delta += k2;
            if (delta >= dx) {
                nowY += yStep;
                dx += k1;
            }
            glDrawPoint(nowX, nowY);
        }
    }
}
```



## 重心坐标要怎么计算？需要注意什么？
**重心坐标概念**：三角形内部任何一个点(包括顶点)都可以表示成三个顶点的线性组合。满足以下关系式的三个系数(α，β，γ)就是该点的**重心坐标**。
![[Pasted image 20240307143126.png]]

**重心坐标的作用：**
1. 通过判断是否有坐标的分量为负值知道是否在三角形的内部。若αβγ都大于 0，则点在三角形内，否则在三角形外
2. 插值顶点数据


**计算重心坐标的方法：**
1. 面积法：利用任意两边向量叉积求面积（叉积的几何意义是两边组成的平行四边形，三角形面积只需除以二即可得到），通过面积计算
![[Pasted image 20230517235519.png]]

2. 公式法：用已经推出的公式计算
 ![[Pasted image 20230520101036.png]]

需要注意：**必须用三维空间的坐标进行插值，不能用投影后得到的二维坐标进行插值！**

## 透视矫正插值
![](1672977119933.png)

本来按照正确的透视，正方形的中心点应该在 AC 和 BD 的交点 Q，我们应该把纹理中 uv 坐标为 $(\frac{1}{2},\frac{1}{2})$ 的颜色值赋给 Q，但是图中却赋给了 P。为什么呢？

**原因：用屏幕空间的坐标计算三角形重心坐标**，因为已经进行了透视除法，所以坐标不是线性的，会导致图像不正确的显示。

解决方法：**将屏幕空间重心坐标转换为世界空间重心坐标**

现在基本由硬件自动处理。

## 视口变换矩阵 bug
项目中的视口变换矩阵并不完整，视口矩阵负责从 NDC 空间转换到屏幕空间，在裁剪空间先完成裁剪，然后对裁剪坐标执行【透视除法】后就能变换到标准化设备坐标（NDC）。
然而项目中并没有执行透视除法 (除以 w 分量)，没有完成从裁剪空间到 NDC 空间的转换，直接就对裁剪空间的坐标进行了视口转换。

【修正方法】：我们在绘制三角形，判断边界以及计算重心的时候，都执行一次透视除法（除以 z 分量）


## AO 的实现

[[图形学/6 软渲染器/软光栅/总结#暴力法求AO]]

## SSAO
环境光遮蔽 (AO) 这一技术会带来很大的性能开销，因为它还需要考虑周围的几何体。我们可以对空间中每一点发射大量光线来确定其遮蔽量，但是这在实时运算中会很快变成大问题。在2007年，Crytek 公司发布了一款叫做**屏幕空间环境光遮蔽(Screen-Space Ambient Occlusion, SSAO)**的技术，

思路：
*   从相机出发，得到屏幕空间深度图
*   对**相机视角**下屏幕表面一着色点，以它为中心，R 为半径的球体范围内随机寻找数个采样点，判断其可见性
*   若该采样点深度大于相机视角下表面深度，则认为该点不可见，记可见性为 0。如下图红点。
*   若该采样点深度小于相机视角下表面深度，则认为该点可见，记可见性为 1。如下图绿点。
*   统计可见性为 1 采样点的占比，记作当前着色点的 visibilit

![[7eaa8361212a7e88b2fd624635ac7bff_MD5.jpg]]
然而，当采样核心是一个球体时，它导致平整的墙面也会显得灰蒙蒙的，因为核心中一半的样本都会在墙这个几何体上。下面这幅图展示了孤岛危机的 SSAO，它清晰地展示了这种灰蒙蒙的感觉：

![[5c2ae872380ac61ea2faee846e36c80e_MD5.jpg]]

由于这个原因，我们将不会使用球体的采样核心，而使用一个**沿着表面法向量的半球体**采样核心。这要求我们生成屏幕空间法线图。

![[7997fb5f1b559c83d8a71ed03de24665_MD5.jpg]]

总结一下，我们需要两张图，
- 一张深度图，它用来判断采样点是否被表面遮蔽，并且还会用来重建相机视角坐标系（先重建世界坐标再转换到观察空间）进而产生随机采样点。
- 一张法线图，用来生成法向半球随机点。




