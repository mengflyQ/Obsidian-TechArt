这个系列是 GAMES104 - 现代游戏引擎：从入门到实践 ([GAMES 104: Modern Game Engine-Theory and Practice](https://games104.boomingtech.com/en/)) 的同步课程笔记。本课程会介绍现代游戏引擎所涉及的系统架构、技术点以及引擎系统相关的知识。本节课主要介绍游戏 AI 的高级技术。

## Hierarchical Tasks Network

**层次任务网络 (hierarchical tasks network, HTN)** 是经典的游戏 AI 技术，和上一节介绍过的行为树相比 HTN 可以更好地表达 AI 自身的意志和驱动力。

![](1680600624052.png)

HTN 的思想是把总体目标分解成若干个步骤，其中每个步骤可以包含不同的选项。AI 在执行时需要按照顺序完成每个步骤，并且根据自身的状态选择合适的行为。

![](1680600624854.png)

### HTN Framework

HTN 框架中包含两部分，**world state** 和 **sensor** 两部分。其中 world state 是 AI 对于游戏世界的认知，而 sensor 则是 AI 从游戏世界获取信息的渠道。

![](1680600625692.png)

除此之外 HTN 还包括 **domain**，**planner** 以及 **plan runner** 来表示 AI 的规划以及执行规划的过程。

![](1680600626546.png)

### HTN Task Types

在 HTN 中我们将任务分为两类，**primitive task** 和 **compound task**。

![](1680600627377.png)

primitive task 一般表示一个具体的动作或行为。在 HTN 中每个 primitive task 需要包含 precondition、action 以及 effects 三个要素。

![](1680600628200.png)

![](1680600629018.png)

而 compound task 则包含不同的方法，我们把这些方法按照一定的优先级组织起来并且在执行时按照优先级高到低的顺序进行选择。每个方法还可以包含其它的 primitive task 或者 compound task，当方法内所有的 task 都执行完毕则表示任务完成。

![](1680600629839.png)

![](1680600630690.png)

在此基础上就可以构造出整个 HTN 的 domain，从而实现 AI 的行为逻辑。

![](1680600631591.png)

![](1680600632412.png)

### Planning

接下来就可以进行规划了，我们从 root task 出发不断进行展开逐步完成每个任务。

![](1680600633234.png)

![](1680600634077.png)

![](1680600635747.png)

![](1680600636639.png)

![](1680600637481.png)

![](1680600638286.png)

![](1680600639088.png)

### Replan

执行 plan 时需要注意有时任务会失败，这就需要我们重新进行规划，这一过程称为 **replan**。

![](1680600639926.png)

当 plan 执行完毕或是发生失败，亦或是 world state 发生改变后就需要进行 replan。

![](1680600640834.png)

总结一下 HTN 和 BT 非常相似，但它更加符合人的直觉也更易于设计师进行掌握。

![](1680600641681.png)

## Goal-Oriented Action Planning

**goal-oriented action planning(GOAP)** 是一种基于规划的 AI 技术，和前面介绍过的方法相比 GOAP 一般会更适合动态的环境。

![](1680600642485.png)

### Structure

GOAP 的整体结构与 HTN 非常相似，不过在 GOAP 中 domain 被替换为 **goal set** 和 **action set**。

![](1680600643297.png)

goal set 表示 AI 所有可以达成的目标。在 GOAP 中需要显式地定义可以实现的目标，这要求我们把目标使用相应的状态来进行表达。

![](1680600644124.png)

![](1680600644947.png)

而 action set 则接近于 primitive task 的概念，它表示 AI 可以执行的行为。需要注意的是 action set 还包含**代价 (cost)** 的概念，它表示不同动作的” 优劣” 程度。在进行规划时我们希望 AI 尽可能做出代价小的决策。

![](1680600645842.png)

### Planning

GOAP 在进行规划时会从目标来倒推需要执行的动作，这一过程称为**反向规划 (backward planning)**。

![](1680600646697.png)

在进行规划时首先需要根据优先级来选取一个目标，然后查询实现目标需要满足的状态。为了满足这些状态需求，我们需要从 action set 中选择一系列动作。需要注意的是很多动作也有自身的状态需求，因此我们在选择动作时也需要把这些需求添加到列表中。最后不断地添加动作和需求直到所有的状态需求都得到了满足，这样就完成了反向规划。

![](1680600647550.png)

![](1680600648359.png)

![](1680600649252.png)

![](1680600650101.png)

GOAP 的难点在于如何从 action set 进行选择，我们要求状态需求都能够得到满足而且所添加动作的代价要尽可能小。显然这样的问题是一个**动态规划 (dynamic programming)** 问题，我们可以利用图这样的数据结构来进行求解。在构造图时把状态的组合作为图上的节点，不同节点之间的有向边表示可以执行的动作，边的权重则是动作的代价。这样整个规划问题就等价于在有向图上的最短路径问题。

![](1680600650924.png)

![](1680600651787.png)

总结一下 GOAP 可以让 AI 的行为更加动态，而且可以有效地解耦 AI 的目标与行为；而 GOAP 的主要缺陷在于它会比较消耗计算资源，一般情况下 GOAP 需要的计算量会远高于 BT 和 HTN。

![](1680600652658.png)

## Monte Carlo Tree Search

**蒙特卡洛树搜索 (Monte Carlo tree search, MCTS)** 也是经典的 AI 算法，实际上 AlphaGo 就是基于 MCTS 来实现的。简单来说，MCTS 的思路是在进行决策时首先模拟大量可行的动作，然后从这些动作中选择最好的那个来执行。

![](1680600653600.png)

![](1680600654473.png)

MCTS 的核心是 **Monte Carlo 方法 (Monte Carlo method)**，它指出定积分可以通过随机采样的方法来进行估计。

![](1680600655293.png)

以围棋为例，MCTS 会根据当前棋盘上的状态来估计落子的位置。

![](1680600656106.png)

从数学的角度来看，我们把棋盘上棋子的位置称为**状态 (state)**，同时把落子的过程称为**行为 (action)**。这样整个游戏可以建模为从初始节点出发的状态转移过程，而且所有可能的状态转移可以表示为一棵树。

![](1680600656925.png)

![](1680600657770.png)

![](1680600658913.png)

显然构造出完整的树结构可能是非常困难的，不过实际上我们并不需要完整的树。在使用 MCTS 时，完成每一个行为后只需要重新以当前状态构造一棵新树即可。

![](1680600659786.png)

### Simulation

**模拟 (simulation)** 是 MCTS 中的重要一环，这里的” 模拟” 是指 AI 利用当前的策略快速地完成整个游戏过程。

![](1680600660598.png)

### Backpropagate

我们从同一节点出发进行不断的模拟就可以估计该节点的价值 (胜率)。

![](1680600661456.png)

然后把模拟的结果从下向上进行传播就可以更新整个决策序列上所有节点的价值。

![](1680600662271.png)

### Iteration Steps

这样我们就可以定义 MCTS 的迭代步骤如下：

![](1680600663114.png)

![](1680600663989.png)

#### Selection

在对节点进行选择时，MCTS 会优先选择可拓展的节点。

![](1680600664812.png)

在进行拓展时往往还要权衡一些 exploitation 和 exploration，因此我们可以把 UCB 可以作为一种拓展的准则。

![](1680600665646.png)

![](1680600666510.png)

这样在进行选择时首先需要从根节点出发然后不断选择当前 UCB 最大的那个节点向下进行访问，当访问到一个没有拓展过的节点时选择该节点进行展开。

![](1680600667336.png)

#### Expansion

对节点进行展开时我们需要根据可执行的动作选择一组进行模拟，然后把模拟的结果自下而上进行传播。

![](1680600668179.png)

![](1680600669015.png)

#### The End Condition

当对树的探索达到一定程度后就可以终止拓展过程，此时我们就得到了树结构上每个节点的价值。

![](1680600669836.png)

然后只需要回到根节点选择一个最优的子节点进行执行即可。

![](1680600670688.png)

总结一下，MCTS 是一种非常强大的决策算法而且很适合搜索空间巨大的决策问题；而它的主要缺陷在于它具有过大的计算复杂度，而且它的效果很大程度上依赖于状态和行为空间的设计。

![](1680600671570.png)

## Machine Learning Basic

### ML Types

近几年在**机器学习 (machine learning, ML)** 技术的不断发展下有越来越多的游戏 AI 开始使用机器学习来进行实现。根据学习的方式，机器学习大致可以分为监督学习、无监督学习、半监督学习以及强化学习等几类。

![](1680600672443.png)

![](1680600673247.png)

![](1680600674085.png)

![](1680600674936.png)

**强化学习 (reinforcement learning, RL)** 是游戏 AI 技术的基础。在强化学习中我们希望 AI 能够通过和环境的不断互动来学习到一个合理的策略。

![](1680600675794.png)

![](1680600676700.png)

### Markov Decision Process

强化学习的理论基础是 **Markov 决策过程 (Markov decision process, MDP)**。在 MDP 中智能体对环境的感知称为**状态 (state)**，环境对于智能体的反馈称为**奖励 (reward)**。MDP 的目标是让智能体通过和环境不断的互动来学习到如何在不同的环境下进行决策，这样的一个决策函数称为**策略 (policy)**。

![](1680600677579.png)

![](1680600678489.png)

![](1680600679329.png)

![](1680600680258.png)

![](1680600681068.png)

![](1680600681993.png)

## Build Advanced Game AI

尽管目前基于机器学习的游戏 AI 技术大多还处于试验阶段，但已经有一些很优秀的项目值得借鉴和学习，包括 DeepMind 的 AlphaStar 以及 OpenAI 的 Five 等。

![](1680600682859.png)

这些基于**深度强化学习 (deep reinforcement learning, DRL)** 的游戏 AI 都是使用一个深度神经网络来进行决策，整个框架包括接收游戏环境的观测，利用神经网络获得行为，以及从游戏环境中得到反馈。

![](1680600683714.png)

### State

以 AlphaStar 为例，智能体可以直接从游戏环境获得的信息包括地图、统计数据、场景中的单位以及资源数据等。

![](1680600684541.png)

![](1680600685365.png)

![](1680600686201.png)

### Actions

在 AlphaStar 中智能体的行为还取决于当前选中的单位。

![](1680600687067.png)

### Rewards

奖励函数的设计对于模型的训练以及最终的性能都有着重要的影响。在 AlphaStar 中使用了非常简单的奖励设计，智能体仅在获胜时获得 + 1 的奖励；而在 OpenAI Five 中则采用了更加复杂的奖励函数并以此来鼓励 AI 的进攻性。

![](1680600687903.png)

![](1680600688772.png)

### Network

在 AlphaStar 中使用了不同种类的神经网络来处理不同类型的输入数据，比如说对于定长的输入使用了 MLP，对于图像数据使用了 CNN，对于非定长的序列使用了 Transformer，而对于整个决策过程还使用了 LSTM 进行处理。

![](1680600689598.png)

![](1680600690773.png)

![](1680600691695.png)

![](1680600692552.png)

![](1680600693391.png)

![](1680600694244.png)

### Training Strategy

除此之外，AlphaStar 还对模型的训练过程进行了大规模的革新。在 AlphaStar 的训练过程中首先使用了监督学习的方式来从人类玩家的录像中进行学习。

![](1680600695144.png)

接着，AlphaStar 使用了强化学习的方法来进行自我训练。

![](1680600695982.png)

![](1680600696810.png)

试验结果分析表明基于监督学习训练的游戏 AI 其行为会比较接近于人类玩家，但基本无法超过人类玩家的水平；而基于强化学习训练的 AI 则可能会有超过玩家的游戏水平，不过需要注意的是使用强化学习可能需要非常多的训练资源。

![](1680600697650.png)

![](1680600698504.png)

因此对于游戏 AI 到底是使用监督学习还是使用强化学习进行训练需要结合实际的游戏环境进行考虑。对于奖励比较密集的环境可以直接使用强化学习进行训练，而对于奖励比较稀疏的环境则推荐使用监督学习。

![](1680600699350.png)

![](1680600700206.png)

## Reference

*   [Lecture 17：Advanced Artificial Intelligence (Part I)](https://www.bilibili.com/video/BV1iG4y1i78Q?spm_id_from=333.337.search-card.all.click&vd_source=7a2542c6c909b3ee1fab551277360826)
*   [Lecture 17：Advanced Artificial Intelligence (Part II)](https://www.bilibili.com/video/BV1ja411U7zK/?spm_id_from=333.788&vd_source=7a2542c6c909b3ee1fab551277360826)