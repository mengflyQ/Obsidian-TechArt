主要内容：**RTRT 的核心思路,什么是 motion vector,工业界的降噪方法和一些 failure cases**

本文是闫令琪教授所教授的 GAMES202 高质量实时渲染笔记 GAMES202 Lecture12：Real-Time Ray-Tracing. 本人属于新手上路暂无驾照，有错误欢迎各位大佬指正.

![[c0f55dc94ff39ac30faf8ae8c9961779_MD5.jpg]]

这节课我们进入新知识, real-time ray-tracing, 我们将要学习:

*   原理
*   核心技术 motion vector 和 trmporal accumulation/filtering 是什么
*   有什么样的问题

下节课我们将具体的如何运用 RTRT 去解决问题, 这节课讲基本的思路.

## **Real Time Ray Tracing(RTRT)**

## **RTRT is the Future**

*   in 2018, NVIDIA announced GeForce RTX serise(Turing architecture)

*   Opening a $250 billion market

![[d30298e0981b451255ade481c10d0e34_MD5.png]]

*   What does RTX do?

![[591ccf32d161592ab8a4f3642adaba7a_MD5.png]]

*   What does RTX actually do?

![[8ebebcb819dce99faf1151470dbd5c61_MD5.png]]

我们先来介绍一下 RTX:

RTX 其本质上只是硬件的提升, 并不涉及任何算法部分, 其本身是作为硬件上的一个突破, 因为在显卡加装了一个用来做光追的部件, 由于光线追踪做的是光线与场景的求交, 结合 games101 我们知道光线要经历 BVH 或者 KD-tree 这种加速结构, 也就是做一个树的遍历从而快速判断光线是否与三角形求交, 这部分对于 GPU 来说是不好做的, 因此 NVIDA 设计了专门的硬件来帮助我们每秒可以 trace 更多的光线.

并且虽然 trace 那么多光线, 但一根光线并不代表一个光路样本

![[553a843a024964198eee15005a4a4d2b_MD5.jpg]]

每秒可以 trace 10 Giga 的 rays, 也就是 100 亿根光线., 看起来虽然多, 但是要对每个像素都 trace 因此先要除以分辨率, 比如 1K 或者 2K, 除以几千万之后得到的结果还是很多, 但我们还要除以每秒的帧数, 而且我们并不是用所有的时间都只去做光线追踪, 剩余的时间我们还需要做降噪, 后期处理, gameplay 本身, 因此 1 秒中并不是全部时间都在做 ray tracing.

1 sample per pixel 就是每个像素采样一个样本. 从而得到最后的光追结果.

**那么什么样本是一个光路的样本呢?**

![[9431c263656e80113b745b16c5c7a8a9_MD5.jpg]]

至少要有四条光线才能构成一个最基本的光路:

*   primary hitpoint(从 camera 出发打出一根光线打到的交点)
*   shadow ray(primary hitpoint 和光源之间连接进行 light sampling 并判断是否有遮挡)
*   secondary ray(在 Hitpoint 根据材质采样出一个方向打出一根光线, 他会打到一个物体上从而得到 secondary hitpoint)
*   secondary shadow ray(从 secondary hitpoint 与光源连接判断是否会被光源看到)

图中的第一步是 rasterization(光栅化) 是因为与 primary ray 做的工作是一样的, 与其每个 pixel 去 trace 一条 primary ray, 不如直接将整个场景光栅化出来, 二者是等价的, 光栅化得到的结果不正就是每个 pixel 都穿过了一条 ray 吗, 而且光栅化更快, 因此大家把所有的 primary ray 替换成了一趟 rasterization, 也就是在第一步我们做完了所有的 primary ray, 因此对每个 shading point 来说所包含的只剩下了三条光线, shadow ray,scdondary ray 和 secondary shadow ray.

我们知道 path tracing 本身是一种蒙特卡洛积分的方法, 本身会产生噪声, 我们采样的样本越多, 其噪声越小, 但是我们在 games101 中用 64 spp 都得不到很好的结果, 何况 1spp, 因此我们知道 1spp 最后会得到一个很 noisy 的结果.

![[3dfbe85d3bdb0be02013d85ffd2d7e54_MD5.jpg]]

大家应该猜出来了, RTRT 最核心的技术正是 **降噪.**

实时光线追踪与 path tracing 相比, 只是进行了一算法上的些简化, 算法的核心思路不变, 它本身的突破是由于硬件的能力提升, 且 1 spp 是在 20 系列时的特点, 现如今的 30 系列可能会支持更多的 spp.

### **State of the Art* Denoising Solution**

![[3fe0d67f95f09c1e140d35cc7fc58f1a_MD5.jpg]]

从图中我们可以知道, 即使是 1spp 得到的这么糟糕的结果, 通过降噪我们仍然可以得到一个很棒的效果, 那么接下来我们来了解如何去降噪.

**首先我们需要知道在 1spp 下我们想要得到的结果:**

*   filter 之后的 quality 要好 (no overblur 不会湖成一片, no artifacts 没有像裂缝这种可见的 bug, 可以保留所有的细节)
*   速度 (降噪的速度要快, 也就是要在小于 2ms 内完成降噪操作)

引用一句秘籍 greed is good.

**从上面来看我们会认为这是不可能的人物, 在 1spp 下得到一个噪声那么大的结果, 你既要求降噪之后质量好又要要求速度快, 特别对于以前的很多方法在这不适用:**

*   Sheared filtering serise(SF,AAF,FSF,MAAF,...)Sheared 方法不行
*   Other offline filtering methods (IPP,BM3D,APR,...) 离线方法不行
*   Deep learning series(CNN,Autoencoder,...) 深度学习不行

降噪的方法有很多, 但是针对实施光线追踪的降噪方法很少, 那么工业界是如何解决这个问题的呢?

### **Industrial Sulution**

![[2a3e8e563bf5f9377236ecc965ae3880_MD5.jpg]]

我工业界来这里只为了三件事:

temporal

temporal

还是他妈的 temporal

**核心方法:**

**首先假设整个看的场景的运动是连续的, 就是 camera 以某种轨迹看向不同的物体, 帧与帧之间有大量的连续性.**

**motion vector, 它是用来告诉我们物体在帧与帧之间是如何运动的, 也就是图中的 A 点在 motion vector 下可以知道在上一帧里 A 点对应的位置 B 点.**

![[06c335d1146b2bdcab66756c2b77984b_MD5.jpg]]

*   我们认为当前帧是需要去进行 filtering, 前一帧时已经 filtering 好的.
*   利用 motion vector 来知道当前帧某一点在上一帧的对应位置.
*   由于我们认为场景运动是连续的, 所以认为 shading 也是连续的, 上一帧得到降噪好的结果比如颜色之类的, 可以在当前帧复用, 相当于增加了 spp, 但并不是简单的上一帧 1 spp + 当前帧 1 spp, 由于是一个递归, 因此上一帧也复用了上上一帧的结果, 因此 spp 其实是很多的, 可以理解为一个指数衰减, 每一帧都有百分比贡献到下一帧.
*   这种方法叫做**时间上的复用**, 通过在找某一点在不同帧上的对应关系去复用已得到的结果. 至于空间上的我们在下节课去讲.

**这里的 temporal 我们根据 motion vector 去找对应关系.**

### 几何缓冲区 ->The G-Buffers

在介绍具体细节之前, 我们引入 G-buffers.

G-Buffer，全称 Geometric Buffer ，译作几何缓冲区

在 screen space 我们可以得到很多的信息, 比如之前的 screen space ray tracing 我们在 screen space 得到了每个点的深度相当于从 camera 看去我们得到了一张深度图.

同样的我们不止可以通过 screen space 生成深度图, 还可以生成像是直接光照下生成图, 发现图或者每个点的 diffuse Albedo(也就是颜色 kd 项)

*   也就是在渲染过程中可以免费得到的额外信息, 如下图的三项.

![[0d003590b251f60dfdf5ea6e3dec9b2a_MD5.jpg]]

*   也可以存储 per pixel depth,normal, 世界坐标 (通过 rgb 三通道来存 x,y,z) 等, 只需要将这些信息各自生成一张图存储在 g-buffer 里, 当需要的时候拿出来用.

![[28a13a11f9cf7028220179810dc63dba_MD5.jpg]]

![[9d6ff1dfd8e1791c49187c7125ebd45a_MD5.jpg]]

**总结: G-buffer 之存储的只是 screen space 的信息.(因为他记录的是 camera 能看到的信息)**  

### **Back projection**

![[4186b97cf34679d6e609bd72bd7cfc5d_MD5.jpg]]

首先声明:

frame i: 当前帧

frame i-1: 上一帧

我们如何找当前帧的一个像素在上一帧的哪里呢?

首先我们要知道, 我们找的不是像素的位置, 而是当前帧像素里包含的内容在上一帧哪一个像素里.

![[7e3444066e19686d66e4cf2f87ef8b6b_MD5.jpg]]

也就是我们需要知道的是透过当前帧 (frame i) 中蓝点这个像素我们所得到的点的世界坐标, 投影到上一帧 (frame i-1) 中对应的是哪个像素.

根据这个思路:

我们首先要求出这个点的世界坐标, 如果 G-buffer 中有世界坐标信息的图, 我们可以拿来直接用

如果没有我们可以计算得到:

我们知道一个点的世界坐标需要通过 MVP 矩阵和视口矩阵从而得到在 screen space 上的坐标, 因此我们可以逆向思考, 让 screen space 这个点按逆顺序乘以他们的逆矩阵就得到了世界坐标.

![[da406578869dc776e8de749c1a6e2b99_MD5.png]]

到此我们求出了这一点的世界坐标, 对于他在上一帧的哪里, 由于我们知道整套渲染的流程, 因此对于每一帧之间是怎么移动的肯定也知道, 因此如果物体移动了的话, 我们只需要用将当前帧世界坐标乘以帧移动的逆矩阵就得到了上一帧中这个点的世界坐标:

![[33714fc52aab63e9af3adce56f2b81fa_MD5.png]]

到此我们知道了发生移动后的上一帧的点的世界坐标, 接着我们需要将他变回屏幕坐标上, 因此我们需要乘以 MVP 矩阵和视口矩阵, 关于图中的各种矩阵由于我们是掌控着渲染的整个流程, 因此每个矩阵都是已知的.:

![[c67f9973c130b622ed373f8712641ffd_MD5.jpg]]

到此我们求出了 frame i 上的蓝点内对应的信息在 frame i-1 上的哪里, 也就是我们通过 Back projection 方法求出了 motion vector

![[f6996fae9e05d6695c421a7bbccfb4c2_MD5.jpg]]

### Temporal Accum./Denoising

![[1ed02d2b5f177bcfb4b6f7b127806a03_MD5.jpg]]

在得到了 motion vector 之后, 我们就可以把当前帧 (noisy 的图) 和上一帧 (没有 noisy 的图) 结合在一起, 最简单的方法就是线性 blending 在一起, 比如上一帧的结果 0.8 + 这一帧的结果 * 0.2 得到新的结果.

我们在此定义:

*   **~** : unfiltered 表示没有 filter 噪声挺多的内容
*   **-** : filtered 没有噪声或者噪声比较小的

![[f69582c9078cba2e594d192c8ea20a49_MD5.jpg]]

![[e9b49b048b7ca4a4f94e183998112649_MD5.jpg]]

这个是时间上的降噪，做了一个线性的 blending，认为上一帧是降噪好了的。

$\alpha$ 表示平衡系数，当前帧的贡献，在 (0.1-0.2) 之间。

现在我们来顺一下时间上的降噪的过程:

1.  如果在进行 rasterization(primary ray) 时得到了世界坐标信息的图存储在 G-buffer 中则直接得到世界坐标
2.  如果没有其信息, 则在当前帧的像素通过逆视口变换和逆 MVP 变换得到世界坐标
3.  已知 motion 矩阵, 将世界坐标逆 motion 得到上一帧的世界坐标
4.  将上一帧的世界坐标进行正 MVP 变换和视口变换得到当前帧中的像素在上一帧中的位置
5.  将两个值线性 blending 起来从而得到当前帧最后的结果

滤波绝不会让一张图变暗或者变亮.

我们知道没有东西是完美无暇的

那么古尔丹.......... 这次的代价是什么呢?

### **Temporal Failure**

时间上的复用十分好用, 我们应该尽可能多用, 但是如果在他不可用时候该怎么办?

*   Failure case 1:switching scenes (burn-in period)

突然切换场景由于我的上一帧并没有新场景的任何信息因此得到的结果肯定不准确, 再比如切换光源, 假设在迪厅中场景中任何物体都不变换, 只是将红灯忽然变为绿灯, 由于上一帧中的信息光源为红色因此如果复用会导致我们得到的结果偏红而不是偏绿, 因此时间的复用在此不可用.

因此需要一个时间也就是 burn-in period, 去累计个新场景的几帧信息以供后续进行复用.

*   Failure case 2:walking backwards in a hallway (screen space issue)

![[6dfd00802004dad4acd8db7c20cb06d7_MD5.jpg]]

我们以图为例, 加入我们的 camera 从离门最近开始倒退, 我们知道在倒退的时候周围的信息会不断的增多, 因此我们当前帧新出现的物体无法在上一帧的屏幕空间内找到相对应的点, 这肯定会带来影响

*   Failure case 3: suddenly appearing background(disocclusion)

![[d1d78996e91c791cc44176e88fdfcfaf_MD5.jpg]]

左边为上一帧, 右边为当前帧, 我们可以看见本来箱子后的内容是被遮挡住的, 但是推走后变成未被遮挡, 此时我们去求箱子后遮挡部分的两帧屏幕对应位置会发现, 右边的蓝点所显示的对应位置为左边的蓝点, 这里是没有出错的, 因为在屏幕空间确实对应的是蓝点只是无法显示遮挡后的内容, 这是屏幕空间的问题, 但是如果进行线性 blending 的话肯定会出问题, 这就是 disocclusion(不遮挡) 问题。

![[2dab3b8f6755528de68968c9796349ad_MD5.jpg]]

我们对于上一帧的东西参考了 80% 甚至更多, 自然会产生这种残影 / 拖尾的现象.

因此为了避免残影的出现, 工业界有两种解决方法:

![[13362de2b330a3f1e8e50a58c7917fca_MD5.jpg]]

clamping 和 detection, 可以分开单独使用也可以二者一起使用.

![[1154eb15704911de5825b5d9d196cbc2_MD5.png]]

对于 clamping 方法来说, 我们是把上一帧拉到接近当前帧的结果, 以刚才的黄色箱子和白色墙壁为例, 不正是因为当前帧里去了 80% 黄色墙壁的值才导致了残影出现吗, 如果我们在任何时候都把上一帧的结果, 先给拉近当前帧的结果不就使得二者之间的颜色很相似, 从而弱化了这一不正确的现象吗?

我个人理解是, 上一帧的黄色, 我们把他拉近白色, 然后再将二者线性 blending, 这样我们得到的结果不仍然是接近于白色的吗?

![[fcad03bcb01aa0210a286ab2b571863f_MD5.jpg]]

detection 的思路是我们要判断是否仍要使用上一帧的结果, 仍然以黄色箱子和白色墙壁为例, 当我们做完 motion vector 之后, 我们增加一步操作来判断所对应的是不是同一个物体, 在工业界我们渲染是认为每个物体都有自己的编号, 如果编号不同, 那么认为 motion vector 就不在靠谱了

因此我们对α进行调整, 此时我们不再用 80% 或者更多了, 我们可以认为是非 0 即 1,0% 上一帧, 100% 这一帧, 但如果差的不是很多, 我们就稍微减少其百分比.

但是我们知道, 之所以引用上一帧是因为他没有噪声, 如果 100% 相信这一帧相当于重新引入了噪声, 但我们可以将当前帧的 filter 变大一点, 让结果模糊一点但噪声小, 这是没有关系的.

![[66df91040ec62e65102b62837e33f10b_MD5.jpg]]

我们可以看到残影现象没有了, 却产生了噪声.

除了这些还有很多其他的问题, 比如:

![[8879538907b25b6199790fd773e0a345_MD5.jpg]]

但其实大部分情况下是 shading 的问题

以这个场景为例, 我们认为光源从左到右移动, 而场景几何并不发生移动, 因此我们知道任何一个像素的 motion vector 是 0, 但是由于我们一直在复用未移动的帧结果, 就会导致拖影的阴影出现.

![[628a39803105b65a785586790ab71842_MD5.jpg]]

对于这种地板上反射出场景的情况来说, 由于地板是不动的, 因此其上面的每一个像素点的 motion vector 为 0, 当我们移动物体时, 其地板需要一定的时间适应, 之后再在地板上反射出当今场景中的物体, 也就是反射滞后.

因此传统的 motion vector 是搞不定这种情况的, 因为变得是 shadow 和反射这种, 我们实际想追踪的是 shading 的变换, 其几何不动因此 motion vector 为 0, 从而会导致各种现象.

.