## **The last lecture of games202,but it's not the end of studying rather a new start for me.**

*   **终于来到了 Newcastle，开始了 10 天的 quarantine，因此决定在这十天内把 202 收尾并且把 101 的东西重新看一遍和 cherno 的 c++ 系列为之后游戏工程的课做好准备。**
*   **在此特别感谢强哥，一起来的路上对我照顾有加，永远的好大哥。**
*   **也十分感谢康姐，我在英国吃的第一顿热饭是康姐煮的汤圆，救了孩子一命。XD**

**好了回归正题。**

本节课主要讲一下工业界的一些技术.

前情回顾:

![[e32bd21ad8de2fb63f1ef711675e3211_MD5.jpg]]

本节内容:

![[a8999ee781b8ff309ae3b496fac025f0_MD5.jpg]]

## Specific Filtering Approaches for RTRT

### SVGF-Basic Idea

![[07421776ed37d2f1a601cd0497c5f924_MD5.jpg]]

我们可以看到 ground truth 和 svgh 结果非常接近, 这说明 svgf 效果很不错.

*   SVGF 的方法与在时空上降噪的方法差不多
*   但是, 加了一些 variance analysis 和 tricks

### SVGF-Joint Bilateral Filtering

![[61d7c2cb891e83c08fcde7de9297d590_MD5.jpg]]

**三个指导 filtering 的重要因素:**

### 1.Depth:

以图中的 A,B 互相贡献为例, 还记得我们上节课说的

如果二者之间的深度差异小, 则贡献权值大

反之二者深度差异大, 则贡献权值小.

$w_z=exp(-\frac{|z(p)-z(q)|}{\sigma_z|\Delta z(p)\cdot (p-q)|+\epsilon})$

我们来分析一下公式的各个部分:

这是 SVGF 用来判断深度贡献权值的公式, 从公式中可以看出：

**由于 exp(x) 是返回 e 的 x 次方, 由于公式返回的是 - x 次方, 所以差异越大, 贡献越小.**

但是分母部分多了个 $\sigma_{z}$ , 多了个 $|\Delta z(p)\cdot (p-q)|$ (深度的梯度 * 两点间的距离), 还多了一个 $\epsilon$ .

**$\epsilon$ 的作用是因为我们 filter 时候考虑一个点 Q 周围所有的点 P, 也包括这个点 Q, 因此它是有可能为 0 的, 避免分母为 0 的情况, 而且如果两点足够接近会导致最后的数值太大, 为了避免一些数值上的问题加上一个$\epsilon$ .**

**$\sigma_{z}$ 是一个用来控制指数衰减的快慢的参数, 或者理解为控制深度的影响大还是小.**

至于$|\Delta z(p)\cdot (p-q)|$ 我们以例子为例来理解:

![[61d7c2cb891e83c08fcde7de9297d590_MD5.jpg]]

1.A 和 B 两点在同一个平面上, 颜色也相近, 所以理论上 A 和 B 会贡献不少给彼此

2. 但是由于这一面是侧向面对我们的, 因此 A 和 B 是有很大深度差异的. 此时如果我们用深度来判断 A 和 B 对彼此的贡献时候, 会发现它所给出的是 A 和 B 之间不应该有特别大的贡献, 这明显是不合理的.

3. 因此简单的用深度来判断贡献值是不行的, 那么我们考虑 A 和 B 在平面法线方向上的深度变化, 比如图中的 A 和 B 虽然在实际深度差异很大, 但是在法线方向上的深度差异几乎没有, 因为二者几乎在同一个平面上.

![[e49fc9c293ab99e1c3ba0e1df3fc8e38_MD5.jpg]]

首先我们要知道, 深度的梯度就是往某一方向上的变化率, 因此当我们知道 A 和 B 之间的距离, 之后用 **深度梯度 X 距离 = 实际深度变化量.（** $|\Delta z(p)\cdot (p-q)|$ **）**

**(垂直于平面法线上的变化 * 距离 = 实际深度变化量)**

**如果实际深度变化量也很大, 从而告诉我们这个平面是侧向我们的, 在这种情况下来看公式, 虽然分子深度差异大, 但是分母中的梯度也大, 二者一除, 值就没那么大了, 因此 EXP() 得到的值就大了.**

**总结来说, 通常情况下不会简单的用二者之间的深度差异来判断贡献值, 而是在他们所在的面的法线方向上投影出来的深度差异来判断, 或者说是在它们所在平面上的切平面上的深度差异.**

**TIPS: 平面的切平面等于平面本身.**

**2.Normal**

![[0291f5d6ac5b55e8c635c0a5435e13e5_MD5.jpg]]

  
$W_n=max(0,n(p)\cdot n(q))^{\sigma_n}$

我们用两个点法线向量求一个点积, 由于求出来的值有可能是负值, 因此使用 max() 把负的值给 clamp 到 0。

如果两个向量相同则点乘结果为 1, 点乘结果为负时则为 0，点乘结果为正时则使用这个结果.

至于 $σ_{n}$ 是用来判断点乘的这个 cos 函数从 1 到 0 是快还是慢,$σ_{n}$ 越大, 从 1 到 0 就越快, 也就是判断法线之间的差异的严格程度.

另外, 如果场景中运用了法线贴图来制造凹凸效果, 我们再判断时运用平面原本的法线, 而不是为了制造凹凸效果而改变过的法线.

**3. Luminance**

![[923f45ebd0a6580309ba1bbe798a10ef_MD5.jpg]]

Luminance (grayscale color value) $w_l=exp(-\frac{|l_i(p)-l_i(q)|}{\sigma_l\sqrt{g_{3\times 3}(Var(l_i(p)))}+\epsilon}$

我们在考虑颜色差异时，最简单就是应用双边滤波里给的颜色差异来考虑，比如我们将 RGB 转换为 grayscale（灰度），这种颜色我们称其为 luminance，只是一个名字，我们就认为其是颜色。

任意两点间我们看他们的颜色差异，如果颜色差异过大，则认为靠近与边界，此时 A 和 B 的贡献不应该互相混合起来，比如 A 不应该贡献到 B，B 也不应该贡献到 A。

由于噪声的存在会出现一些干扰，也就是 B 点虽然在阴影里，但是可能刚好选择的点是一个噪声，也就是其特别亮，此时 A 特别亮，B 也特别亮，那么 A 和 B 就会互相贡献，但是这样是错误的现象。

此时就是 SVGF 中 V 的作用了 ------》variance。

**variance（方差）：我们先去看 A，B 两点间的颜色差异，并且思考 filter 的中心点的 variance，当 variance 比较大时，我们不应该去过多的相信两点间的颜色差异。这就是为什么会去除以一个标准差。**

**eg: 但我们在考虑 A 点是否贡献到 B 点时，先看 A 和 B 点之间的颜色差异，并且除以 B 点周围的一个标准差（** $\sqrt{方差}$ **）。**

我们要知道 B 点的 variance，由于 variance 是一个统计学的量，不可能只看他自己本身，假设当前帧渲染出来之后的结果是很 noisy 的：

$\sqrt{g_{3\times 3}(Var(l_i(p)))}$

*   在点的周围取一个 7*7 的区域算出区域里的 variance。
*   同样的操作在时间上累积下来，所谓累积也就是在时间上平均下来，它可以去找上一帧中对应点的 variance，从而通过 motion vector 在 temporal 上把 variance 信息给 filter 下来，这样先 spatial filter 求出 variance 之后，再随时间平均从而得到了一个比较平滑的 variance 值。
*   最后在使用 variance 时如果不放心，我们再在周围取一个 3*3 的区域做一次 spatial filter 得到 variance。

**也就是进行 spatial filter --> temporal filter --> spatial filter 从而得到 B 点精准的 variance，**

![[47f28239857fa5c85931d6ba7b5df505_MD5.jpg]]

可以看到 SVGF 的结果总体还是不错的，但还是有一点点的噪声。

![[58208c056e123440a86c7654d6b136fa_MD5.jpg]]

但是 SVGF 也有他自己无法处理的情况，当一个场景固定，我们只移动光源时候，阴影会随着光源的移动而变化，当前帧会复用上一帧的阴影，但由于没有发生任何的几何变换，因此 motion vector 等于 0，此时复用上一帧时会产生 “残影 “现象，这是 SVGF 无法解决的一个问题。

## **RAE（Recurrent AutoEncoder）**

RAE 是指 Recurrent AutoEncoder, 用 RAE 这么一种结构对 Monte carlo 路径追踪得到的结果进行 reconstruction，也就是对 RTRT 做一个 filter 从而得到一个 clean 的结果。

*   是一个后期处理降噪的方法，将一个 noisy 的图输入最后输出一张 clean 的图
*   会用到一些 G-buffer 上的信息，因此是与 noisy 的图一起作为神经网络的输入。
*   神经网络会自动将 temporal 的结果累积起来。

之所以不使用 motion vector 也可以将 temporal 的结果累计起来是因为两个设计原则”

1.  AutoEncoder, 是一个漏斗形的结构，输入经过若干层神经网络后变成一个很小的东西，之后再将小的东西不断地展开，形状很像一个 U 字形，因此也可以称之为 U 型神经网络，其适合去做一些图像上的各种操作。

AutoEncoder

*   Skip / residual connections for faster and better training

![[f1640c128aee45004cbad964c474fbde_MD5.jpg]]

2. 之所以可以利用历史的信息是由于每一层神经网络叫做 convolution block 但是有一个 recurrent 连接，也就是每一层神经网络不仅可以连向下一层，也可以连回自己这一层。因此假设神经网络一直在跑，跑完当前帧后，会有信息遗留在神经网络里面，信息每一层又可以连向它自己，因此在跑下一帧图时候，可以用神经网络自己学出来的方法去复用上一帧遗留下的信息而不是通过 motion vector。

![[34dc9cac15c5c73f59ca0140f10f6c0c_MD5.jpg]]

*   Recurrent block

*   Accumulates (and gradully forgets) information from previous frames

![[f1e27c0d4995e6694dfb652fd18f157a_MD5.jpg]]

![[c03dbae673b75c5477d1172ca3ba4207_MD5.jpg]]

最早 temporal 的思路是用来解决 Anti-Aliasing 的，先有 TAA 的巨大成功才会有 RTRT 里的应用，因此我们接下来了解一下 TAA。

## **Temporal Anti-Aliasing(TAA) 在时间上的抗锯齿或反走样**

*   Recall:why aliasing?

*   Not enought samples per pixel during rasterization **一个像素中的样本数量不足**
*   Therefore, the ultimate solution is to use more samples **终极解决方案就是用更多的样本，也就是 101 中的 MSAA**

**Tmeporal AA 的思路也是需要用更多的 sample，只不过是当前帧会复用上一帧的 sample，使得这一帧仍然用 1SPP，但是无形中通过 temporal 的 reuse，增加了 SPP。它的思路与 RTRT 中如何运用 temporal 的思路一模一样。**

Temporal AA 尝试用在避免性能损失的情况近似 Super Sampling AA 的结果。它的做法一句话总结就是，把样本分布到过去的 N 帧中去，然后每一帧从过去的 N 帧中取得样本信息然后 Filter，达到 N 倍 Super Sampling 的效果，如下图。

![[ebf267ca420900ffe7ddfea8c087ad09_MD5.jpg]]

我们考虑一个静止的场景，我们通过 TAA 提供的一个叫 Jittered sampling 的方法去复用上一帧的 sample，假设当前帧 1 我们都像图中一样将样本分布在像素的左上角，上一帧的在右上角 2，上上一帧在 3，上上上一帧在 4，我们可以认为连续的四帧之间有一个移动的 pattern，他们在时间上各不相同，样本就这样从 1，2，3，4 这样的转.

![[6ea1fc15f36845ce941c84606562c663_MD5.jpg]]

假设我们在当前帧每个像素里只有 1 这个点，我们就可以复用上一帧每个像素中 2 这个点，由于是一个递归的过程，因此又可以复用再上一帧像素中 3 这个点，以此类推到 4 这个点，等于我们把 temporal 的这些样本都考虑进去了，由于我们的场景是静止的，就等于把之前各自样本的结果在一起做了一个平均。

![[b33dd1448b32118d697d168b7125b8d4_MD5.jpg]]

这样就好象我们在当前帧中做了一个 2*2 的 upsampling，两个结果是很接近的。

**也就是通过将前 N 帧内的样本点的结果平均起来，其效果与在当前帧内增加样本点的效果一样。**

之所以不随机采样点是因为随机的效果不一定好，因为在 temporal 中可能会引入额外的高频的信息，因此使用规定好的样本点，如上图每四帧一个，这样固定样本点位置避免了分布不均匀的情况。

当场景是运动的情况下，原本在静止情况下我们是在一个像素里找 sample 的结果并复用上一帧中的样本点的结果，在运动场景中，当前帧几何的位置通过 motion vector 找到上一帧中对应的位置，并复用其结果，如果 temporal 的信息不太可信时，使用 clamping 方法，也就是把上一帧的结果拉到接近当前帧的结果。

### Notes on Anti-Aliasing 01

_MSAA_ VS _SSAA：_

SSAA 可以看作是将一个场景按照几倍的分辨率先渲染后再降采样，把几个像素的结果平均起来，思路是完全正确的，就是开销比较大。

MSAA 则是在 SSAA 的基础上做了一个近似从而使得其效率提升开销没那么大，如果我们用 4X 的 MSAA 时，帧率不会调得太多，但如果是 SSAA，帧率会掉的十分严重。

*   MASS:an improvement on performance，假设一个像素里有四个感知样本

*   对于同一个 primitive（几何体）中，每个样本只做一次 shading，如图，有四个 sample，两个 primitive。只做两次 shading，第一次在三角形中的 1 号样本做 shading，第二次在 0,3,2 号样本点找一个平均位置做 shading。如果使用 SSAA 的话，此处需要做 4 次 shading，所以 MSAA 提升了效率。

![[cb7def00cd0f79eb7efa39e4e8fa8247_MD5.jpg]]

*   MSAA 允许进行 sample　reuse，这个不是时间上的 reuse 而是空间上的。如图，在 1 和 2 两个像素内，在两个像素的连接处有两个采样点，这两个采样点既可以贡献给像素 1 也可以贡献给像素 2，因此实际上等于通过 reuse 在 6 个采样点的情况下得到了 8 个采样点的结果，减少了采样点的数量，提升了效率。

![[4a435d6640a5ad8f4af80a4d3fa6feb4_MD5.jpg]]

### Notes on Anti-Aliasing 02

我们记得有一种反走样是基于图像的，先渲染出有锯齿的图然后通过图像处理的方法将锯齿给提取出来并替换成没有锯齿的图，这种方法叫 image based anti-aliasing solution。、

比较流行的方法叫 SMAA(Enhanced subpixel morphological AA)

*   History:　FXAA　--->　MLAA (Morphological AA)　--->　SMAA

![[9d7caf6f78cf71087e841831ab30d6a6_MD5.jpg]]

图 1 可以知道，这是有锯齿的，其想要的结果应该是一条斜着的直线，图像方法会先去识别它然后通过各种匹配的方法找到它正确结果的样子，图 2 是 MLAA 的方法，从而得到它应该变成的样子，然后通过其边界在各个像素内占的百分比进行 shading。

### Notes on Anti-Aliasing 03

*   **_G-buffers should never be anti-aliased! G-buffers 是绝对绝对不能反走样的！！！_**

## **Temporal Super Resolution**

*   Super resolution (or super sampling) 低分辨率变成高分辨率

*   字面理解：为了增加图像的分辨率

DLSS 就是这么一种技术，将一张低分辨率的图输入最后得到一张高分辨率的输出：

*   **Source 1 (DLSS 1.0):out of nowhere / completely guessed**

DLSS1.0 的思路是通过数据驱动的方法来做的，效果并不是很好。因为他只在当前帧中进行，不依靠 temporal 的累积，等于没有任何的额外信息来源，但我们将低分辨率硬拉成高分辨率，如果不想让最后的结果模糊，必须需要一些额外的信息，DLSS1.0 是通过猜测来提供额外信息的。

也就是针对于每个游戏或者场景单独训练出一个神经网络，其会去学习一些常见的物体边缘从而在低分辨率拉成高分辨率之后将模糊的边缘换成不模糊的边缘，这就是 DLSS1.0 的思路。

*   **Source 2 (DLSS 2.0):from temporal information**

DLSS2.0 则摒弃了通过神经网络猜测的结果，而是更希望去利用 temporal 的信息，核心思想不在 DL 上了而在 temporal 上。

DLSS2.0 的核心思路在于 TAA，更多的去结合上一帧的信息运用到当前帧中，仍然是 temporal reuse。

之前我们讲过，TAA 对于静止场景中，连续四帧我们使用不同的感知 sample 点，当前帧使用上一帧的信息就等于是变相的提升了分辨率，这样想是对的，但是 DLSS2.0 中要面临一些问题。

1. 如果有 temporal failure 时，我们不可以在使用 clamping 的方法来解决，也就是对 temporal 的信息利用要求更加严格。

2. 因为我们最终要的是一个增大了分辨率的图，如图，分辨率提高也就是像素点增多，那么我们需要知道新增加的小的 pixel 的像素值是多少，如果此时我们用上一帧的结果盲目的 clamping 势必会因为一些小的像素的值是根据周围的点的颜色猜测出来的，而且猜测的值很像周围的点，也就是我们得到了一个高分辨率的图但是很糊。总结来说由于 DLSS 真正的提升了分辨率，因此我们要求新产生的像素的值是要与之前有本质的不同的，否则就会得到一个糊掉的结果。

![[ec8d9cf431c0fbca87c4a610e69780d8_MD5.jpg]]

3. 因此我们需要一个比 clamping 更好的复用 temporal 信息的方案。

![[928f180f6a6423f55efffc7758f53169_MD5.jpg]]

左边中的蓝色代表上一帧，绿色代表当前帧，绿点是当前帧给了一个采样信号得到的值，在上一帧也就是蓝色曲线中我们可以从另一个信号采样出来值，最后我们要把二者综合在一起得出一个当前帧增加了采样点后的值。

DLSS2.0 中的神经网络没有输出任何混合后的颜色，而是去告诉你应该怎么将将上一帧找到的信息和当前帧结合在一起。

## DLSS 2.0

*   An importance practical issue

*   如果 DLSS 每一帧需要消耗 30ms，那 DLSS 就太慢了，因此训练出这个网络之后去提升 inference 性能，针对 nVidia 的硬件进行优化，但具体如何做的...... 老师也没有权限了解。

*   其他公司的”DLSS“算法：

*   By AMD:FidelityFX Super Resolution
*   By Facebook:Neural Supersampling for Real-time Rendering [Xiao et al.]

## Deferred Shading（延迟渲染）

Deferred Shading 是一个节省 shading 时间的方法，是为了让 shading 变得更高效更快。

*   传统的光栅化过程：

*   Triangles -> fragments -> depth test-> shade ->pixe
*   这可能会出现每一个 fragment 都需要 shading 的情况，比如你需要从远处到近处渲染时，需要将每一个 fragment 都进行 shading，
*   此时的复杂度为：O(#fragment * #light) 因为每一个 fragment 都要考虑 light

*   Key observation

*   很多 fragment 在渲染过程中会通过 depth test，但最终可能会因为被后续的 fragment 所覆盖而不会被看到，因此这些在过程中被 shading 的 fragment 浪费了很多的资源。
*   那么我们能不能只去渲染可见的 fragment 呢？、

*   解决思路：

*   将场景光栅化两次。Just rasterize the scene twice
*   Pass 1: 在第一次光栅化中得到 fragment 之后我们不做 shading，所有的 fragment 只对深度缓存（depth buffer）做一个更新。
*   Pass 2：由于在 Pass1 中我们已经写好了 depth buffer 并且知道了最前深度，因此在 pass2 中只有深度等于最浅深度的 fragment 才可以通过 depth test 并进行 shading，从而实现了只对 visible fragment 着色。
*   Implicitly,this is assuming rasterizing the scene is way faster than shading all unseen fragments (usually true) ----> 光栅化场景的开销是小于对所有 fragment 着色的开销的。
*   复杂度从 O(#fragment * **#light) -----> O(#vis.frag, *** #light)

至此我们把 shading 延迟到了 pass2 中进行，并且把 fragment 的数量降低到了可见的 fragment 的数量。

### **古尔丹，这次你想要什么？**

*   Issue

*   由于依赖于 depth buffer(属于 G-buffer), 所以 pass1 和 pass2 都无法做 anti-aliasing。
*   但是不是什么大问题，我们可以通过 TAA 或者是 imaged based AA 来解决。因此现在是工业界的主流操作。

我们对 fragment 已经优化了很多，既然而然为了更优方法，就将目光放在了 light 身上，从而引出 Tiled Shading。

## Tiled Shading

Tiled shading 是建立在 Deferred Shading 的基础之上，将屏幕分成若干个小块，比如一个小块是 32 * 32，然后对每个小块单独的做 shading。

![[755a7f5768c756ab2707ee1f1c3dc29f_MD5.jpg]]

将屏幕分成了若干个小块，但由于是俯视因此是个平面图，看起来是分成了若干个小条，圆圈代表的是光源的范围，每个圆代表一个光源。

*   Key observation

*   节省每个小块要考虑的 light 数量，每个切出来的小块代表场景中 3D 的区域，并不是所有的光源都会跟这片区域相交。
*   我们在做 light sampling 时知道光照强度会随着距离的平方衰减，因此面光源或点光源的覆盖范围是很小的，我们根据其随距离平方的减少找出最小值，也就是设定一个范围，光源的覆盖范围看做一个球形。因此我们在渲染时，只需要找会影响到区域的光源即可，不需要考虑所有的光源。
*   如图，数字代表区域内会影响到它的光源数量。
*   复杂度：O(#vis.frag. * **#light)->O(#vis.frag. ***avg light)
*   但在此之后，又有人对其进行了一个复杂的优化。

### Clustered Shading

在刚才的基础上，我们不仅将其分成若干个小块，还要在深度对其进行切片，也就是我们将整个 3D 空间拆分成了若干个网格。

![[423e9dcbbf47ec7bb147048bf1d00e81_MD5.jpg]]

*   Key observation

*   如果只分成若干个小块区域的话，区域内的深度可能会十分大
*   因此光源可能会对这个小块区域有贡献，但不一定会对根据深度细分后的网格有贡献。所以再划分过后我们可以发现每个网格内的光源数量更少了。
*   **Complexity:O(#vis.frag. * avg #light per tile)->O(#vis. frag. * avg #light per cluster)**

![[6436782f678475e18a8edaaeb7428123_MD5.jpg]]

这两部分讲了如何避免没有意义的 shading 从而提高效率。

### **Level of Detail Solutions**

*   **Level of Detail (LoD) is very important**

*   texture 的 mip-map 就是一个 LOD, 在 high level 时 detail 少一点，但可以很快的计算出一个区域的平均
*   LOD 的核心思路就是为了在任何计算过程中能够快速准确的找出一个正确的 level 去进行各种运算。

*   在工业界中运用 LOD 方法或者在不同情况下选用不同层级的思路称之为”cascaded 方法 “。
*   **Example**

*   **Cascaded shadow maps**

我们知道在生成 shadow map 时我们需要给 shadow map 一个分辨率，当 shadow map 上的一个 texel 离 camera 越近，其在屏幕中所覆盖的内容越多，反之离 camera 越远，其所覆盖的内容越少，因此我们可以知道从 camera 出发看向场景，离 camera 远的我们可以用粗糙的 shadow map。

但我们是无法拥有一个会变化的 shadow map 的，因此在实际操作过程中，我们通常会生成两种或以上不同分辨率的 shadow map 进行使用的，如图，红色区域是一个高分辨率的 shadow map，蓝色区域是覆盖范围更大但分辨率相同也就是更粗糙的 shadow map.

从而我们根据物体在场景的位置来选用 shadow map, 远点的就用粗糙的，近处的就用精细的，但是从图中可以看到会有重叠区域内，因为在突然切换层级时会有一个突变的 artifact, 为了有一个平滑的过度，在这个区域内我们通过距离，也就是近处的以蓝色区域的 shadow map 为主将二者 blend 起来，从而产生平滑的过度。

![[e74aae0f1df87b01849af9a5932ef01f_MD5.jpg]]

*   **Cascaded LPV**

同样的，先在细小的格子上，之后随着距离的增加到大一点的格子，再远就更大，从而省去了很多的计算量。

![[b7e7ced1badd837272978633014580d5_MD5.jpg]]

*   **Key challenge**

*   在不同层级之间的过渡是一个难点
*   通过我们使用 blending 的方法实现手动平滑过度

*   **Another example:geometric LoD**

*   如果有一个精细的有许多三角形的模型（高模），我们可以将其减化，减少其三角形使其变成一个低模，我们可以生成一系列的低模。
*   我们根据物体离 camera 的距离，来选择放什么层级的模型进去（高模还是低模），并且一个物体的不同部分也可以做不同的 LOD，只需要提供一个标准，比如说这个标准是保证任何时候三角形都不会超过一个像素的大小。这样可以保证一个模型可以根据 camera 在哪，从而动态的告诉渲染管线需要渲染什么层级的模型或者部分。
*   但是这样会出现 popping artifacts，因为 transition 是最困难的，如果 camera 推进，在推进过程中由于是突变的，会出现突然变化几何的现象，也就是 popping artifacts，但是我们不用去管它，TAA 可以很好的解决这个现象。
*   This is Nanite in UE5(but of course,Nanite has way more)

*   **FYI,some(strongly) technical difficulties**

*   Different places with different levels,how about cracks?
*   Dynamiclly load and schedule different [levels. how](http://levels.how) to make the best use of cache and bandwidth,etc.?
*   Representing geometry using triangles or geometry textures?
*   Clipping and culling for faster performance?

## Global Illumination Solutions

*   From this course,we can see that

*   Recall,when would screen space ray tracing (SSR) fail?

1.  出了屏幕
2.  反射物在 camera 背后
3.  由于是屏幕空间的几何表示，因此只用了 depth 表示了一层壳，在 trace 时如果在壳的后面是无法知道的。
4.  等各种情况。

没有任何的 GI 解决方法可以将所有的情况都给解决掉，除了 RTRT（实时光线追踪），因为他理论上一个是一个正确的 path tracing，肯定可以解决各种情况。

但是 RTRT 在现如今只适用于部分，如果整体都是用 RTRT 那么帧率自然而然会下降，因为 RTRT 太 costly。

因此工业界经常使用 hybrid solutions，也就是将许多中方法结合在一起。

*   For example,a possible solution to GI may include

*   我们选做一遍 SSR，得到一个近似的 GI。
*   基于 SSR 上，我们对于 SSR 无法得到的结果，专用其他的 ray tracing 方法

*   用硬件 (RTRT) or software(?) 去做 tracing

*   Software ray tracing

*   **近处: 在任意一个 shading point，在他周围的一定范围内的物体我们用一个较高分辨率或称为较高质量的 SDF，SDF 可以让我们在 shader 中快速的 tracing.**
*   **远处：我们则用一个稍低质量的 SDF 将整个场景覆盖，至此不论近处还是远处我们都可以通过 SDF 得到最终光照打到的结果。**
*   **如果场景中有强烈的方向光源或者点光源，也就是手电筒之类的光源，我们则通过 RSM 来解决。**
*   如果场景时 diffuse 的我们则通过 DDGI 来解决。Probes that stores irradiance in a 3D grid (Dynamic Diffuse GI, or DDGI)

*   Hardware ray tracing

*   **Doesn't have to use the original geometry,but low-poly proxies。**

我们没有必要去使用原始 geometry，在 RTRT 中我们可以从任何一个 shading point 去 trace 整个场景，但是这样是没必要的，我们要算的是 indirect illumination，没必要那么准确，因此我们可以用一些简化了的模型去代替原始模型从而加快 trace 的速度，这样 RTRT 就十分快了。

*   Probes (RTXGI)，用 RTRT 的思路结合 PROBE 的思路，叫做 RTXGI。

上述中加粗的四条结合在一起就是 UE5 的 lumen 思路。

It's the end of Games202,but it's not the end of my studying.

To be continued..........XD