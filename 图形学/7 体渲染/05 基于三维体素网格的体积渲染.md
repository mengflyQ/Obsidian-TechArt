## Volume Rendering Based On 3D Voxel Grids  
基于三维体素网格的体积渲染

**Reading time: 33 mins. 阅读时间 33 分钟**

At the end of this chapter, you will be able to produce this sequence of images:  
本章结束时，您将能够制作这一系列图像：
<iframe src="https://www.scratchapixel.com/images/volume-rendering-developers/voldev-smoke.mp4" allow="fullscreen" allowfullscreen="" style="height:100%;width:100%; aspect-ratio: 16 / 9; "></iframe>
As mentioned in the previous chapter, you can create density fields using two techniques: procedurally or using fluid simulation software. In this chapter, we will look at the latter. Note that in this lesson, we won't learn how fluid simulation works.  
如前一章所述，您可以使用两种技术创建密度场：程序化或使用流体模拟软件。在本章中，我们将学习后者。请注意，在本课中，我们不会学习流体模拟的工作原理。  
We will learn how to render the data produced from a fluid sim. We will learn about fluid simulation in the future, promise.  
我们将学习如何渲染流体模拟产生的数据。我们保证，今后还会学习流体模拟。

## Step 1: Using a 3D Grid to Store Density Values  
步骤 1：使用 3D 网格存储密度值

Many techniques for simulating a fluid (smoke, water, etc.) exist, but generally, at some point or another in the process, the result of the simulation is stored in a **3D grid**. For simplicity and this lesson, we will assume that these grids have an equal resolution in all three dimensions (e.g. 32x32x32 in the x, y, and z coordinates respectively) and that this resolution is a power of 2 (8, 16, 32, 64, 128, etc.).  
模拟流体（烟雾、水等）的技术有很多，但一般来说，在模拟过程的某个阶段，模拟结果会存储在三维网格中。为简单起见，在本课中，我们将假设这些网格在所有三个维度上都具有相同的分辨率（例如，在 x、y 和 z 坐标上分别为 32x32x32），并且该分辨率是 2 的幂次（8、16、32、64、128 等）。  
This is just for the sake of this lesson. In practice, it doesn't have to be the case. Also, we won't be transforming the grid in this lesson. So we assume the grid is an axis-aligned box: we can treat our grid as an **axis-aligned bounding box (AABB)** which will simplify our ray-box intersection test. Making the solution generic (aka supporting grids that are not axis aligned can easily be done by transforming the camera ray into the grid object space using the grid's world-to-object transformation matrix as explained in this [lesson](https://www.scratchapixel.com/lessons/3d-basic-rendering/transforming-objects-using-matrices/using-4x4-matrices-transform-objects-3D.html)).  
这只是为了本课的目的。实际上，情况并不一定如此。此外，我们在本课中不会对网格进行变换。因此，我们假设网格是轴对齐的方框：我们可以将网格视为轴对齐的边界方框（AABB），这将简化我们的射线方框相交测试。要使解决方案具有通用性（也就是支持非轴线对齐的网格，可以通过使用网格的世界到对象变换矩阵将摄像机光线变换到网格对象空间中，这一点在本课中已经解释过），就很容易做到。

![[deaa4db9f311058542d22594ba7dcdb9_MD5.png]]

**Figure 1:** a 8x8x8 grid, storing density values.  
图 1：存储密度值的 8x8x8 网格。

Grids are nice to simulate the motion of fluids because, the grid's **voxels** (these are the small volume elements making up the grid which we can also call cells) are set with some initial density (imagine that they are filled up with smoke), and this density is distributed among the neighboring cells, as time progresses.  
网格非常适合模拟流体的运动，因为网格的体素（构成网格的小体积元素，我们也可以称之为单元格）被设定了一定的初始密度（想象它们被烟雾填满），随着时间的推移，这种密度会在相邻的单元格之间分布。  
The way they move from voxel to voxel is ruled by the **Navier-Stokes equation**. But again, this topic is left to another lesson. All you need to know for this lesson is that the results of fluid sims are stored in 3D grids made up of voxels that are storing density values (either 0 or a value greater than 0). **Each voxel in the grid stores one unique density value** (the density "filling up" the volume of the voxel). 3D Grids are too fluid sims, what bitmaps are to images. The density field is quantized.  
它们从一个体素移动到另一个体素的方式由纳维-斯托克斯方程决定。不过，这个话题还是留到下一课再说吧。本课只需了解流体模拟的结果存储在由存储密度值（0 或大于 0 的值）的体素组成的三维网格中。网格中的每个体素存储一个唯一的密度值（密度 "填满 "体素的体积）。三维网格对于流体模拟来说就像位图对于图像一样。密度场是量化的。  
A 3D grid storing density values is rather simple to define in any programming language through they are subtleties that we will look into at the end of this chapter.  
用任何编程语言定义存储密度值的 3D 网格都非常简单，但其中存在一些微妙之处，我们将在本章结尾进行探讨。

Besides the **grid resolution** (the number of voxels it contains in any dimension like 32x32x32), we also need to define the **size of the grid** in world space. That is the size of the object in the scene. We can do this in different ways, but for convenience, we will do so by defining the grid minimum and maximum bound or extent of the grid in world space like so: (-2,-2,2) and (2,2,2).  
除了网格分辨率（在任意维度上包含的体素数量，如 32x32x32），我们还需要定义网格在世界空间中的大小。也就是场景中物体的大小。我们可以用不同的方法来定义网格大小，但为了方便起见，我们将通过定义网格在世界空间中的最小和最大边界或范围来实现：(-2,-2,2) 和 (2,2,2)。  
Note that because our grid is a cube, for now, the values don't need to be symmetrical with respect to the world's origin however, the size of the grid in world space has to be the same in all three dimensions: e.g.  
请注意，由于我们的网格是一个立方体，因此数值暂时不需要与世界原点对称，但网格在世界空间中的大小必须在三个维度上都相同：例如  
(0.2,0.2,0.2), (10.2,10.2,10.2) is fine, (-1.2,2.2,3.2), (8.2,4.2,7.2) is not. Defining our grid world-space size that way is convenient, as the minimum and maximum extents can be plugged directly into the ray-box intersection routine (which we studied in the lesson [A Minimal Ray-Tracer: Rendering Simple Shapes (Sphere, Cube, Disk, Plane, etc.)](https://www.scratchapixel.com/lessons/3d-basic-rendering/minimal-ray-tracer-rendering-simple-shapes/ray-box-intersection.html)). So we already know how to compute the values for t0 and t1 for any ray that intersects a cube (similarly to the way we've done it for our sphere primitive).  
(0.2,0.2,0.2)、(10.2,10.2,10.2) 可以，(-1.2,2.2,3.2)、(8.2,4.2,7.2) 不行。以这种方式定义网格世界空间的大小非常方便，因为最小和最大范围可以直接插入光线盒交汇例程（我们在 "最小光线跟踪器 "一课中学习过）：渲染简单形状（球体、立方体、圆盘、平面等）》一课中学习过）。因此，我们已经知道如何为任何与立方体相交的光线计算 t0 和 t1 的值（与球体基元的计算方法类似）。

You can modify the code from our previous chapters to render cubes instead of spheres. Let's also now use matrices to render the scene from a more interesting point of view. You should get something like this:  
您可以修改前几章的代码来渲染立方体而不是球体。现在，我们还可以使用矩阵从更有趣的角度来渲染场景。你应该得到这样的效果

```
struct Grid {
    float *density;
    size_t dimension = 128;
    Point bounds[2] = { (-30,-30,-30), (30, 30, 30) };
};

bool rayBoxIntersect(const Ray& ray, const Point bounds[2], float& t0, float& t1) {
    ...
    return true;
}

void integrate(const Ray& ray, const Grid& grid, Color& radiance, Color& transmission) {
    Vector lightDir(-0.315798, 0.719361, 0.618702);
    Color lightColor(20);

    float t0, t1;
    if (rayBoxIntersect(ray, grid->bounds, t0, t1) {
        // ray march from t0 to t1

        radiance = ...;
        transmission = ...;
    }
}

Matrix cameraToWorld{ 0.844328, 0, -0.535827, 0, -0.170907, 0.947768, -0.269306, 0, 0.50784, 0.318959, 0.800227, 0, 83.292171, 45.137326, 126.430772, 1 };

int main()
{
    Grid grid;
    // allocate memory to read the data from the cache file
    size_t numVoxels = grid.resolution * grid.resolution * grid.resolution;
    // feel free to use a unique_ptr if you want to (the sample program does)
    grid.density = new float[numVoxels];
    std::ifstream cacheFile;
    cacheFile("./cache.0100.bin", std::ios::binary);
    // read the density values in memory
    cacheFile.read((char*)grid.density, sizeof(float) * numVoxels);

    Point origin(0);
    for (each pixel in the frame) {
        Vector rayDir = ...;
        Ray ray;
        ray.orig = origin * cameraToWorld;
        ray.dir = rayDir * cameraToWorld;
        
        Color radiance = 0, transmission = 1;
        integrate(ray, grid, radiance, transmission);
        
        pixelColor = radiance;
        pixelOpacity = 1 - transmission;
    }

    // free memory
    delete [] grid.density;
}
```

Don't worry too much about the content of the cache file for now. We will look into that later. But from the code above you should see that all we do is create a contiguous memory block to store as many float values as they are voxels in the grid.  
现在不用太担心缓存文件的内容。我们稍后会研究这个问题。但从上面的代码中，你应该可以看到我们所做的就是创建一个连续的内存块来存储网格中尽可能多的浮点数值。  
Then move the values from the disk to memory (line 36). Nothing fancy.  
然后将数值从磁盘移动到内存中（第 36 行）。没什么花哨的。

![[2572a19b6e0fae3861b8f6201f38f821_MD5.png]]

This is the image you should get.  
这就是你应该得到的图像。  
Now that we know that we will be using a 3D grid to store the density field values, that this grid has a resolution and a size, and that we know how to calculate the points where a ray intersects the box (aka the grid), let's see how we read the grid density values as we march along the ray.  
既然我们知道要使用三维网格来存储密度场值，知道网格有分辨率和大小，也知道如何计算射线与方框（又称网格）的交点，那么让我们看看如何在沿射线行进时读取网格密度值。

## Step 2: Ray-marching Through a Grid  
步骤 2：光线穿过网格

Using a grid as an acceleration structure as explained in [this lesson](https://www.scratchapixel.com/lessons/3d-basic-rendering/introduction-acceleration-structure/grid.html) is quite involved. We have to find every single cell from the grid where the ray intersects to see if the geometry (triangles) that each of the intersected cells contains does intersect with the ray as well.  
正如本课所解释的那样，使用网格作为加速度结构是一项相当复杂的工作。我们必须从网格中找到射线相交的每一个单元格，看看每个相交单元格所包含的几何体（三角形）是否也与射线相交。  
While not super complicated this is not a super trivial task either (especially if you want to make it fast). So you might worry that we have to go through this process again. Well good news: not at all.  
虽然不是超级复杂，但也不是超级琐碎的任务（尤其是如果你想快速完成的话）。因此，你可能会担心我们必须再次经历这个过程。好消息是：完全不用。  
With ray-marching, this is a trivial task since all we care about are the sample points along the ray as shown in the image below.  
对于射线行进，这是一项微不足道的任务，因为我们所关心的只是射线沿线的采样点，如下图所示。

![[9acfb53172149e7e5886aef7df916cc1_MD5.png]]

As we know the position of these points, all we are left with is to figure out the position of the voxel in the grid these points overlap (this is the topic of the next item) and read the density value stored in those voxels. No grid traversal is involved. Super simple.  
既然我们知道了这些点的位置，剩下的工作就是找出这些点在网格中重叠的体素的位置（这是下一个项目的主题），然后读取这些体素中存储的密度值。不涉及网格遍历。超级简单。  
Now, let's see how we go from the sample point to the voxel's coordinates and read the value from memory using these coordinates.  
现在，让我们看看如何从采样点到体素坐标，并使用这些坐标从内存中读取数值。

## Step 3: Reading the Density Values As We March Along The Ray  
步骤 3：沿着射线行进时读取密度值

All we need to change from the code we used so far, is the evalDensity function. Instead of evaluating the density field defined by a procedural noise function at any given point in space, we will read the content of the grid at that position instead. That's a very small change. Everything else works the same.  
与目前使用的代码相比，我们只需修改 evalDensity 函数。我们将读取该位置的网格内容，而不是在空间的任意给定点评估由程序噪声函数定义的密度场。这是一个很小的变化。其他一切工作原理不变。  
Normally we know that the sample point is always contained within the bounds of the grid (since it should be somewhere between the points where the ray interests the box).  
通常情况下，我们知道采样点总是包含在网格的边界内（因为它应该位于射线与方框相交的点之间）。  
So this shouldn't be a problem though to make the code more robust, you probably still want to check the sample point coordinates against the grid's bounds, to prevent a crash.  
所以这应该不是问题，不过为了使代码更健壮，你可能还是要根据网格的边界来检查样本点坐标，以防止崩溃。

Now, reading the density value from the grid can have different names. We generally call this a **lookup**. In OpenVDB, which is a library designed to efficiently store and read volume data, this is called **probing** (a rather uncommon name though). Okay, then how do we read the data?  
现在，从网格中读取密度值可以有不同的名称。我们一般称之为查找。在 OpenVDB（一个旨在高效存储和读取卷数据的库）中，这被称为探测（一个相当不常见的名称）。好吧，那么我们该如何读取数据呢？

![[d8742a9d14a627a0e295384baf1b1eaa_MD5.png]]

What we need to do is find a correspondence between the sample point and the voxel from the grid it falls into. The original sample point is defined in world space. So we need to convert that sample point to grid discrete coordinates. To do so we need to:  
我们需要做的是找到采样点和它所在网格中的体素之间的对应关系。原始样本点是在世界空间中定义的。因此，我们需要将采样点转换为网格离散坐标。为此，我们需要

*   Transform the point from **world space** to **object space**. In our situation, all we need to do is remove from the sample position, the grid minimum extent. The sample point is now defined with respect to the "lower left" corner of the grid.
    
*   Then we need to divide the point in object space by the **grid size** (10 in our example). The point's coordinates are now normalized. We can speak of normalized coordinates (the coordinates of the point are contained in the range [0,1].
    
*   Finally, we need to multiply the normalized coordinates by the grid's resolution. We call this space the **voxel space**. At this point, the coordinates are still defined as floating numbers. To compute the grid coordinates and read the value for these coordinates from memory we need to round these floating numbers to the nearest integer.  
    To prevent a crash, we need to be sure that the sample's coordinates in voxel space aren't greater than the grid resolution minus 1 (which can happen if any of the normalized coordinates is exactly 1). We can now read what the density value is at that position.
    

If you look at the (2D) example above, you can see that the point has coordinates (3.36, 3.28) in voxel space.  
如果查看上面的（二维）示例，可以看到该点在体素空间中的坐标为（3.36，3.28）。  
From there we know that finding the index of the voxel in memory is just a matter of multiplying the integer part of the y-coordinate by 8 (the grid resolution) plus 3 (the integer part of the x-coordinate).  
由此我们知道，要在内存中找到体素的索引，只需将 y 坐标的整数部分乘以 8（网格分辨率）再加上 3（x 坐标的整数部分）即可。

Here is the code: 代码如下：

```
float evalDensity(const Grid* grid, const Point& p) {
    Vector gridSize = grid.bounds[1] - grid.bounds[0];
    Vector pLocal = (p - grid.bounds[0]) / gridSize;
    Vector pVoxel = pLocal * grid.baseResolution;

    int xi = static_cast<int>(std::floor(pVoxel.x));
    int yi = static_cast<int>(std::floor(pVoxel.y));
    int zi = static_cast<int>(std::floor(pVoxel.z));

    // nearest neighbor
    return grid->density[(zi * grid->resolution + yi) * grid->resolution + xi]; 
}
```

Here again (like for the Perlin noise function which has a similar need for converting points from world space to lattice space), we use the floor function, because if the normalized point's coordinates are lower than 0 (we will see why this can happen in a moment), we want the function to return 0 and not -1 (which a simple `static_cast` would eventually do).  
在这里（就像佩林噪声函数一样，它也需要将点从世界空间转换到网格空间），我们再次使用了 floor 函数，因为如果归一化点的坐标小于 0（我们稍后会看到为什么会出现这种情况），我们希望函数返回 0，而不是 -1 （简单的 `static_cast` 最终会做到这一点）。

We call this method the **nearest-neighbor search** interpolation because what you essentially do is retrieve the coordinates of the voxels the sample point falls into (as such it's not an interpolation). Other methods exist, though, they all start by calculating the coordinates of that voxel.  
我们称这种方法为近邻搜索插值法，因为它的本质是获取样本点所在体素的坐标（因此它不是插值法）。虽然还有其他方法，但它们都是从计算该体素的坐标开始的。  
These other methods involve interpolating the values of the neighboring voxels which we will study next.  
其他方法涉及对相邻体素值进行内插，我们将在下文中进行研究。  
But before we look into that topic, we now have everything we need to render a first image of the fluid sim (various fluid caches are provided in the source code section for your to download alongside the sample program). Let's see what it looks like.  
不过，在研究这个主题之前，我们现在已经拥有了渲染流体模拟的第一幅图像所需的一切（源代码部分提供了各种流体缓存，供您在下载示例程序的同时下载）。让我们来看看它是什么样子的。

![[445d34e5ec20ce64a31ea574ef3dcc8e_MD5.png]]

As you know, we aim to provide examples that have no dependency on external libraries. Consequently, we are not using an industry format such as OpenVDB (or Field3D, OpenVDB's ancestor) to store our grid data.  
众所周知，我们的目标是提供不依赖外部库的示例。因此，我们没有使用 OpenVDB（或 Field3D，OpenVDB 的祖先）等行业格式来存储网格数据。  
Instead, we dumped the data of our fluid sim to a binary file in the most basic way using something similar to:  
取而代之的是，我们将流体模拟的数据转储到一个二进制文件中，最基本的方法与下面类似：

```
// our binary cache format
for (size_t z = 0; z < grid.res; ++z) {
    for (size_t y = 0; y < grid.res; ++y) {
        for (size_t x = 0; x < grid.res; ++x) {
            float density = grid.density[x][y][z];
            cacheFile.write((char*)&density, sizeof(float));
        }
    }
}
```

Note that OpenVDB works on the same principles, however, they support compression and things such as multi-resolution and sparse volumes (something we will touch on at the end of this lesson).  
请注意，OpenVDB 的工作原理与此相同，但它们支持压缩以及多分辨率和稀疏卷等功能（我们将在本课结束时介绍）。  
Our approach has the merit of being straightforward which is what we look for when it comes to teaching and learning. So, what you need to pay attention to in the code above, is that we store slices of voxels back to front, along the z-axis.  
我们的方法具有简单明了的优点，这也是我们在教学过程中所追求的。因此，在上面的代码中，你需要注意的是，我们沿 Z 轴从前到后存储体素切片。  
Keep in mind that (for a right-hand coordinate system) the minimum extent of our grid is located at the back of the grid, in the lower-left corner (the magenta voxel in the figure below).  
请记住（对于右手坐标系），我们网格的最小范围位于网格的背面，即左下角（下图中的洋红色体素）。  
In voxel space, the first voxel in the grid has coordinates (0,0,0) and in our example, the last voxel (front of the grid, along the positive z-axis, in the upper-right corner) has coordinates (8,8,8) (the yellow voxel in the figure below.  
在体素空间中，网格中第一个体素的坐标是（0,0,0），而在我们的例子中，最后一个体素（网格前方，沿 Z 轴正向，位于右上角）的坐标是（8,8,8）（下图中的黄色体素）。  
In this example, the grid resolution is 8).  
在本例中，网格分辨率为 8）。

![[c5d44edab90f3331a9bd18140cc7c970_MD5.png]]

So if we map out the voxels in a simple float array like we do when we dumped the data out to the file (and when we read them back into memory from the file), the first voxel in this array as index 0 and the last voxel has index 511 (it's a [0-based array](https://en.wikipedia.org/wiki/Zero-based_numbering), so 8*8*8 minus 1). To compute the index of a voxel from its voxel-space coordinates, all we need to do is:  
因此，如果我们将体素映射到一个简单的浮点数组中，就像我们将数据转储到文件中时所做的那样（以及我们从文件中将它们读回内存时所做的那样），那么这个数组中的第一个体素的索引为 0，最后一个体素的索引为 511（这是一个基于 0 的数组，所以是 8*8*8 减 1）。要根据体素空间坐标计算出体素的索引，我们需要做的是

```
size_t index = (zi * gridResolution + yi) * gridResolution + xi;
```

This is the most basic way of getting a result. However, we can improve the image quality using what we call trilinear interpolation. Let's see how this works.  
这是获得结果的最基本方法。不过，我们可以使用所谓的三线性插值来提高图像质量。让我们来看看它是如何工作的。

# Step 4: Improve the Result With a Trilinear Interpolation  
# 第 4 步：用三线插值法改进结果

The underlying idea is as follows: a voxel represents a single density value however a sample point can be located anywhere within the volume of any given voxel.  
其基本思想如下：一个体素代表一个密度值，但采样点可以位于任何给定体素体积内的任何位置。  
Therefore it makes sense to assume that the density value at this location should somehow be a mix between the density of the voxel that the sample point is in as well as the density values of the voxels that are directly next to it as shown in the image below.  
因此，我们可以假定，该位置的密度值应该是样本点所在体素的密度值与旁边体素密度值的混合值，如下图所示。

![[e9a96624bb01336a4a6d4f6f83514aca_MD5.png]]

For the trilinear interpolation, we simply mix the values of 8 voxels. How do we mix them? Simply by calculating the distance from the sample point to each one of these 8 voxels and using these distances to weigh the contribution of each one of these 8 voxels to the final result.  
对于三线插值，我们只需混合 8 个体素的值。如何混合呢？只需计算样本点到这 8 个体素中每个体素的距离，并利用这些距离来权衡这 8 个体素中每个体素对最终结果的贡献。

The process is a [linear interpolation](https://www.scratchapixel.com/lessons/mathematics-physics-for-computer-graphics/interpolation/introduction.html) but in 3D. The equation we are using to interpolate 2 values is where the varies from 0 and 1\. In 2D, this process is called a [bilinear interpolation](https://www.scratchapixel.com/lessons/mathematics-physics-for-computer-graphics/interpolation/bilinear-filtering.html) and requires 4 pixels. In 3D, we call this a trilinear interpolation and it requires 8 voxels. Note that the equation used to interpolate between a and b is called an **interpolant**. In the case of linear interpolation, the interpolant is a first-order polynomial.  
这个过程是线性插值，不过是三维插值。我们用来对 2 个值进行插值的等式为 0 和 1。在二维中，这一过程称为双线性插值，需要 4 个像素。在 3D 中，我们称之为三线插值，需要 8 个体素。需要注意的是，用于在 a 和 b 之间进行插值的等式称为内插剂。在线性插值的情况下，插值器是一阶多项式。

We assume that the density stored in a given voxel is the value that the sample point should take if it was exactly in the middle of that voxel.  
我们假定，存储在给定体素中的密度是采样点恰好位于该体素中间时的值。  
Due to the nature of the trilinear interpolation scheme, to get that result, we need to shift the sample position in voxel space by -0.5\. Let's look at the code first and then explain how it works.  
由于三线性插值方案的性质，要得到这一结果，我们需要将体素空间中的样本位置移动 -0.5\。让我们先看看代码，然后再解释它是如何工作的。

```
float evalDensity(const Grid* grid, const Point& p)
{
    Vector gridSize = grid.bounds[1] - grid.bounds[0];
    Vector pLocal = (p - grid.bounds[0]) / gridSize;
    Vector pVoxel = pLocal * grid.baseResolution;
    <span style="color: red; font-weight: 700;">Vector pLattice(pVoxel.x - 0.5, pVoxel.y - 0.5, pVoxel.z - 0.5);</span>
    int xi = static_cast<int>(std::floor(pLattice.x));
    int yi = static_cast<int>(std::floor(pLattice.y));
    int zi = static_cast<int>(std::floor(pLattice.z));

    float weight[3];
    float value = 0;

    // trilinear interpolation
    for (int i = 0; i < 2; ++i) {
        weight[0] = 1 - std::abs(pLattice.x - (xi + i));
        for (int j = 0; j < 2; ++j) {
            weight[1] = 1 - std::abs(pLattice.y - (yi + j));
            for (int k = 0; k < 2; ++k) {
                weight[2] = 1 - std::abs(pLattice.z - (zi + k));
                value += weight[0] * weight[1] * weight[2] * grid(xi + i, yi + j, zi + k);
            }
        }
    }

    return value;
}
```

Now let's see how (and why) this works.  
现在让我们来看看它是如何工作的（以及为什么）。

![[7427dbdc337d499e94fce884d7c980d1_MD5.png]]

In the image above, the two techniques are illustrated. In the top-left corner, you have an example of the nearest-neighbor interpolation method. Nothing fancy here. We have 4 voxels (in the 2D case, 8 in 3D), and the sample point (the blue dot) falls in one of them.  
上图展示了这两种技术。左上角是近邻插值法的示例。这里没有什么花哨的东西。我们有 4 个体素（二维情况下为 8 个，三维情况下为 8 个），样本点（蓝点）位于其中之一。  
Therefore the sample point takes on the density value stored in that voxel.  
因此，采样点将采用存储在该体素中的密度值。

Things get a little bit more complicated with the trilinear interpolation. You will probably agree that if our sample point was right in the middle of our four voxels, we should probably return the average of the density value stored in these four voxels.  
三线插值的情况会变得复杂一些。您可能会同意，如果我们的采样点位于四个体素的正中间，那么我们应该返回存储在这四个体素中的密度值的平均值。  
That is, in this particular example the sum of the voxel density values is divided by 4.  
也就是说，在这个例子中，体素密度值的总和除以 4。

```
result = (0.9 + 0.14 + 0.08 + 0.63) / 4
```

Now this works ok if the point is right at the cross-over of four voxels which is an exceptional case. So we need to find a generic solution.  
现在，如果点正好位于四个体素的交叉点上（这是一种特殊情况），这种方法就可以正常工作。因此，我们需要找到一种通用的解决方案。

First (we will explain why we need to do this later), we need to offset our sample point by half a voxel space: (-0.5, -0.5, -0.5).  
首先（稍后我们会解释为什么要这样做），我们需要将采样点偏移半个体素空间：（-0.5, -0.5, -0.5）。  
If we keep the example of our sample point that falls exactly right in the middle of our four voxels, the sample point with the offset applied is now located right in the middle of the lower-left voxel.  
如果我们继续以正好位于四个体素中间的采样点为例，则应用偏移后的采样点现在位于左下方体素的正中间。  
What we do next compute the distances from this point to each one of the voxel boundaries.  
接下来，我们将计算该点到每个体素边界的距离。  
In our illustration (2D case) this is represented by the distance dx0 (the distance from the sample point x coordinate to x0, the voxel lower-left x coordinate), dy0 (the distance from the sample point y coordinate to y0, the voxel lower-left y-coordinate), dx1 (the distance from the sample point x coordinate to x1, the voxel upper-right x-coordinate) and dy1 (the distance from the sample point y coordinate to y1, the voxel upper right y-coordinate).  
在我们的示例中（二维情况），这由距离 dx0（从采样点 x 坐标到 x0 的距离，即体素左下 x 坐标）、dy0（从采样点 y 坐标到 y0 的距离，即体素左下 y 坐标）、dx1（从采样点 x 坐标到 x1 的距离，即体素右上 x 坐标）和 dy1（从采样点 x 坐标到 x1 的距离，即体素右上 x 坐标）来表示、dx1（取样点 x 坐标到 x1（体素右上方 x 坐标）的距离）和 dy1（取样点 y 坐标到 y1（体素右上方 y 坐标）的距离）。  
These are technically called the **Manhattan distances**.  
严格来说，这些距离被称为曼哈顿距离。

Manhattan distance: the distance between two points measured along axes at right angles. In a plane with p1 at (x1, y1) and p2 at (x2, y2), it is |x1 - x2| + |y1 - y2|.  
曼哈顿距离：沿直角轴测得的两点间的距离。在 p1 位于 (x1, y1) 和 p2 位于 (x2, y2) 的平面上，曼哈顿距离为 |x1 - x2| + |y1 - y2|。

In practice, as we will see, we only need dx0, dy0, and dz0\. Here is the general idea of the trilinear interpolation again (check the [lesson devoted to interpolation](https://www.scratchapixel.com/lessons/mathematics-physics-for-computer-graphics/interpolation/trilinear-interpolation.html) if you want a more complete explanation).  
实际上，正如我们将要看到的，我们只需要 dx0、dy0 和 dz0\. 下面是三次线性插值的总体思路（如果想要更完整的解释，请查看插值课程）。

![[8ca8a6642682cba64645ec51210f84c7_MD5.png]]

We have a block of 2x2x2 voxels. Let's call them v000, v100 (moving 1 voxel to the right), v010 (moving 1 voxel up), v110 (1 to the right, 1 up), v001 (moving 1 voxel forward), v101 (you get the idea by now), v011 and v111. The idea is to linearly interpolate v000-v100, v010-v110, v001-v1001, and v011-v111 using x0 as the weight.  
我们有一个 2x2x2 像素块。让我们称它们为 v000、v100（向右移动 1 个体素）、v010（向上移动 1 个体素）、v110（向右移动 1 个体素，向上移动 1 个体素）、v001（向前移动 1 个体素）、v101（你现在应该明白了）、v011 和 v111。我们的想法是使用 x0 作为权重，线性插值 v000-v100、v010-v110、v001-v1001 和 v011-v111。  
This gives us 4 values that we then linearly interpolate (in pairs again) using y0 as the weight. We are left with 2 values that we then finally linearly interpolate using z0 as the weight.  
这样我们就得到了 4 个值，然后用 y0 作为权重进行线性插值（再次成对）。最后剩下 2 个值，使用 z0 作为权重进行线性插值。  
Note that whether you start the first 4 linear interpolations with x0, y0 or z0 doesn't make any difference as long as you linearly interpolate the successive results using a different dimension each time. Remember that our linear interpolant is where is the weight.  
请注意，无论从 x0、y0 还是 z0 开始进行前 4 次线性插值，只要每次使用不同的维度对连续结果进行线性插值，就不会有任何区别。请记住，我们的线性插值是权重。  
In code, we get something like this:  
在代码中，我们会得到这样的结果

```
float result = 
    (1 - z0) * (  // blue
                (1 - y0) * (v000 * (1 - x0) + v100 * x0) + // green and red
                     y0  * (v010 * (1 - x0) + v110 * x0)   // green and red
                ) + 
          z0 * ( // blue
                (1 - y0) * (v001 * (1 - x0) + v101 * x0) + // green and red
                     y0  * (v011 * (1 - x0) + v111 * x0)); // green and red
```

Hopefully, the layout helps you see the three nested levels of interpolation. With 4 (red) + 2 (green) + 1 (blue) interpolations.  
希望这样的布局能帮助您看清三个嵌套层次的插值。4（红色）+ 2（绿色）+ 1（蓝色）插值。  
Now if we replace (1-x0), (1-y0), (1-z0), x0, y0, and z0 with wx0, wy0, wz0, wx1, wy1, and wz1 respectively, expand and rearrange the terms from the previous code snippet (2), we get:  
现在，如果我们将 (1-x0)、(1-y0)、(1-z0)、x0、y0 和 z0 分别替换为 wx0、wy0、wz0、wx1、wy1 和 wz1，展开并重新排列前面代码片段 (2) 中的项，就可以得到：

```
float result = 
    v000 * wx0 * wy0 * wz0 +
    v100 * wx1 * wy0 * wz0 +
    v010 * wx0 * wy1 * wz0 +
    v110 * wx1 * wy1 * wz0 +
    v001 * wx0 * wy0 * wz1 +
    v101 * wx1 * wy0 * wz1 +
    v011 * wx0 * wy1 * wz1 +
    v111 * wx1 * wy1 * wz1;
```

That's the code from our snippet 1 (though in snippet 1 the weights are calculated on the fly in nested loops). If you do the math using the 2D example from the image above where dx0 = dy0 = 0.5, we get:  
这就是我们代码片段 1 中的代码（不过在代码片段 1 中，权重是在嵌套循环中即时计算的）。如果使用上图中的二维示例（dx0 = dy0 = 0.5）进行计算，我们可以得到以下结果：

```
result = 
    0.9 * (1-dx0) * (1-dy0) + 0.14 * dx0 * (1-dy0) *  + 0.08 * (1-dx0) * dy0 + 0.63 * dx0 * dy0 = 
    0.9 * 0.25 + 0.14 * 0.25 + 0.08 * 0.25 + 0.63 * 0.25
```

Which is the result we were looking for. Great, trilinear interpolation (bilinear in the 2D case) works. You also now understand why we need to offset the sample point by -0.5 in voxel space. This is required for the math to work.  
这正是我们想要的结果。很好，三线性插值（二维情况下为双线性插值）成功了。现在你也明白了为什么我们需要在体素空间中将采样点偏移 -0.5。这是数学运算的必要条件。  
To be convinced, consider what happens when the sample point is right in the middle of the voxel (before we apply the offset). Once the offset is applied, the point will then lie right in the lower-left corner of the voxel (in 2D).  
为了使我们信服，请考虑一下当采样点位于体素的正中间时（在我们应用偏移之前）会发生什么情况。应用偏移后，采样点将位于体素的左下角（二维）。  
That means that dx0 = dy0 (=dz0) =0 while all the other distances will be 1. And you get:  
这意味着 dx0 = dy0 (=dz0) =0，而所有其他距离都是 1。这样就得到

```
float result = grid.density(voxelX, voxelY, voxelZ) * (1-0) * (1-0) * (1-0) +
               0 + // 0 * (1-0) * (1-0)
               0 + // (1-0) * 0 * (1-0)
               0 + // 0 * 0 * (1-0)
               0 + // (1-0) * (1-0) * 0
               0 + // 0 * (1-0) * 0
               0 + // (1-0) * 0 * 0
               0;  // 0 * 0 * 0
```

In other words, the value returned for that sample will be the value stored in that voxel (0.9 in our example if we assume the sample point before the offset is in the middle of the lower-left voxel) which is exactly what you would expect.  
换句话说，该样本返回的值将是存储在该体素中的值（在我们的例子中，如果假设偏移前的样本点位于左下方体素的中间，则为 0.9），这正是您所期望的。

That's all there is to a trilinear interpolation. Now let's see how different the result is compared to the nearest-neighbor method.  
这就是三线插值法的全部内容。现在让我们看看与近邻法相比，结果有什么不同。

![[eabdf75d9b85dfd23e4794e8e1ca3ce1_MD5.png]]

As you can see the image of the cache that was rendered using trilinear interpolation (right) is softer. Though this improvement comes at a price, as render time increases significantly when this method is used (compared to the nearest-neighbor method).  
可以看到，使用三线性插值（右图）渲染的缓存图像更加柔和。但这种改进是有代价的，因为使用这种方法（与最近邻方法相比）时，渲染时间会大幅增加。  
A trilinear interpolation takes about 5 times as much as it takes to do a nearest-neighbor lookup (assuming no optimization).  
三线性插值所需的时间约为最近邻查找（假设未进行优化）的 5 倍。

## Now you can read other people's code (again): OpenVDB  
现在你又可以阅读别人的代码了：OpenVDB

Let's see how [OpenVDB does its trilinear interpolation](https://github.com/AcademySoftwareFoundation/openvdb/blob/6cf972a061db7018e636c0c83b81707c8b7d5b89/openvdb/openvdb/tools/Interpolation.h):  
让我们看看 OpenVDB 是如何进行三线插值的：

```
template<class ValueT, class TreeT, size_t N> 
inline bool 
BoxSampler::probeValues(ValueT (&data)[N][N][N], const TreeT& inTree, Coord ijk) 
{ 
    bool hasActiveValues = false; 
    hasActiveValues |= inTree.probeValue(ijk, data[0][0][0]);  //i, j, k 
 
    ijk[2] += 1; 
    hasActiveValues |= inTree.probeValue(ijk, data[0][0][1]);  //i, j, k + 1 
 
    ijk[1] += 1; 
    hasActiveValues |= inTree.probeValue(ijk, data[0][1][1]);  //i, j+1, k + 1 
 
    ijk[2] -= 1; 
    hasActiveValues |= inTree.probeValue(ijk, data[0][1][0]);  //i, j+1, k 
 
    ijk[0] += 1; 
    ijk[1] -= 1; 
    hasActiveValues |= inTree.probeValue(ijk, data[1][0][0]);  //i+1, j, k 
 
    ijk[2] += 1; 
    hasActiveValues |= inTree.probeValue(ijk, data[1][0][1]);  //i+1, j, k + 1 
 
    ijk[1] += 1; 
    hasActiveValues |= inTree.probeValue(ijk, data[1][1][1]);  //i+1, j+1, k + 1 
 
    ijk[2] -= 1; 
    hasActiveValues |= inTree.probeValue(ijk, data[1][1][0]);  //i+1, j+1, k 
 
    return hasActiveValues; 
} 
 
template<class ValueT, size_t N> 
inline ValueT 
BoxSampler::trilinearInterpolation(ValueT (&data)[N][N][N], const Vec3R& uvw) 
{ 
    auto _interpolate = [](const ValueT& a, const ValueT& b, double weight) 
    { 
        const auto temp = (b - a) * weight; 
        return static_cast<ValueT>(a + ValueT(temp)); 
    }; 
 
    // Trilinear interpolation:
    // The eight surrounding lattice values are used to construct the result. \n
    // result(x,y,z) =
    //     v000 (1-x)(1-y)(1-z) + v001 (1-x)(1-y)z + v010 (1-x)y(1-z) + v011 (1-x)yz
    //   + v100 x(1-y)(1-z)     + v101 x(1-y)z     + v110 xy(1-z)     + v111 xyz
 
    return  _interpolate( 
                _interpolate( 
                    _interpolate(data[0][0][0], data[0][0][1], uvw[2]), 
                    _interpolate(data[0][1][0], data[0][1][1], uvw[2]), 
                    uvw[1]), 
                _interpolate( 
                    _interpolate(data[1][0][0], data[1][0][1], uvw[2]), 
                    _interpolate(data[1][1][0], data[1][1][1], uvw[2]), 
                    uvw[1]), 
                uvw[0]); 
} 
 
template<class TreeT> 
inline bool 
BoxSampler::sample(const TreeT& inTree, const Vec3R& inCoord, 
                   typename TreeT::ValueType& result) 
{ 
    ... 
    const Vec3i inIdx = local_util::floorVec3(inCoord); 
    const Vec3R uvw = inCoord - inIdx; 
 
    // Retrieve the values of the eight voxels surrounding the
    // fractional source coordinates.
    ValueT data[2][2][2]; 
 
    const bool hasActiveValues = BoxSampler::probeValues(data, inTree, Coord(inIdx)); 
 
    result = BoxSampler::trilinearInterpolation(data, uvw); 
 
    ... 
}
```

The `sample()` method is first fetching the values stored in the 8 voxels. This happens in the method `probeValues`. Then, the voxels are interpolated as described above (in the `trilinearInterpolation` method). The function `_interpolate` is nothing else than a linear interpolation more often called `lerp` in code.  
`sample()` 方法首先获取存储在 8 个体素中的值。这发生在 `probeValues` 方法中。. 然后，如上所述（在 `trilinearInterpolation` 方法中）对这些体素进行插值。函数 `_interpolate` 就是线性插值，在代码中通常称为 `lerp` 。

## Going further with volume caches  
更进一步的容量缓存

We are just listing a series of topics that we think are worth mentioning here for reference. To keep this lesson reasonably short, we won't get into much detail for now.  
我们只是列出了一系列我们认为值得一提的主题，以供参考。为使本课简明扼要，我们暂时不做过多介绍。  
Most of these topics will be either developed in a future revision of this lesson or studied in separate lessons.  
这些主题中的大多数将在本课今后的修订版中加以阐述，或在单独的课程中学习。

### Other interpolation schemes: cubic and filter kernels  
其他插值方案：立方内核和滤波内核

As mentioned earlier, the linear interpolant is a first-order polynomial. It is possible to use higher-order polynomials for smoother results. You can for example use cubic interpolation. In 2D, we need to sample 4x4 pixels to perform a bicubic interpolation.  
如前所述，线性插值是一阶多项式。使用高阶多项式可以获得更平滑的结果。例如，您可以使用三次插值。在二维图像中，我们需要采样 4x4 像素来执行双三次插值。  
In 3D, we need voxels. As you can imagine, the result will look smoother, but the render time will increase.  
在 3D 中，我们需要体素。可以想象，这样的效果看起来会更流畅，但渲染时间也会增加。

You can also use filter kernels, similar to those used in image filtering (such as a triangle, Mitchell, or Gaussian filter kernels). Like with images, the larger the filter's extent, the longer the processing time.  
您还可以使用滤波器内核，类似于图像滤波中使用的滤波器内核（如三角形、米切尔或高斯滤波器内核）。与图像一样，滤波器的范围越大，处理时间就越长。

Both techniques will be fully explained in a future revision of this lesson.  
这两种技术将在本课今后的修订版中进行全面讲解。

### Storing more data alongside density  
在提高密度的同时存储更多数据

Volume cache formats used in production are generally designed to let you store any channel you want in the cache alongside the density such as the temperature (useful to render fire), the velocity (useful to render 3D motion blur), etc.  
生产中使用的体积缓存格式通常是为了让您可以在缓存中存储任何您想要的密度通道，如温度（用于渲染火焰）、速度（用于渲染 3D 运动模糊）等。

### Filtering and brick maps 过滤和砖块地图

If you render volume caches for production purposes, you should probably care about filtering and LODs. The problem you may have with rendering volume caches is the same as with rendering textures.  
如果您是出于制作目的而渲染体缓存，那么您可能应该关心过滤和 LOD。渲染体缓存时可能会遇到的问题与渲染纹理时一样。  
Textures have a fixed resolution (number of pixels like 512, 1024, and so on) and if you render an object to which a given texture is applied from super far away, then you assume your object is just a few pixels wide in the image, as the object moves or the camera moves, each time a new frame is rendered different pixels or texels from the textures will be rendered.  
纹理有固定的分辨率（像素数，如 512、1024 等），如果从很远的地方渲染一个应用了特定纹理的物体，则假定物体在图像中只有几个像素宽，当物体移动或摄像机移动时，每次渲染新的帧都会渲染纹理中的不同像素或色素。  
As a result, the texture will appear to jiggle (to say the least, but it more often simply looks like a kind of noise) from frame to frame.  
因此，纹理在帧与帧之间会出现抖动（至少可以说是抖动，但更常见的是像一种噪音）。  
To minimize this problem, we make smaller versions of the texture stored alongside the original texture data; we call these lower-resolution versions of the original texture **mipmaps**. Using a lower resolution of the texture when the object is far away (small in the image) removes the noise (technically called aliasing). We haven't touched on the topic of mipmap yet, but we will soon.  
为了尽量减少这一问题，我们将原始纹理数据存储在较小的纹理版本中；我们称这些原始纹理的低分辨率版本为 mipmaps。当物体距离较远（在图像中较小的）时，使用较低分辨率的纹理可以消除噪点（技术上称为混叠）。我们还没有谈到 mipmap 的话题，但很快就会讨论。

In the meantime and if you are already familiar with the concept of mipmap, what they are, what they are used for, and how to create them, you should know that the problem we have with filtering 2D textures is also a problem we have with 3D caches (or 3D textures for that matter).  
与此同时，如果你已经熟悉了 mipmap 的概念、它们是什么、有什么用途以及如何创建它们，你就应该知道我们在过滤 2D 纹理时遇到的问题也是我们在处理 3D 缓存（或 3D 纹理）时遇到的问题。  
Therefore, we can adapt the mipmap method to 3D caches. For that, you need to create downsized versions of the original grid data and use the right level (downsized version), based on how small the cache is in the image.  
因此，我们可以将 mipmap 方法应用于 3D 缓存。为此，需要创建原始网格数据的缩小版本，并根据缓存在图像中的大小使用合适的级别（缩小版本）。  
While the process is very similar to mipmaps and while you can call these downsized versions mipmaps as well, in 3D we call them **brick maps** instead. They look like Minecraft objects.  
虽然这个过程与 mipmaps 非常相似，而且你也可以把这些缩小版的 mipmaps 称为砖块地图，但在 3D 中，我们称它们为砖块地图。它们看起来就像 Minecraft 中的物体。

The term **brick map** was coined by the [Pixar Renderman's](https://renderman.pixar.com/resources/RenderMan_20/brickmapgprim.html) team we believe it but we are not too sure about its origin... if someone knows, please write to us about it. But generally, the idea of having a cache storing multiple resolutions of its data is not particularly new or uncommon.  
我们相信砖块地图一词是由皮克斯 Renderman 团队创造的，但我们不太确定它的起源...... 如果有人知道，请写信告诉我们。不过一般来说，让缓存存储数据的多个分辨率并不是什么特别新颖或罕见的想法。  
It's probably supported by all production renderers and formats such as OpenVDB.  
可能所有的生产渲染器和格式（如 OpenVDB）都支持它。

![[d3752a55a1535b2dea500c87e3029b32_MD5.png]]

Building these brick maps is very similar to building the texture's mipmap levels, however, we are taking the average of 8 voxels (instead of taking the average of 4 pixels or texels).  
绘制这些砖块贴图与绘制纹理的 mipmap 层次非常相似，不过我们是取 8 个体素的平均值（而不是取 4 个像素或纹理的平均值）。  
That's why dealing with grids whose resolution is exactly a power of 2 is convenient, as downsizing the levels becomes trivial in this case.  
这就是为什么处理分辨率正好是 2 的幂次的网格非常方便，因为在这种情况下，缩小层次变得微不足道。

Here is some code that shows how you can create these levels from our original fluid sim caches (their base resolution at level 0 is 128). This is just a quick example for now until we get a chance to address this topic properly:  
下面的代码展示了如何从原始的流体模拟缓存中创建这些关卡（关卡 0 的基本分辨率为 128）。在我们有机会适当解决这个问题之前，这只是一个快速示例：

```
size_t baseResolution = 128;
size_t numLevels = log2(baseResolution); /* float to size_t implicit cast */
std::unique_ptr<Grid []> gridLod = std::make_unique<Grid []>(numLevels - 2); // ignore res. 2 and 4

// load level 0 data
gridLod[0].baseResolution = baseResolution;
std::ifstream ifs;
char filename[256];
sprintf_s(filename, "./grid.%d.bin", frame);
ifs.open(filename, std::ios::binary);
gridLod[0].densityData = std::make_unique<float[]>(baseResolution * baseResolution * baseResolution);
ifs.read((char*)gridLod[0].densityData.get(), sizeof(float) * baseResolution * baseResolution * baseResolution);
ifs.close();

for (size_t n = 1; n < numLevels - 2; ++n) {
    baseResolution /= 2;
    gridLod[n].baseResolution = baseResolution;
    gridLod[n].densityData = std::make_unique<float[]>(baseResolution * baseResolution * baseResolution);
    for (size_t x = 0; x < baseResolution; ++x) {
        for (size_t y = 0; y < baseResolution; ++y) {
            for (size_t z = 0; z < baseResolution; ++z) {
                gridLod[n](x, y, z) = 
                    0.125 * (gridLod[n - 1](x * 2,     y * 2,     z * 2) + 
                             gridLod[n - 1](x * 2 + 1, y * 2,     z * 2) +
                             gridLod[n - 1](x * 2,     y * 2 + 1, z * 2) +
                             gridLod[n - 1](x * 2 + 1, y * 2 + 1, z * 2) +
                             gridLod[n - 1](x * 2,     y * 2,     z * 2 + 1) + 
                             gridLod[n - 1](x * 2 + 1, y * 2,     z * 2 + 1) +
                             gridLod[n - 1](x * 2,     y * 2 + 1, z * 2 + 1) +
                             gridLod[n - 1](x * 2 + 1, y * 2 + 1, z * 2 + 1));
			}
		}
	}
}
...
// to render level 3 of the cache for example (resolution: 16)
trace(ray, L, transmittance, rc, gridLod[3]);
...
```

We won't explain how to select the right level for now, but we will in a future revision of the lesson. In the meantime, at least you are aware of the problem and what the solution could be.  
我们暂时不解释如何选择合适的级别，但我们会在今后的课程修订中解释。在此期间，至少你们已经意识到了问题所在，以及可能的解决办法。

Here is an image of our cache rendered at different levels (level 0 is the original cache resolution, that is 128 in our example):  
下面是不同级别缓存的渲染图像（0 级为原始缓存分辨率，在我们的示例中为 128）：

![[af68f7848a40968d077438e6c1074e5d_MD5.png]]

### Motion blur 运动模糊

What about 3D motion blur? To render this effect we need to store the voxel with the motion of the fluid. This is typically represented as a direction called the motion vector.  
三维运动模糊效果如何？要呈现这种效果，我们需要存储流体运动的体素。这通常表现为一个称为运动矢量的方向。  
This vector's direction indicates in which average direction the fluid is moving through the grid; its magnitude is how fast the fluid moves. Using this information, 3D motion blur can be simulated at render time.  
该矢量的方向表示流体在网格中移动的平均方向；矢量的大小表示流体移动的速度。利用这些信息，可以在渲染时模拟 3D 运动模糊。

This topic will be studied in a future lesson.  
这一主题将在以后的课程中学习。

### Advection 平流

While advection is not directly related to volume rendering (it has more to do with fluid simulation), we are adding it here as a reminder (so that we can write a lesson about it in the future if we can). It can be done at render time to add details to an existing fluid sim.  
虽然平流与体积渲染没有直接关系（它与流体模拟有更大的关系），但我们还是将其添加到这里以作提醒（这样我们就可以在将来编写有关它的课程）。它可以在渲染时完成，为现有的流体模拟添加细节。

### Sparse volumes: what are they?  
稀疏卷：它们是什么？

A lot of the time, only a fraction of the voxels in a grid are storing density values that are greater than 0. That leaves a lot of "empty" voxels; storing them can potentially be a significant waste.  
很多时候，网格中只有一小部分体素存储的密度值大于 0，这就留下了很多 "空 "体素；存储这些体素可能会造成严重浪费。  
Similarly, you may have a group of 8 voxels storing the same density value for example 0.138. This is also a waste of space. Sparse volumes were designed to address this issue.  
同样，您可能会有一组 8 个体素存储相同的密度值，例如 0.138。这也是一种空间浪费。稀疏体积就是为了解决这个问题而设计的。

The general idea is like this: we create a voxel that has the size of a block of 2x2x2 voxels. Therefore this bigger voxel is twice the size of the original voxels.  
总体思路是这样的：我们创建一个体素，其大小相当于一个 2x2x2 体素块。因此，这个更大的体素是原始体素大小的两倍。  
Next: if all the voxels in the block of 2x2x2 voxels have the same density value (it can be 0 or any value greater than 0 for example 0.139), we then delete the group of 2x2x2 voxels and store the single value in the larger voxel (say 0.139 to follow our example), otherwise, the bigger voxel points to the original block of 2x2x2 voxels.  
下一步：如果 2x2x2 象素块中的所有象素都具有相同的密度值（可以是 0 或任何大于 0 的值，例如 0.139），我们就会删除这组 2x2x2 象素，并将单个值存储在较大的象素中（例如 0.139），否则，较大的象素就会指向原来的 2x2x2 象素块。

This process is recursive. We start from the highest resolution grid (highest level), then build the lower levels from the bottom up. Voxels pointing to blocks of more voxels are **internal nodes** whereas voxels storing a value are **leaf nodes**. The image below shows an example of a sparse 2D volume.  
这个过程是递归的。我们从分辨率最高的网格（最高层）开始，然后自下而上建立较低层次的网格。指向更多体素块的体素是内部节点，而存储数值的体素是叶子节点。下图显示了一个稀疏二维体的示例。

![[591de393b9a6f337f44503c00115ba27_MD5.png]]

When a block isn't merged, we store 1 (the bigger voxel) + 8 voxels (the block of 2x2x2 voxels), but when a block is merged, we only store 1 voxel. In most fluid simulations, a large percentage of the original voxels are either empty or share similar density values.  
当区块未合并时，我们存储 1（较大的体素）+ 8 个体素（2x2x2 体素的区块），但当区块合并时，我们只存储 1 个体素。在大多数流体模拟中，很大一部分原始体素要么是空的，要么具有相似的密度值。  
This for example is often the case with clouds. The core of a cloud is generally quite homogeneous. Density varies essentially at the edge of the cloud.  
例如，云就经常是这种情况。云的核心部分通常非常均匀。在云的边缘，密度基本上是变化的。  
So encoding these volumes with a sparse representation can be a great way of reducing the size of the cache on disk and in memory.  
因此，用稀疏表示法对这些卷进行编码是减少磁盘和内存中缓存大小的好方法。

![[03a02920d366fd374b88243b17910ec3_MD5.png]]

Note that using blocks of 2x2x2 voxels is not mandatory. Many systems including OpenVDB, choose an octree structure for the first few upper levels of the hierarchy but then store the following levels into blocks of 32x32x32 voxels (for example).  
请注意，使用 2x2x2 象素块并不是强制性的。包括 OpenVDB 在内的许多系统都会为层次结构的前几层选择八叉树结构，但会将后面的层次存储到 32x32x32 象素块中（例如）。  
Organizing data that way might improve cache coherency and data access (compared to using smaller block sizes).  
与使用较小的块大小相比，以这种方式组织数据可能会改善缓存一致性和数据访问。

Finally, sparse volumes are also useful in rendering. Large voxels that are empty can be skipped. Large voxels with uniform density can be rendered as homogenous volumes. Added together, these allow for potentially large time savings.  
最后，稀疏体块在渲染中也很有用。空的大体块可以跳过。密度均匀的大体素可以渲染为均匀体积。这些功能加在一起可以节省大量时间。

The topic of sparse volumes (that share similarity with LODs and brick maps) is important and large enough to deserve a lesson of its own. In the meantime, if you are interested in the topic, you can also search for **[Gigavoxels](http://gigavoxels.inrialpes.fr/)** (an octree of bricks).  
稀疏体量（与 LOD 和砖块贴图有相似之处）是一个重要且庞大的主题，值得单独开设一课。同时，如果您对该主题感兴趣，也可以搜索 Gigavoxels（砖块的八叉树）。

### Out-of-core rendering 核外渲染

There's a reason why we mention out-of-core rendering right after sparse volume.  
我们之所以在稀疏卷之后才提到核外渲染，是有原因的。  
Sparse volumes and octrees of bricks have often been designed (besides the optimizations we mentioned above) to make it possible to render very large volume caches, that wouldn't normally be renderable as a grid, due to memory limitations.  
稀疏卷和八度砖块的设计（除了我们上面提到的优化）通常是为了能够渲染非常大的卷缓存，而由于内存限制，这些卷缓存通常无法渲染成网格。

When you can't load the entire file in memory (you don't want to be limited in how large your simulation might be), what you do is only load from the cache (using a kind of buffer mechanism), the bricks that you need (those for example that are visible to the camera).  
当您无法在内存中加载整个文件时（您不想限制模拟的大小），您所要做的就是从缓存中（使用一种缓冲机制）加载您所需要的砖块（例如摄像机可见的砖块）。  
This is of course only possible if your volume cache is organized as a collection of bricks that can be loaded on the fly (on demand). This topic will be covered in full in a future lesson.  
当然，这只有在您的卷缓存被组织成可即时（按需）加载的砖块集合时才有可能实现。我们将在以后的课程中全面介绍这一主题。

### Mind how you iterate through the grid data  
注意如何遍历网格数据

If you need to iterate through the voxels, it's best to do it that way:  
如果需要遍历体素，最好用这种方法：

```
for (size_t x = 0; x < resolution; ++x) {
    for (size_t y = 0; y < resolution; ++y) {
        for (size_t z = 0; z < resolution; ++z) {
            ...
        }
    }
}
```

Rather than that way: 而不是那样：

```
for (size_t z = 0; z < resolution; ++z) {
    for (size_t y = 0; y < resolution; ++y) {
        for (size_t x = 0; x < resolution; ++x) {
            ...
        }
    }
}
```

This is because of the way data is laid out in memory. By iterating over x first and z last, we are iterating through the values stored in memory sequentially. By iterating over z first and x last, we would be jumping to different parts of the memory.  
这是因为数据在内存中的排列方式。首先迭代 x，最后迭代 z，我们是按顺序迭代内存中存储的值。如果先迭代 z，后迭代 x，我们就会跳转到内存的不同部分。  
This is likely to generate a lot of cache misses which decreases performance. Of course, this depends on the format. We stored the data in the cache by iterating through x first, then y, then z but other formats might be using a different convention.  
这可能会产生大量的缓存缺失，从而降低性能。当然，这取决于格式。我们在缓存中存储数据时，先迭代 x，再迭代 y，最后迭代 z，但其他格式可能使用不同的约定。  
So be mindful of this particular aspect of your code, as this might be a place to look for optimizations.  
因此，请注意代码的这一特定方面，因为这可能是需要优化的地方。

### Baking the lighting 烘烤照明

You can bake the lighting in the grid's voxels and read this data from the voxels at render time to skip the step of evaluating the Li term at each raymarching step on the camera ray. This will speed up your rendering.  
你可以在网格的体素中烘焙光照，并在渲染时从体素中读取这些数据，从而跳过在摄像机光线的每个光线行进步骤中评估 Li 项的步骤。这将加快渲染速度。  
Of course, you will still need to render this Li term for each voxel in the grid in pre-pass (the baking pass) which is still going to take a while. Furthermore, you will need to start baking again whenever the lighting or the simulation changes.  
当然，您仍然需要在预处理（烘焙处理）中为网格中的每个体素渲染这个 Li 项，这仍然需要一段时间。此外，每当光照或模拟发生变化时，您都需要重新开始渲染。  
We are mostly mentioning this technique for reference and historical reasons.  
我们主要是出于参考和历史原因而提及这种技术。

### Deep shadow maps 深阴影贴图

Shadow maps are about to become an artifact of the past, though again for the sake of completeness and historical reasons, we thought it would be great to mention deep shadow maps, which in a way have similarities with the idea of baking the lighting into the grid's voxels.  
虽然阴影贴图即将成为过去式，但出于完整性和历史原因的考虑，我们认为还是应该提及深层阴影贴图，它在某种程度上与将光照烘焙到网格体素中的想法有相似之处。  
The idea here is to compute a shadow map per light in the scene.  
这样做的目的是为场景中的每一束光计算阴影贴图。  
But rather than storing the depth from the light to the nearest object in the scene for any given pixel (which is what shadow maps are used for), deep shadow maps store the density of the volume instead as a function of distance.  
但是，深阴影贴图存储的不是任何给定像素从光线到场景中最近物体的深度（这正是阴影贴图的用途），而是体积密度与距离的函数关系。  
In other words, each pixel in the deep shadow maps stores a curve that represents how transmission through the volume changes as we move through it. This technique was developed in 2000 by Tom Lokovic and Eric Veach from Pixar (you can find their [paper here](https://graphics.pixar.com/library/DeepShadows/paper.pdf) if you want to learn more about it). Several variations of that technique exist.  
换句话说，深层阴影贴图中的每个像素都存储了一条曲线，这条曲线代表了当我们穿过该区域时，通过该区域的传输是如何变化的。这项技术是由皮克斯的汤姆-洛科维奇和埃里克-维奇于 2000 年开发的（如果你想了解更多相关信息，可以在这里找到他们的论文）。该技术有多种变体。

## Source Code 源代码

As usual, you can find the source code for this chapter in the source code section of this lesson.  
与往常一样，您可以在本课的源代码部分找到本章的源代码。  
We also provide a sequence of about 100 cache files (download the cachefiles. zip file and unzip it on your local drive alongside the program source code) which you can use to render the animation shown at the beginning of this chapter.  
我们还提供了一连串约 100 个缓存文件（下载 cachefiles. zip 文件并解压到本地驱动器上，与程序源代码放在一起），您可以用它们来渲染本章开头所示的动画。