
### 相机射线：

很明显，既然要进行光线追踪，自然就要先有射线。所幸我们使用的是 Unity 引擎自带的摄像机已经可以正常工作了，就不需要对摄像机部分进行续写。通过一个变换矩阵得到相机的射线。

```c
using System;
using UnityEngine;

public class RayTracing : MonoBehaviour
{
    public ComputeShader RayTracingShader;
    private RenderTexture _target;
    private Camera _camera;

    private void Awake()
    {
        _camera = GetComponent<Camera>();//获取相机组件
    }

    private void SetShaderParameters()
    {
        //通过变换到世界空间，得到射线
        RayTracingShader.SetMatrix("_CameraToWorld",_camera.cameraToWorldMatrix);
        RayTracingShader.SetMatrix("_CameraInverseProjection",_camera.projectionMatrix);

    }

    private void OnRenderImage(RenderTexture Source, RenderTexture destination)
    {
        Render(destination);//把纹理画在屏幕上
    }

    private void Render(RenderTexture destination)
    {
        InitRenderTexture();//创建一个渲染纹理
        RayTracingShader.SetTexture(0,"Result",_target);
        int threadGrouphsX = Mathf.CeilToInt(Screen.width / 8.0f);
        int threadGrouphsY = Mathf.CeilToInt(Screen.height / 8.0f);
        RayTracingShader.Dispatch(0, threadGrouphsX, threadGrouphsY, 1);//和ComputerShader进行关联

        Graphics.Blit(_target,destination);//进行屏幕绘制

    }

    private void InitRenderTexture()
    {
        if (_target == null || _target.width != Screen.width || _target.height != Screen.height)
        {
            if (_target != null)
                _target.Release();

            _target = new RenderTexture(Screen.width, Screen.height, 0, RenderTextureFormat.ARGBFloat, RenderTextureReadWrite.Linear);
            _target.enableRandomWrite = true;
            _target.Create();
        }//目的是创建一张和屏幕大小一样的纹理

    }
}
```

然后剩下的工作自然就是**在 ComputerShader 当中定义射线的行为**，比如对于每一个屏幕像素的中心位置，都要计算射线的原点位置和射线方向，并且将结果作为颜色输出。

### 射线行为：

既然要创建射线，基于上面一样的思路，是要做两个行为，**先是要得到射线，然后得到每一个像素的 uv，根据每一个像素去创建一条射线**，对于拿到射线的原点，通过把相机空间转换到世界空间这一步还好说，但是射线的方向部分，想拿到的话，就得对投影矩阵取反。

投影过程：

![[48e750f661341c28be327245a1b94d98_MD5.jpg]]

在投影之后，得到图像，包括 RenderTexutre。那么只要做反过程，就能从到 Texture 拿到其余空间下的 Texture（这里说纹理好了，**既然有了纹理的坐标，也自然可以基于纹理拿到其射线坐标**）。

```c
//定义射线的结构体
struct  Ray
{
    float3 origin;
    float3 direction;//原点和方向
    float3 energy;
};

//创建射线的函数
Ray CreateRay(float3 origin,float3 direction) {
    Ray ray;
    ray.origin=origin;
    ray.direction=direction;
    return ray;
}

//创建相机中的射线
Ray CreateCameraRay(float2 uv) {
    //得到射线的原点和方向
    float3 origin=mul(_CameraToWorld,float4(0.0f,0.0f,0.0f,1.0f)).xyz;
    //把射线从透视到相机
    float3 projectivePoint = mul(_CameraInverseProjection, float4(uv, 0.0f, 1.0f)).xyz;
    //得到确切的方向
    float3 raydirection = projectivePoint - float3(0, 0, 0).xyz;
    raydirection = mul(_CameraToWorld, float4(raydirection, 0.0f)).xyz;
    raydirection = normalize(raydirection);

    return CreateRay(origin,raydirection);//得到实际的射线
}
```

这里或许大家也发现了, 是在 P 矩阵做逆过程到相机的空间，再转到世界空间才算正确拿到了相机的射线。

然后要做的就是**根据每一个像素去产生每一条像素自己的射线**。这里的产生射线的方法是为**每一个像素去分配一条线程**，值得说的是，这里并不是一个 uv，只是为了说明，以及方便性，毕竟真的很像在用 uv 采样，但实际上还是有区别的。所以还是先说一下 ID 的工作模式：

```
id :SV_DispatchThreadID
```

指明了此时的 id 代表的是线程的 id，而我们就要线程如何去计算像素，假如我们分配的 id 是（1，1），然后除以屏幕宽长，那么就会只会渲染最左上角的像素。所以我们要让表示每一个像素是干什么的，就得用整个 xy 去除以屏幕宽长，为每一个像素分配一个线程

```
void CSMain (uint3 id : SV_DispatchThreadID)
{
  uint width,height;//RenderTexture的确切大小
  Result.GetDimensions(width,height);
//拿到像素UV
  float2 uv = float2((id.xy ) / float2(width, height) );//id是分配的线程id

 Ray ray=CreateCameraRay(uv);//从每一个像素uv去创建一条射线

    Result[id.xy] = float4(ray.direction * 0.5f + 0.5f, 1.0f);

}
```

于是便能得到这样一幅图像：

![[ac0bdbb0b3a0e46f31c26b489decd87f_MD5.jpg]]

如果你并没有得到这样的一张图像的话，请去检查 OnRenderImage 函数是否调用了 SetShaderParameters 函数，如果没有调用的话，那么通过外部给与的天空盒的纹理信息自然是传递不到 ComputerShader 的，也自然会报错。笔者没有记错的话，报错信息是 KernelIndex(0) 等等

然后把每一个像素分配的线程给限制到 - 1 到 1 的区间，方便产生射线进行后续的采样过程（理解成 uv 做映射就好了）：

```
float2 uv=float2(id.xy+float2(0.5f,0.5f))/float2(width,height)*0.5f-1.0f)
```

![[75edae783b1b71cc21f35ffee67e76c6_MD5.jpg]]

然后我们完成了一个基本的过程：有了射线。但是依然差的很远，对于整个画面开说很单调，其主要原因在于缺少了对 SKYBOX 的采样，以及球体，但是这部分后面讨论，先说对 skybox 的采样。

### skybox 采样部分：

既然要放置采样贴图，就要在 c# 脚本当中加中这一部分：

```c
public Texture SkyboxTexture;
```

同时要把 skybox 传入到 computershader 当中：

```c
RayTracingShader.SetTexture(0,"_SkyboxTexture",SkyboxTexture);
```

然后在 ComputerShader 中对 Skybox 采样：

```c
Texture2D<float4>_SkyboxTexture;
SamplerState sampler_SkyboxTexture;
static const float PI=3.14159265f;
```

其中的 PI，是为了对 skybox 进行采样的时候，把笛卡尔方向向量转换到球坐标上，进而进行映射纹理坐标，故而引入。然后就是正式的采样部分：

```c
float theta = acos(ray.direction.y) / -PI;
    float phi = atan2(ray.direction.x, -ray.direction.z) / -PI * 0.5f;
    Result[id.xy] = _SkyboxTexture.SampleLevel(sampler_SkyboxTexture, float2(phi, theta), 0);
```

然后我们成功地完成了对天空盒的采样：

![[275ddce592b03edcd8c73e6eb080c1b4_MD5.jpg]]

先是转换坐标，然后进行纹理映射。当完成了对 skybox 的采样的时候，就应该来完成我们的重点部分：Tarcing。

### 光线追踪部分：

做了一堆的前期准备，终于能在这里完成追踪部分了，我们已经得到了射线，也完成了对 Skybox 的采样。接下来要完成光线击中物体的行为了。这部分内容算正式的大头内容，因为如何算击中球体，击中后如何描述着色等等。我们一一说明。

### 击中行为：

这里的写法参考上面创建光线的写法，其实本质没有太大区别

```c
//光线击中行为
struct RayHit
{
    float3 position;
    float distance;
    float3 normal;
};
//创建射线击中的函数
RayHit CreateRayHit() {
    RayHit hit;
    hit.position = float3(0.0f, 0.0f, 0.0f);
    hit.distance = 1.#INF;
    hit.normal = float3(0.0f, 0.0f, 0.0f);
    return hit;
}
```

其中的 `#INF`，代表的是无限大. 因为描述击中是针对于对象物体所在的地平面来说的，而地平面是无限远的。

然后就是击中地平面的行为，就需要写一个击中地平面的函数，因此我们需要明白这个击中行为的本质是在做什么，以及会返回什么信息。

射线的本质就是一个线性函数：Y=a+bx。只不过这里的 b 变成了 direction。然后返回的信息自然就是物体表面的法线，这样才能在后续过程计算着色。因此代码就应该这样写：

```c
//击中地平面
void IntersectGroundPlane(Ray ray, inout RayHit bestHit) {
    //计算地面和射线相交的距离
    float t = ray.origin.y / ray.direction.y;
    if (t < 0 && t < bestHit.distance)
    {
        bestHit.distance = t;
        bestHit.position = ray.origin + t * ray.direction;
        bestHit.normal = float3(0.0f, 1.0f, 0.0f);
    }
}
```

### 追踪过程（Tracing）：

那经过前文的撰写，这部分也更清楚，自然就是得到射线，击中平面，然后返回法线信息到着色函数：

```
//追踪过程
RayHit Trace(Ray ray) {
    RayHit bestHit = CreateRayHit();
    IntersectGroundPlane(ray, bestHit);
    return bestHit;
}
```

### 着色函数（shade）：

我们现在自然还没有任何物体，只能和地平面做相交测试，那么要返回的信息也就简单了，把物体返回的法线作为颜色输出就行，没有击中的部分，就采样天空盒就行。

```
//描述击中后的着色
float3 Shade(inout Ray ray, RayHit hit) {
    if (hit.distance < 1.#INF)
    {
        //返回法线
        return hit.normal * 0.5f + 0.5f;
    }
    else
    {
        //采样天空盒
        float theta = acos(ray.direction.y) / -PI;
        float phi = atan2(ray.direction.x, -ray.direction.z) / -PI * 0.5f;
        return _SkyboxTexture.SampleLevel(sampler_SkyboxTexture, float2(phi, theta), 0).xyz;
    }
}
```

然后自然就对主函数做一个小小的处理，进行光线追踪和着色。

```
//进行光线追踪和着色
    RayHit hit = Trace(ray);
    float3 result = Shade(ray, hit);
    Result[id.xy] = float4(result, 1);
```

然后你便能得到这样一副图片：

![[140570f43aa4e80cf7438737346e227c_MD5.jpg]]

恭喜你，完成了一次相交测试。那么接下来的部分就是添加物体了，进行实际的光线追踪，为了教程的简单，笔者决定还是采用和外网笔者一样的球体进行讲述。

### 球体部分：

自然是和球体相交，那么前面和地平面相交就不需要使用了。接下来只需要用射线和球体完成一次相交就行。怎么完成直线和球体之间的相交呢？这部分的数学，其实高中就有学过

球体的方程用向量可以表示为：

![[6118d44b007e8b1374f350795d0db702_MD5.jpg]]

而直线的方程可以表示为：

![[6d83357dfa49af92ea2a23e6b8a0c687_MD5.jpg]]

那么两者做计算，实则就是在求解下面的方程：

![[ced404b7e9e60dafef473c31b6852414_MD5.png]]

知道了，如何用数学来表达直线和球体相交，那么代码的写法呢：

```c
// 计算射线和球体之间的距离
    float3 d = ray.origin - sphere.xyz;
    float p1 = -dot(ray.direction, d);
    float p2sqr = p1 * p1 - dot(d, d) + sphere.w * sphere.w;
    if (p2sqr < 0)
        return;
    float p2 = sqrt(p2sqr);
    float t = p1 - p2 > 0 ? p1 - p2 : p1 + p2;
    if (t > 0 && t < bestHit.distance)
    {
        bestHit.distance = t;
        bestHit.position = ray.origin + t * ray.direction;
        bestHit.normal = normalize(bestHit.position - sphere.xyz);
    }
```

其实和上面的地平面相交一样的框架，只不过其中的求解方程变成了直线和球体之间的计算方程。再然后既然没有了地平面相交函数，就需要在 Trace 函数里面修改为和球体相交的函数：

```
//追踪过程
RayHit Trace(Ray ray) {
    RayHit bestHit = CreateRayHit();
    IntersectSphere(ray, bestHit, float4(0, 3.0f, 0, 1.0f));
    return bestHit;
}
```

于是你会得到这样一张和球体相交的图：

![[b8e109ab19d0bbdeb9d26da207eda292_MD5.jpg]]


在写着色函数之间，你或许已经留意到了锯齿。所以在写着色函数之间，让我们先完成抗锯齿。

### 抗锯齿：

上面已经说过了，做一个低通滤波就能起到一个抗锯齿的作用，相当于加一层后处理。工作原理就是平均化。

```c
Shader "kerios/KerShader"
{
    Properties
    {
        _MainTex ("Texture", 2D) = "white" {}
    }
    SubShader
    {

        Cull Off ZWrite Off ZTest Always
        Blend SrcAlpha OneMinusSrcAlpha

        Pass
        {
            CGPROGRAM
 #pragma vertex vert
 #pragma fragment frag
 #include "UnityCG.cginc"
            sampler2D _MainTex;
            float _Sample;

            struct vertexinput
            {
                float4 vertex : POSITION;
                float2 uv : TEXCOORD0;
            };

            struct vertexoutput
            {
                float2 uv : TEXCOORD0;
                float4 vertex : SV_POSITION;
            };

            vertexoutput vert (vertexinput v)
            {
                vertexoutput o;
                o.vertex = UnityObjectToClipPos(v.vertex);
                o.uv = v.uv;
                return o;
            }

            float4 frag (vertexoutput i) : SV_Target
            {
                return float4(tex2D(_MainTex, i.uv).rgb, 1.0f / (_Sample + 1.0f));
            }
            ENDCG
        }
    }
}
```

然后只需要像做后处理一样，给加上就行。然后在 C# 脚本当中写上这一部分的内容：

```
//样本采样和抗锯齿材质
    private uint _currentSample = 0;
    private Material _kerMaterial;
```

然后自然就是要检查一下，在相机运行过程中，位置是否发生了变换，并且要规定一下采样，去不断地进行抗锯齿。所以要写一个 Update 函数：

```c
private void Update() {
        if (transform.hasChanged)
        {
            _currenctSample = 0;
            transform.hasChanged = false;
        }
    }
```

那么接下来自然就是要到 Render 函数中去使用该材质了，进行渲染抗锯齿：

```c
//抗锯齿
        if (_kerMaterial == null)
            _kerMaterial = new Material(Shader.Find("Kerios/KerShader"));
            _kerMaterial.SetFloat("_sample", _currentSample);

        Graphics.Blit(_target,destination,_kerMaterial);//进行屏幕绘制
        _currentSample++;
```

然后一切顺利地话，你会得到下面这一张图，而这一张图明显少了许多锯齿：

![[1fb8d322dbc9f603b1e5b03e3a87c992_MD5.jpg]]

但是依然不够。让我们想一想还能这么抗锯齿，从原理出发，射线地产生实际是用的每一片像素的中心位置，如果我们在每一个像素内做随机移动产生射线，是不是就能减少一些锯齿了呢？答案肯定是可以的。那么很显然在 ComputerShader 的主函数当中做 uv 的映射的时候（实际不是 uv，请不要理解错误了），做随机就好了。具体怎么操作呢？在 ComputerShader 内部定义一个像素偏移的变量，然后由脚本在外部传入。

```
//随机产生射线的像素偏移
float2 _PixelOffset;
```

那么现在的主函数变为了：

```
//把uv换算到-1到1的区间
    float2 uv = float2((id.xy + _PixelOffset) / float2(width, height) * 2.0f - 1.0f);
```

然后就是外部的脚本部分，就要在 SetShaderParameters 函数当中传入随机值了：

```
RayTracingShader.SetVector("_PixelOffset", new Vector2(UnityEngine.Random.value, UnityEngine.Random.value));
```

于是可以发现我们的球体更加圆润了：

![[fd578bdebc5ce9791f6347af3a77f6eb_MD5.jpg]]

当然肉眼观察肯定，还是难以观察的。那么接下来就是重头戏了，球体的着色部分。

### 光线衰减计算：

要计算球体表面的着色，我们自然是希望足够地物理。那么就要对射线添加一个能量值，去尽量模拟现实的情况。同时在主函数去计算光线的弹射之间的衰减。

```
//定义射线的结构体
struct Ray
{
    float3 origin;
    float3 direction;
    float3 energy;
};
```

然后自然就是在 CreateRay 函数当中，对光的能量做一次初始化：

```
ray.energy = float3(1.0f, 1.0f, 1.0f);
```

剩下的部分就是在 ComputerShade 要编译的函数中进行光照的衰减计算：

```
//计算光线弹射的衰减造成的阴影
    float3 result=float3(0,0,0);
    for(int i=0;i<0;i++)
    {
        RayHit hit=Trace(ray);
        result=ray.energy*Shade(ray,hit)+result;
        if(!any(ray.energy))
            break;//光线衰减为0自然就得退出了

    }
```

### 球体着色函数：

现在的 Shade 函数自然就没那么简单了，还得更新能量和生成弹射之后产生的反射光线，为了进行这个镜面反射的部分，就得把射线光的每一个值进行计算，例如黄金的反射率就是 float3(1.0f,0.78f,0.34f)。这里的理解就从 RGB 的角度去理解就行，反射百分之一百的红光，反射百分之 78 的绿光，反射百分之 34 的蓝光。所以现在的 Shade 函数就为：

```c
//描述击中后的着色
float3 Shade(inout Ray ray, RayHit hit) {
    if (hit.distance < 1.#INF)
    {
        float3 specular = float3(0.6f, 0.6f, 0.6f);//反射率
        ray.origin = hit.position + hit.normal * 0.001f;
        ray.direction = reflect(ray.direction, hit.normal);
        ray.energy *= specular;
        //不返回内容
        return float3(0.0f, 0.0f, 0.0f);
    }
    else
    {
        // 消除光线的能量，因为skybox不会反射
        ray.energy = 0.0f;
        // 采样天空盒
        float theta = acos(ray.direction.y) / -PI;
        float phi = atan2(ray.direction.x, -ray.direction.z) / -PI * 0.5f;
        return _SkyboxTexture.SampleLevel(sampler_SkyboxTexture, float2(phi, theta), 0).xyz;
    }
}
```

然后顺利的话，你会得到一个成功拥有镜面反射的球体：

![[c0376ec4fab822d39e8b5a7015a4c164_MD5.jpg]]

可以发现镜面反射有点弱了，可以通过修改反射率来获得更好的反射效果。同时可以加上上面的地表面相交和着色，让场景更漂亮，同时想要增加球体的话，之间在 Trace 函数中进行修改就行。

![[94c2a6a7ecb3384bf6197f87624531e5_MD5.jpg]]

### 指定光照：

光照自然也是在 c# 脚本中传入 ComputerShader 当中。添加 c# 代码：

```c
//拿取光照
    public Light DirectionalLight;
private void SetShaderParameters()
    {

        //通过变换到世界空间，得到射线
        RayTracingShader.SetMatrix("_CameraToWorld",_camera.cameraToWorldMatrix);
        RayTracingShader.SetMatrix("_CameraInverseProjection",_camera.projectionMatrix);
        RayTracingShader.SetTexture(0,"_SkyboxTexture",SkyboxTexture);
        RayTracingShader.SetVector("_PixelOffset", new Vector2(UnityEngine.Random.value, UnityEngine.Random.value));
        //分配光照到ComputerShader当中
        Vector3 l = DirectionalLight.transform.forward;
        RayTracingShader.SetVector("_DirectionalLight", new Vector4(l.x, l.y, l.z, DirectionalLight.intensity));


    }
```

然后让我们想一想，拿到这个光照是为了干啥。没错，计算漫反射，基于兰伯特的余弦定理。 $lambert=||a||*||b||*cos θ$ 现在做一些说法修正。上面的反射率实际上是镜面反射率，但是描述当中含糊不清这里强调一下的原因在于。我们要写物体自身的漫反射颜色，而这个颜色就需要在 ComputerShader 的 shade 函数中进行描述，并且在返回的时候进行兰伯特的计算得到这个正确的漫反射效果：

```c
//光照
float4 _DirectionalLight;
float3 albedo=float3(0.8f,0.8f,0.8f);//shade函数中定义
return saturate(dot(hit.normal, _DirectionalLight.xyz) * -1) * _DirectionalLight.w * albedo;//计算兰伯特
```

于是便能得到这样的一个球体：

![[0e272e6d68a27be458a5fda8fd5fd444_MD5.jpg]]

同时要做的事情是追踪阴影光线，如果阴影光线的投射过程中遇到了阻碍，自然是不能由漫反射颜色的。所以要加入阴影测试的部分：

```
//阴影追踪：
        bool shadow=false;
         Ray shadowRay=CreateRay(hit.position+hit.normal*0.001f,-1*_DirectionalLight.xyz);
         RayHit shadowHit=Trace(shadowRay);
         if(shadowHit.distance!=1.#INF)
         {
           return float3(0.0f,0.0f,0.0f);
         }
```

那么到这里为止，我们已经完成了光线追踪的过程。但是我们依然缺少很多东西例如菲涅尔，也例如更多的球体以及可控化的场景，接下来让我们完善这一步

### 场景修改：

要想做到场景修改，就要在 unityscripts 中也建立对于球体的认识，让外部就能对球体的数据进行操控：

```c
struct Sphere
    {
        public Vector3 position;
        public float radius;
        public Vector3 albedo;
        public Vector3 specular;
        public Vector3 refraction;
    }//球体的属性
```

然后在 ComputerShader 中对球体的数据进行定义：

```c
struct Sphere
{
    float3 position;
    float radius;
    float3 albedo;
    float3 specular;
    float3 refraction;
};
```

那么对应的在计算击中后的返回数据也得增加为球体的数据，进行控制：

```c
struct RayHit
{
    float3 position;
    float distance;
    float3 normal;
    float3 albedo;
    float3 specular;
    float3 refraction;
};
```

接下来要添加渲染球体的 buffer。cpu 中的球体数据都会存储在这里：

```c
//球体的buffer
StructuredBuffer<Sphere> _Spheres;
```

然后自然就是要拿 buffer 到 ComputerShader 中，这里依然在 SetShaderParmeters 函数中进行：

```c
//球体的buffer
StructuredBuffer<Sphere> _Spheres;
```

然后为了控制自然要传入参数进去：

```c
//球体的属性
    [Header("球体")]
    public Vector2 SphereRadius = new Vector2(3.0f, 8.0f);
    public uint SpheresMax = 100;
    public float SpherePlacementRadius = 100.0f;
```

当球体的数据都解决了时候，需要考虑的物体就是对场景的设置，就需要一个设置场景的函数，负责把球体规定在一定范围，不和其余球体发生相交，同时给与一些随机属性，例如漫反射的反射率和镜面的反射率。这样思考的话，需要的是两个三个函数，分别是 OnEnable 函数，负责设置场景，另一个则是 OnDisable 函数，自然要检测当前的球体 buffer 有没有东西，如果有的话，那也自然要把 buffer 释放掉，然后就是核心的 setupScene 函数用于设置场景球体的属性：

```c
//设置场景
    private void OnEnable()
    {
        _currentSample = 0;
        SetUpScene();
    }



    //检查Buffer
    private void OnDisable()
    {
        if (_sphereBuffer != null)
            _sphereBuffer.Release();
    }
```

以及核心的场景设置函数：

```c
//场景设置
    private void SetUpScene()
    {
        List<Sphere> spheres = new List<Sphere>();
        //随机球体的数量
        for (int i = 0; i < SpheresMax; i++)
        {
            Sphere sphere = new Sphere();
            //最大半径和最小半径
            sphere.radius = SphereRadius.x + Random.value * (SphereRadius.y - SphereRadius.x);
            Vector2 randomPos = Random.insideUnitCircle * SpherePlacementRadius;
            sphere.position = new Vector3(randomPos.x, sphere.radius, randomPos.y);
            //防止球体之间相交
            foreach (Sphere other in spheres)
            {
                float minDist = sphere.radius + other.radius;
                if (Vector3.SqrMagnitude(sphere.position - other.position) < minDist * minDist)
                    goto SkipSphere;
            }
            //设置漫反射颜色和金属度
            Color color = Random.ColorHSV();
            bool metal = Random.value < 0.5f;
            sphere.albedo = metal ? Vector3.zero : new Vector3(color.r, color.g, color.b);
            sphere.specular = metal ? new Vector3(color.r, color.g, color.b) : Vector3.one * 0.04f;
            //把球体数据加载在list当中
            spheres.Add(sphere);
            SkipSphere:
            continue;
        }
        //链接buffer
        _sphereBuffer = new ComputeBuffer(spheres.Count, 40);
        _sphereBuffer.SetData(spheres);
    }
```

那么现在在计算球体击中的时候，要用的数据就是球体当中的数据了：

```c
//击中球体
void IntersectSphere(Ray ray, inout RayHit bestHit, uint sphereIndex) {
    // 计算计算到球体的距离
    Sphere sphere = _Spheres[sphereIndex];
    float3 d = ray.origin - sphere.position;
    float p1 = -dot(ray.direction, d);
    float p2sqr = p1 * p1 - dot(d, d) + sphere.radius * sphere.radius;
    if (p2sqr < 0)
        return;
    float p2 = sqrt(p2sqr);
    float t = p1 - p2 > 0 ? p1 - p2 : p1 + p2;
    if (t > 0 && t < bestHit.distance)
    {
        bestHit.distance = t;
        bestHit.position = ray.origin + t * ray.direction;
        bestHit.normal = normalize(bestHit.position - sphere.position);
        bestHit.albedo = sphere.albedo;
        bestHit.specular = sphere.specular;
        bestHit.refraction = sphere.refraction;
    }
}
```

然后现在说回 shade 函数。在解决了对于球体的操控之后，需要做的是计算菲涅尔效应等。

### 菲涅尔现象：

关于菲涅尔的计算可以用 Schlick 近似法，在说这个方法之前，先来瞻仰一下菲涅尔的计算公式：

![[305231153243db246c2706285d17b589_MD5.jpg]]

推导过程就不会在本文涉及了。很明显十分复杂，因此得用近似公式来计算，而 Schlick 近似的方法为：  $R=R_{0}+(1-R_{0})(1-cosθ_{i})^{5}$

其中 cos θ的部分其实就是入射光和法线的点乘，而 R0 的计算为：

 $R_{0}=(\frac{(n_{1}-n_{2})}{(n_{1}+n_{2})})^{2}$

而 n1 和 n2 自然就是不同介质的折射率。这段公式用代码表达为：

```
vec3 fresnel(float cosTheta, vec3 R0) {
    return R0 + (1.0 - R0) * pow(1.0 - cosTheta, 5.0);
}
```

稍微扩充一下：

```
//利用shlick做近似计算
void fresnel(const float3 I, const float3 N, const float3 ior, inout float kr) {
    float cosi = clamp(-1, 1, dot(I, N));
    float etai = 1, etat = ior;
    if (cosi > 0)
    {

        float temp = etai;
        etai = etat;
        etat = temp;
    }
    //折射定律
    float sint = etai / etat * sqrt(max(0.f, 1 - cosi * cosi));

    if (sint >= 1)
    {
        kr = 1;
    }
    else
    {
        float cost = sqrt(max(0.f, 1 - sint * sint));
        cosi = abs(cosi);
        float Rs = ((etat * cosi) - (etai * cost)) / ((etat * cosi) + (etai * cost));
        float Rp = ((etai * cosi) - (etat * cost)) / ((etai * cosi) + (etat * cost));
        kr = (Rs * Rs + Rp * Rp) / 2;
    }

}
```

当我们有了镜面反射和漫反射以及菲涅尔之后，我们依然差一个要素：折射计算，物体本身带有一定的透光属性如何计算。

### 折射计算：

![[34cf27c42f2be8da8ff1b2c00ec00c0e_MD5.jpg]]

根据中学时期学习过关于折射相关的公式为： $sinθ'/sin θ=n_{1}/n_{2}= η$ 根据上图，换为代码表述就为：

```
//计算折射
float3 GetRefraction(const float3 I, const float3 N, const float ior) {
    float cosi = clamp(-1, 1, dot(I, N));
    float etai = 1, etat = ior;
    float3 n = N;
    if (cosi < 0)
    {
        cosi = -cosi;
    }
    else
    {

        float temp = etai;
        etai = etat;
        etat = temp;

        n = -N;
    }
    float eta = etai / etat;
    float k = 1 - eta * eta * (1 - cosi * cosi);
    return k < 0 ? 0 : eta * I + (eta * cosi - sqrt(k)) * n;
}
```

### 重写 shade 函数：

那么现在就能重新来计算着色了。把菲涅尔和折射的计算

```c
float3 Shade(inout Ray ray, RayHit hit) {
    if (hit.distance < 1.#INF)
    {      
        if (any(hit.refraction))
        {//玻璃材质计算
            float kr;
            fresnel(ray.direction, hit.normal, 1.55, kr);
            bool fromOutside = dot(ray.direction, hit.normal) < 0;
            float3 bias = hit.normal * 0.001f;

            if (kr > 1)
            { //全反射现象
                ray.origin = hit.position + hit.normal * 0.001f;
                ray.direction = reflect(ray.direction, hit.normal);
                ray.energy *= hit.specular;
            }
            else
            {
                if ( _PixelOffset.x < kr)
                {
                    ray.origin = hit.position + hit.normal * 0.001f;
                    ray.direction = reflect(ray.direction, hit.normal);
                    ray.energy *= hit.specular;
                }
                else
                {

                    //计算折射
                    float3 dir = fromOutside ? normalize(refract(ray.direction, hit.normal, 1 / 1.55)) : normalize(refract(ray.direction, -hit.normal, 1.55));
                    ray.origin = fromOutside ? hit.position - bias : hit.position + bias;
                    ray.direction = dir;
                    ray.energy *= hit.refraction;
                }


            }
        }
        else
        {
            ray.origin = hit.position + hit.normal * 0.001f;
            ray.direction = reflect(ray.direction, hit.normal);
            ray.energy *= hit.specular;
        }


        //阴影追踪：
        bool shadow = false;
        Ray shadowRay = CreateRay(hit.position + hit.normal * 0.001f, -1 * _DirectionalLight.xyz);
        RayHit shadowHit = Trace(shadowRay);
        if (shadowHit.distance != 1.#INF)
        {
            return float3(0.0f, 0.0f, 0.0f);
        }
        else
        {
            //兰伯特计算
            return saturate(dot(hit.normal, _DirectionalLight.xyz) * -1) * _DirectionalLight.w * hit.albedo;
        }
    }
    else
    {
        // 消除光线的能量，因为skybox不会反射
        ray.energy = 0.0f;

        // 采样天空盒
        float theta = acos(ray.direction.y) / -PI;
        float phi = atan2(ray.direction.x, -ray.direction.z) / -PI * 0.5f;
        return _SkyboxTexture.SampleLevel(sampler_SkyboxTexture, float2(phi, theta), 0).xyz * 1.8f;
    }
}
```

### 收尾：

最终你将会得到这样的渲染图：

![[a225fdeaf3f4916cce31df8be34c5a93_MD5.jpg]]

![[bfba47f4d09f8c44d97792dd8c110af8_MD5.jpg]]

本文会根据读者反应情况决定是否迭代，进行深入讲解。个人认为细化到了这个程度应该是能让读者明白如何去写一个光线追踪的。那么最后，祝愿你收获自己的光追，我是在野，感谢阅读。