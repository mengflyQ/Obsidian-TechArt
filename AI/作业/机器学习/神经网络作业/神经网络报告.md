[pytorch利用卷积神经网络进行CIFAR-10图像分类_使用 pytorch 和 cnn 对 cifar-10 数据集进行图像分类-CSDN博客](https://blog.csdn.net/qq_28368377/article/details/106245489)


报告写法：[构建报告 - 报告写作 - LibGuides at University of Reading](https://libguides.reading.ac.uk/reports/structuring)
# 使用 ML 算法和方法进行分类
# 目录
# 1 Abstract

Keywords:
# 2 Introduction

机器学习是人工智能的一个重要分支，它的目标是开发和应用算法，使计算机可以从数据中学习并做出预测或决策。机器学习中的分类问题是一类常见的监督学习任务，其主要目标是根据给定的输入数据，将其划分到不同的类别中。在分类问题中，算法通过学习输入数据与其对应的标签之间的关系，从而能够对新的、未标记的数据进行分类。分类问题在实际应用中非常广泛，涉及到图像识别、自然语言处理、医学诊断、金融欺诈检测等多个领域。通过不断改进算法和优化模型参数，研究者们致力于提高分类模型的性能和适应性。

在众多的分类算法中，随机森林和 K 近邻（KNN）算法是两种广泛使用的方法。随机森林是一种集成学习方法，它通过构建并结合多个决策树来做出预测。KNN 算法则是一种基于实例的学习方法，它根据输入数据在特征空间中的邻近样本的类别来做出预测。 

本报告的目标是使用随机森林和 KNN 分类器对 Fashion MNIST 和 CIFAR-10 数据集进行分类并评估。我们将首先对数据进行预处理，然后使用随机森林和 KNN 分类器进行模型训练，最后使用和分类报告和混淆矩阵来评估模型的性能。

本文的结构如下︰第三章介绍了利用随机森林和 KNN 进行分类的相关工作。第四章是主体部分，将对数据集和分类器的选择进行解释。第五章首先解释了机器学习管道，然后展示了我们获得的结果，并对每种情况分别进行了讨论。最后，我们以结论和参考文献结束本文。附录为实验使用的代码。

# 3 Related work

本节总结了利用随机森林和 KNN 进行分类的相关工作。
Komarasamy. G 等人使用随机森林算法进行社交媒体情感分类，与其他方法相比，和谐梯度提升随机森林机器学习技术产生了更好的结果。

Wolffang 等人使用机器学习算法来描述和提取西红柿图像颜色统计的特征，对 KNN，MLP 等算法进行了全方位的评估。他发现用于番茄颜色分类的 K-NN 算法与 MLP 神经网络方法相结合，可以获得最佳性能。
Roger Singh Chugh 等人使用经典机器学习算法（（KNN）、（MLP）和随机森林分类器（RF）对数据集的准确率、时间复杂度、F1 分数、召回率和精确度等参数进行了对比分析。结果表明，MLP 的准确率最高。

Ying Li 和 Bo Cheng 借鉴裁剪 KNN 的思想，采用改进的 KNN 分类算法，并将其应用于高分辨率遥感影像的面向对象分类。发现改进的 KNN 算法在高分辨率遥感影像分类中可以达到更高的精度。
Osim Kumar Pal 基于 KNN 和随机森林算法进行设计。患者可以使用该模型将他的皮肤病归类为主要检测，医生也可以通过使用该模型来确保他的判断。
Anna Bosch 等人将随机森林/Ferns 分类器的性能与基准多向 SVM 分类器进行了比较，提供了一种抑制背景杂波和增加对象实例位置不变性的方法，提出的模型性能比现有技术提高了约 10%。

Krithika 和 Selvarani 在他们的工作中使用 K 最近邻分类来对葡萄叶病进行分类并识别叶骨架。使用葡萄图像识别叶子骨架。他们发现通过使用 KNN 分类可以有效地对葡萄叶部病害进行分类。


# 4 Main body
## 4.1 Machine learning pipeline
解释机器学习管线，（代码解释？）

机器学习中，一般将数据分为**训练数据**和**测试数据**两部分来进行学习和实验等。
- 首先，使用训练数据进行学习，寻找最优的参数。
- 然后，使用测试数据评价训练得到的模型的实际能力。

 求解机器学习问题的步骤可以分为“学习（也称为训练）” 和“推理”两个阶段。
> 
> 首先，在学习阶段进行模型的学习，然后，在推理阶段，用学到的模型对未知的数据进行推理（分类）。如前所述，**推理阶段一般会省略输出层的 softmax 函数。** 

机器学习（Machine Learning，ML）pipeline 是一系列有序的数据处理步骤，用于将原始数据转化为可用于训练和部署模型的格式。下面是一个典型的机器学习 pipeline 的概括：

1. **数据收集：**
    
    - **获取数据：** 收集用于训练和测试的数据。数据可以来自各种来源，包括数据库、文件、API 等。
    - **数据清理：** 处理缺失值、异常值和重复项。确保数据质量，以提高模型的性能。
2. **数据探索与分析：**
    
    - **数据可视化：** 使用图表和统计工具对数据进行探索，理解特征之间的关系和数据分布。
    - **统计分析：** 进行描述性统计分析，识别潜在的模式和趋势。
3. **特征工程：**
    
    - **特征选择：** 选择对模型预测有影响的特征，减少维度和计算成本。
    - **特征转换：** 对特征进行变换，使其更适合模型的学习。
4. **数据划分：**
    
    - **训练集、验证集和测试集：** 将数据划分为训练集、验证集和测试集，用于模型的训练、调优和评估。
5. **模型选择：**
    
    - **选择模型：** 根据问题的性质选择适当的机器学习模型，如决策树、支持向量机、神经网络等。
    - **模型配置：** 配置模型的超参数，以优化性能。
6. **模型训练：**
    
    - **训练模型：** 使用训练集对选择的模型进行训练，学习数据的模式和关系。
7. **模型评估：**
    
    - **验证集评估：** 使用验证集评估模型的性能，调整超参数以提高准确性。
    - **测试集评估：** 最终使用测试集评估模型的泛化性能。
8. **模型部署：**
    
    - **部署模型：** 将训练好的模型部署到生产环境中，以进行实时预测。
    - **集成：** 将模型集成到现有系统中，确保与其他组件协同工作。
9. **监控与维护：**
    
    - **模型监控：** 定期监控模型性能，检测潜在的漂移和性能下降。
    - **更新维护：** 根据需要更新模型，以适应新的数据分布和业务需求。
10. **反馈循环：**
    

- **反馈：** 利用模型在生产中的表现反馈到整个 pipeline 中，可能需要调整数据收集、特征工程、模型选择等步骤。

以上步骤不是严格线性的，而是一个迭代的过程。在实践中，可能需要多次调整和优化，以达到更好的模型性能和适应性
## 4.2 Dataset

### 4.2.2 CIFAR-10
CIFAR-10 是一个彩色图片数据集，它有 10 个类别: "airplane", "automobile", "bird", "cat","deer", "dog", "frog", "horse", "ship", "truck"。每张图片都是  3 * 32 * 32 ，也即 3 通道（RGB）彩色图片，分辨率为 32 * 32。

该数据集共有 60000 张彩色图像，这些图像是 32 * 32，分为 10 个类，每类 6000 张图。这里面有 50000 张用于训练，构成了 5 个训练批，每一批 10000 张图；另外 10000 用于测试，单独构成一批。测试批的数据里，取自 10 类中的每一类，每一类随机取 1000 张。抽剩下的就随机排列组成了训练批。注意一个训练批中的各类图像并不一定数量相同，总的来看训练批，每一类都有 5000 张图。

Here are the classes in the dataset, as well as 10 random images from each: [CIFAR-10 and CIFAR-100 datasets (toronto.edu)](https://www.cs.toronto.edu/~kriz/cifar.html)
![[Pasted image 20231125140355.png]]
## 4.3 Data loading and pre-processing

图像分类中的预处理是一个对图像进行转换以准备分类的过程。这些步骤包括导入必要的库，然后从文件夹中检索图像。然后调整图像大小并转换为 NumPy 数组。然后将数组分割为训练数据和测试数据。

## 4.4 Classifier Introduction

### 4.4.1 Random forest


Random Forest 是一种集成学习方法，它通过构建多个决策树并将它们整合在一起来进行分类或回归任务。随机森林是由 Leo Breiman 在 2001 年提出的（Breiman L, 2001a. Random forests. Mach. Learn., 45:5-32.），它在机器学习中被广泛应用，因为它具有良好的性能和鲁棒性。随机森林由多个决策树组成。每个决策树都是一个分类器，通过对输入数据进行逐层的判定，最终输出属于哪个类别。随机森林在构建每棵决策树时引入了随机性。对于分类任务，随机森林采用投票机制进行决策。每棵决策树对输入数据进行分类，最终的分类结果是获得最多投票的类别。随机森林通过组合多个决策树的预测结果，减小了过拟合的风险，提高了模型的鲁棒性和泛化能力。它对于处理高维数据和大规模数据集都表现出色。

随机森林中的树按以下规则生成：
1. 如果训练集大小为 N，对于每棵树而言，随机且有放回地从训练集中的抽取 N 个训练样本（这种采样方式称为 bootstrap sample 方法, 为拔靴法采样），作为该树的训练集；从这里我们可以知道：每棵树的训练集都是不同的，而且里面包含重复的训练样本。
2. 如果存在 M 个特征维度，则指定数量 m << M，使得在每个节点处，从 M 中随机选择 m 个特征维度，并且使用这些 m 个特征维度中最佳特征 (最大化 information gain)来分割节点。在森林生长期间，m 的值保持不变。
3. 每棵树都尽最大程度的生长，并且没有剪枝过程。

###  4.3.2 KNN

使用 KNN 算法应用图像分类。在模式确认中，KNN 算法是一种用于分类和回归的非参数方法。输出基于 KNN 是用于排序还是回归：在 KNN 分类中，最后一个输出是周期附属。一个对象是通过其邻居的极端投票来分类的，被分配到其 k 个最近邻居中最常见的类的对象，其中 k 是正整数，通常很小。如果 k＝1，则该对象仅被选择为唯一一个最近邻居的类。在 KNN 回归中，最后一个乘积是对象的属性值。该特定值是其 k 个最近邻居的正常值。KNN 是一种基于发生率的学习，或缓慢的实现，其中体积与近处相似，所有计算都被允许，直到顺序。KNN 计算是所有机器学习计算中最直接的计算之一。KNN 分类器的执行基本上由 K 的决定以及所连接的分离度量来决定。该度量受到区域度量 K 的确定的可影响性的影响，理由是附近区域的扫描是由与问题的第 K 个最近邻居的分离决定的，并且不同的 K 产生不同的限制性类概率。在 K 很小的情况下，由于信息匮乏的条件和喧闹、模糊或错误标记的焦点，附近的量规通常会非常差。最终目标是进一步平滑度量，我们可以构建 K，并围绕这个问题考虑一个实质性的区域设置。令人震惊的是，对 K 的广泛估计有效地使规范过度平滑，并且排列执行随着不同类别异常的出现而降低。为了解决这一问题，已经进行了相关的研究工作，以提高 KNN 的安排执行力。( https://ieeexplore.ieee.org/document/9058042 )

KNN（k 近邻算法）的步骤:
1.      计算测试数据与各个训练数据之间的距离
2.      按照升序（从小到大）对距离（欧氏距离）进行排序
3.      选取距离最小的前 k 个点
4.      确定前 k 个点所在类别出现的频率
5.      返回前 k 个点中出现频率最高的类别作为测试数据的分类

## 4.4 分类器选择的合理性
### 4.4.1 随机森林

(1) 高准确性： 随机森林是一种集成学习方法，通过组合多个决策树的预测结果，可以提高整体模型的准确性。每个决策树都是基学习器，通过投票或平均的方式来进行分类，减少了过拟合的风险。
(2) 对特征的自适应性： 随机森林对于特征的选择具有自适应性，它会在每个决策树的训练过程中随机选择一部分特征进行划分。这有助于减少特征之间的相关性，提高模型的泛化能力，避免过度依赖某些特征。
(3) 处理大规模数据： 随机森林通常对大规模数据表现良好，因为可以并行训练多个决策树。这有助于加速模型的训练过程，使得随机森林在大规模数据集上具有可扩展性。
(4) 能够处理高维数据： 随机森林在高维数据上的表现相对较好，不容易受到维度灾难的影响。通过随机选择特征，可以降低维度的复杂性，提高模型的鲁棒性。
(5) 抗过拟合能力：随机森林通过引入随机性，包括随机选择样本和随机选择特征，减缓了过拟合的风险。这使得模型更具泛化能力，对新数据的适应性较好。
(6) 提供特征重要性评估： 随机森林可以输出各个特征的重要性，帮助理解模型对问题的解释，有助于特征选择和模型解释。

总体而言，随机森林是一种强大的分类方法，适用于各种问题，并在许多实际场景中表现出色。
### 4.1.2 KNN 
选择使用 KNN（K-最近邻）方法的合理性可以从以下几个方面考虑：

1. 简单实现： KNN 是一种非常简单直观的分类算法。它不需要训练阶段，而是在预测时直接根据最近邻的样本进行分类。这种简单性使得 KNN 易于理解和实现。
2. 无参数： KNN 是一种无参数的算法，不需要对模型进行复杂的参数调整。这使得在处理一些复杂的问题时，不需要过多的领域知识，而且可以更加灵活地应用在不同类型的数据上。
3. 适用于小规模数据集： 如果数据集相对较小，KNN 可以是一个合适的选择。由于 KNN 算法在训练时不需要构建模型，而是在预测时计算距离，因此对于小规模数据集而言，计算成本相对较低。
4. 适用于局部性较强的问题： KNN 的核心思想是利用邻近样本的信息进行分类，因此适用于局部性较强的问题。如果数据在特征空间中有较好的聚集性，KNN 可能能够取得不错的效果。
5. 不受数据分布影响： KNN 对于数据的分布没有过多的假设，适用于各种类型的数据分布情况。这使得它在处理一些复杂的、非线性的问题时表现较为出色。

然而，需要注意的是，KNN 也有一些缺点，例如对于大规模数据集的计算成本较高，对噪声和异常值敏感等。在实际应用中，需要根据具体问题和数据的特点来综合考虑是否选择 KNN 算法。
# 5 Results and discussion

## 5.2 CIFAR-10

### 5.2.1 随机森林
#### 5.2.1.1 分类结果
分类结果信息表格：

| class | precision | recall | f1-score | support |
|-------|-----------|--------|----------|---------|
| 0     | 0.54      | 0.56   | 0.55     | 1000    |
| 1     | 0.52      | 0.53   | 0.52     | 1000    |
| 2     | 0.36      | 0.33   | 0.35     | 1000    |
| 3     | 0.33      | 0.27   | 0.30     | 1000    |
| 4     | 0.39      | 0.38   | 0.38     | 1000    |
| 5     | 0.40      | 0.39   | 0.39     | 1000    |
| 6     | 0.47      | 0.56   | 0.51     | 1000    |
| 7     | 0.51      | 0.45   | 0.48     | 1000    |
| 8     | 0.59      | 0.62   | 0.60     | 1000    |
| 9     | 0.47      | 0.55   | 0.51     | 1000    |
||||||
| accuracy    | -   | -   | 0.46     | 10000   |
| macro avg   | 0.46| 0.46| 0.46     | 10000   |
| weighted avg| 0.46| 0.46| 0.46     | 10000   |

混淆矩阵：
```
[[[8529  471]
  [ 438  562]]

 [[8513  487]
  [ 475  525]]

 [[8415  585]
  [ 666  334]]

 [[8460  540]
  [ 733  267]]

 [[8400  600]
  [ 622  378]]

 [[8421  579]
  [ 614  386]]

 [[8367  633]
  [ 435  565]]

 [[8567  433]
  [ 547  453]]

 [[8573  427]
  [ 382  618]]

 [[8391  609]
  [ 452  548]]]
```

#### 5.2.1.2 结果分析
(1) Accuracy 为 0.46，说明整体分类效果一般。
(2) Precision 精确度的平均值为 0.46，加权平均也为 0.46，说明模型对负样本的区分能力一般。
(3) Recall 的平均值为 0.46。说明模型对正样本的识别能力一般。
(4) F1-score 的加权平均为 0.46，说明模型不太稳健
(5) Macro average 和 Micro average 相等，说明每个类别对整体性能的贡献基本相同，模型具有较为均衡的分类能力。

混淆矩阵反映了模型在每个类别上的具体表现。例如，在类别 0 中，有 8529 个样本被正确分类为类别 0，但有 471 个样本被错误分类为其他类别。同样，在类别 0 中，有 438 个样本被错误分类为类别 1，有 562 个样本被正确分类为类别 0。

类别 2、3、4、5、6、7、9 的精确度较低，可能是因为这些类别的样本在数据集中分布较为分散或者类别之间的特征差异较小，导致模型难以准确分类。

类别 3、4、5、6、7 的召回率较低，说明模型在这些类别中漏检了很多正样本。可能是因为这些类别的样本特征与其他类别相似，使得模型难以正确识别。

总体而言，模型在 CIFAR-10 数据集上的分类效果较一般，可能需要进一步优化模型结构或调整参数以提高性能。
### 5.2.2 KNN

#### 5.2.2.1 分类结果
分类结果信息表格：

| class | precision | recall | f1-score | support |
|-------|-----------|--------|----------|---------|
| 0     | 0.38      | 0.54   | 0.45     | 1000    |
| 1     | 0.65      | 0.20   | 0.31     | 1000    |
| 2     | 0.23      | 0.45   | 0.30     | 1000    |
| 3     | 0.29      | 0.22   | 0.25     | 1000    |
| 4     | 0.24      | 0.51   | 0.33     | 1000    |
| 5     | 0.39      | 0.22   | 0.28     | 1000    |
| 6     | 0.35      | 0.25   | 0.29     | 1000    |
| 7     | 0.68      | 0.21   | 0.32     | 1000    |
| 8     | 0.40      | 0.66   | 0.50     | 1000    |
| 9     | 0.70      | 0.14   | 0.23     | 1000    |
||||||
| accuracy    | -   | -   | 0.34     | 10000   |
| macro avg   | 0.43| 0.34| 0.33     | 10000   |
| weighted avg| 0.43| 0.34| 0.33     | 10000   |

混淆矩阵：
```
[[[8135  865]
  [ 463  537]]

 [[8888  112]
  [ 795  205]]

 [[7446 1554]
  [ 548  452]]

 [[8479  521]
  [ 783  217]]

 [[7408 1592]
  [ 486  514]]

 [[8659  341]
  [ 780  220]]

 [[8532  468]
  [ 752  248]]

 [[8900  100]
  [ 790  210]]

 [[8012  988]
  [ 345  655]]

 [[8939   61]
  [ 860  140]]]
```

#### 5.2.2.2 结果分析
首先，让我们解释表格中的各项指标：
(1) Accuracy 为 0.34，说明整体分类效果一般。
(2) recision 的平均值为 0.43，加权平均也为 0.43，说明模型对负样本的区分能力一般。
(3) Recall 召回率的平均值为 0.34，说明模型对正样本的识别能力较弱。
(4) F1-score 的加权平均为 0.33，模型说明模型不够稳健
(5) Macro average 和 Micro average 相等，说明每个类别对整体性能的贡献基本相同，模型具有较为均衡的分类能力。

混淆矩阵反映了模型在每个类别上的具体表现。例如，在类别 0 中，有 8135 个样本被正确分类为类别 0，但有 865 个样本被错误分类为其他类别。同样，在类别 0 中，有 463 个样本被错误分类为类别 1，有 537 个样本被正确分类为类别 0。

类别 1、2、3、5、6、8、9 的精确度较低，可能是因为这些类别的样本在数据集中分布较为分散或者类别之间的特征差异较小，导致模型难以准确分类。

类别 1、5、8、9 的召回率较低，说明模型在这些类别中漏检了很多正样本。可能是因为这些类别的样本特征与其他类别相似，使得模型难以正确识别。

总体而言，模型在 CIFAR-10 数据集上的分类效果较一般，可能需要进一步优化模型结构或调整参数以提高性能。
# 6 Concclusion
结论：

(1) 随机森林和 KNN 在 Fashion-MNIST 和 CIFAR-10 数据集上的分类效果均较好，其中随机森林在 Fashion-MNIST 数据集上的表现更优。
(2) 随机森林和 KNN 在分类结果上的差异主要体现在精度、召回率和 F1-score 上，其中随机森林在精度和 F1-score 上表现更优，而 KNN 在召回率上表现更优。
(3) 随机森林和 KNN 在分类结果上的差异可能与数据集的特征分布、类别间的相似度以及模型的参数设置等因素有关。


# 7 References

[Fashion Images Classification using Machine Learning, Deep Learning and Transfer Learning Models | IEEE Conference Publication | IEEE Xplore](https://ieeexplore.ieee.org/document/9786364)

# 8 Appendices