[pytorch利用卷积神经网络进行CIFAR-10图像分类_使用 pytorch 和 cnn 对 cifar-10 数据集进行图像分类-CSDN博客](https://blog.csdn.net/qq_28368377/article/details/106245489)


报告写法：[构建报告 - 报告写作 - LibGuides at University of Reading](https://libguides.reading.ac.uk/reports/structuring)
# 使用 ML 算法和方法进行分类
# 目录
# 1 Abstract

Keywords:
# 2 Introduction

  
人工神经网络（Artificial Neural Networks, ANN）是受到人类大脑神经元网络结构启发而设计的一类机器学习模型。它们模拟了生物神经网络的基本工作原理，包括多个神经元（也称为节点或单元）之间的连接、信息传递和权重调整。人工神经网络在机器学习领域的应用广泛，它们的灵活性和强大的学习能力使其成为解决复杂任务和大规模数据集的理想选择。

本报告的目标是使用人工神经网络知识以及用于生成能够对图像进行分类的神经网络模型的方法，并描述循环神经网络的用例。

本文的结构如下︰

# 3 Related work
以下是人工神经网络相关的经典文献和相关工作

Y. Lecun 等人提出了 LeNet，这是一个最早发布的卷积神经网络之一，在计算机视觉任务中展现了高效性能，用于识别图像中的手写数字。
[Gradient-based learning applied to document recognition | IEEE Journals & Magazine | IEEE Xplore](https://ieeexplore.ieee.org/document/726791)

A. Krizhevsky 等人提出了 AlexNet，这是一个深度卷积神经网络，首次在 ImageNet 图像分类竞赛中获得显著性能提升。
[[PDF] ImageNet classification with deep convolutional neural networks | Semantic Scholar](https://www.semanticscholar.org/paper/ImageNet-classification-with-deep-convolutional-Krizhevsky-Sutskever/abd1c342495432171beb7ca8fd9551ef13cbd0ff)

Karen Simonyan 和 Andrew Zisserman 提出了 VGGNet，使用了非常小的卷积核和一个更深的卷积神经网络结构。在 mageNet 挑战赛中获得成功并可以推广到其他数据集。
[[1409.1556] Very Deep Convolutional Networks for Large-Scale Image Recognition (arxiv.org)](https://arxiv.org/abs/1409.1556)

Lin 等人于2013年提出的 NiN 深度神经网络结构。NiN 的设计旨在增加网络的表达能力，改进特征提取的方式，并减轻过拟合的问题。创造性的在每个像素的通道上分别使用多层感知机。
[[1703.03130] A Structured Self-attentive Sentence Embedding (arxiv.org)]( https://arxiv.org/abs/1703.03130 )

Szegedy 等人在 2014 年提出 GoogLeNet，这是一个著名的深度卷积神经网络架构，主要用于图像分类任务。其创新点之一是引入了 Inception 模块，使得网络更加深层、宽阔，同时仍然保持高效。
[Rethinking the Inception Architecture for Computer Vision | IEEE Conference Publication | IEEE Xplore --- 重新思考计算机视觉的 Inception 架构 | IEEE 会议出版物 | IEEE探索](https://ieeexplore.ieee.org/document/7780677)

He 等人 ResNet ，提出的一种深度卷积神经网络结构，旨在解决深度神经网络中的梯度消失和梯度爆炸问题。在2015年的 ImageNet 挑战赛获得冠军，引入了 residual blocks，对如何建立深层神经网络产生深远影响。
[[1703.03130] A Structured Self-attentive Sentence Embedding (arxiv.org)](https://arxiv.org/abs/1703.03130)

Zachary 等人在其综述中全面评述循环神经网络（RNN）在序列学习任务中的应用。详细介绍了 RNN 的基本原理、结构、梯度传播算法以及在实际任务中的表现。

[[1506.00019] 用于序列学习的递归神经网络的批判性综述 (arxiv. org)]( https://arxiv.org/abs/1506.00019 )

F.A. Gers 等人提出了 LSTM，解决了传统 RNN 中的长期依赖问题。使用了自适应的"forget gate"来防止网络崩溃。
[Learning to forget: continual prediction with LSTM | IET Conference Publication | IEEE Xplore](https://ieeexplore.ieee.org/document/818041)

Ilya Sutskever 等人提出了一种通用的端到端序列学习方法，展示了 LSTM 在序列到序列学习中的成功应用，在 WMT'14 数据集的英法翻译任务重达到了较高分数。
[[1409.3215] Sequence to Sequence Learning with Neural Networks (arxiv.org)](https://arxiv.org/abs/1409.3215)
# 4 Main body
## 4.1 优化人工神经网络的方法
人工神经网络的优化目标是通过调整网络参数，使其在特定任务上最小化损失函数，提高模型的泛化能力，最大化性能指标，加速收敛过程，降低计算和存储成本，提高鲁棒性，以及适应不同任务和领域，从而实现对复杂数据关系的学习和智能决策。通常要最大限度减少神经网络的损失函数，除了使用优化算法来减少训练误差之外，我们还需要注意过拟合。

### 随机梯度下降
随机梯度下降（SGD）是机器学习中常用的优化算法之一。它是一种迭代的优化算法，用于最小化损失函数，并更新模型参数以适应训练数据。与传统的梯度下降算法相比，随机梯度下降每次迭代只使用一个样本来计算梯度，因此具有更快的训练速度。
小批量随机梯度下降（Mini-Batch Stochastic Gradient Descent，Mini-Batch SGD）是在训练神经网络时使用的一种优化算法。与传统的梯度下降算法不同，小批量SGD每次更新模型参数时仅使用小批量（mini-batch）随机选择的训练样本的梯度信息。

### 动量法
动量法（Momentum）由 Polyak 提出，是一种用于优化算法的技术，特别是在神经网络的训练中常被使用。动量法用过去梯度的平均值来替换梯度，这大大加快了收敛速度。 对于无噪声梯度下降和嘈杂随机梯度下降，动量法都是可取的。动量法可以防止在随机梯度下降的优化过程停滞的问题。
Polyak, B. T. (1964). Some methods of speeding up the convergence of iteration methods. _USSR Computational Mathematics and Mathematical Physics_, _4_(5), 1–17.

### AdaGrad 算法
Duchi, J., Hazan, E., & Singer, Y. (2011). Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(Jul), 2121-2159.

Adagrad 算法是一种梯度下降法，它是对批量梯度下降法的改进，但并不是对动量法的改进。Adagrad 算法的目的是在解决优化问题时自动调整学习率，以便能够更快地收敛。

在优化问题中，我们通常需要找到使目标函数最小的参数值。批量梯度下降法是一种求解此类问题的方法，它在每次迭代时使用整个数据集来计算梯度。然而，批量梯度下降法的收敛速度可能较慢，因为它需要较多的计算。Adagrad 算法在每次迭代中，会根据之前的梯度信息自动调整每个参数的学习率。

Adagrad算法会在每次迭代中计算每个参数的梯度平方和，并使用这些平方和来调整学习率。这样，Adagrad算法就可以使用较小的学习率来解决那些更难优化的参数，而使用较大的学习率来解决更容易优化的参数。

### RMSProp 算法
[1] Tieleman, T., & Hinton, G. (2012). Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2), 26-31.

RMSProp 优化算法和 AdaGrad 算法唯一的不同，就在于累积平方梯度的求法不同。RMSProp 算法不是像 AdaGrad 算法那样暴力直接的累加平方梯度，而是加了一个衰减系数来控制历史信息的获取多少。鉴于神经网络都是非凸条件下的，RMSProp 在非凸条件下结果更好,MSProp被证明有效且实用的深度学习网络优化算法。

### AdaDelta 算法

AdaDelta 算法针对 AdaGrad 算法在迭代后期可能较难找到有用解的问题做了改进 [1]。AdaDelta 算法没有学习率这一超参数。
[1] Zeiler, M. D. (2012). ADADELTA: an adaptive learning rate method. arXiv preprint arXiv: 1212.5701.

### Adam 算法
Kingma, D. P., & Ba, J. (2014). Adam: a method for stochastic optimization. _arXiv preprint arXiv: 1412.6980_.

adam 是 Kingma 等人提出的一种随机优化方法，该算法是在梯度下降算法(SGD)的理念上，结合 Adagrad 和 RMSProp 算法提出的，计算时基于目标函数的一阶导数，保证了相对较低的计算量。adma 的优点如下：参数更新的大小不随着梯度大小的缩放而变化；更新参数时的步长的边界受限于超参的步长的设定；不需要固定的目标函数；支持稀疏梯度；它能够自然的执行一种步长的退火。
Adam 算法主要是在 REMSprop 的基础上增加了 momentum，并进行了偏差修正。 
Adam 优点: 
(1) 参数更新的大小不随着梯度大小的缩放而变化；
(2) 更新参数时的步长的边界受限于超参的步长的设定；
(3) 不需要固定的目标函数；支持稀疏梯度；它能够自然的执行一种步长的退火。

Adam 缺点：
(1)可能不收敛(ref: On the Convergence of Adam and Beyond, ICLR2018 )
(2)可能错过全区最优解 (ref1: The Marginal Value of Adaptive Gradient Methods in Machine Learning, NIPS2017 ; ref2: Improving Generalization Performance by Switching from Adam to SGD))

adam 尽管有步长自动退火的功能，但是仍然可以手动设置学习率的递减，并且会有比较好的效果(bert 和 xlnet 训练是均有用到)。考虑到其收敛性问题，adam 可以结合 SGD 同时使用，即先进行 Adam 训练，再进行 SGD。
## 卷积神经网络和循环神经网络的用例

在深度学习领域，循环神经网络（RNN）和卷积神经网络（CNN）作为两类强大的神经网络结构，已经在各个领域展现了出色的应用。本节将讨论它们各自的用例，突显它们在不同任务中的优势和适用性。

### 卷积神经网络的用例

卷积神经网络（Convolutional Neural Networks，CNN）是深度学习领域的重要成果，其在机器视觉和图像处理领域中具有广泛应用。

#### 1. 特征提取

卷积神经网络（CNN）的设计灵感源自人类视觉系统，其核心构建块是卷积层（Convolutional Layer）。在这一层中，通过滤波器（又称卷积核），网络对输入图像进行扫描和提取特征。这一过程模拟了人眼对图像的感知方式。不同的滤波器可以捕捉到图像中的边缘、纹理、颜色等多样特征，将原始像素级的信息逐渐转化为富含语义信息的特征图。这种设计使得CNN能够更有效地学习和理解图像的复杂结构，为图像处理和识别任务提供了强大的能力。

#### 2. 物体识别和分类

CNN在图像分类任务中表现卓越。通过叠加多个卷积层和池化层，CNN能够逐层提取图像中更加抽象的特征，使其能够巧妙地区分不同类别的物体，如猫、狗、汽车等。在训练过程中，CNN通过自动学习，发现哪些特征对于区分不同类别的物体具有判别性，从而实现对物体的高效识别和分类。这种层级学习的方式使得CNN成为处理图像任务的强大工具。

#### 3. 目标检测

除了用于分类任务，CNN 还可以应用于目标检测。通过将卷积网络与边界框回归结合，CNN 能够有效地定位图像中多个目标的位置，并为每个目标分配相应的类别标签。这种技术在自动驾驶、安防监控、医疗影像等领域具有重要的应用价值。

#### 4. 图像分割

CNN在图像分割任务中也展现卓越性能。通过充分利用卷积层和反卷积层，CNN能够将图像精准分割成不同的区域，并为每个区域赋予相应的类别。这一技术在医疗影像、地质勘探、自然灾害评估等多个领域具有重要的意义。

#### 5. 风格转换和图像生成

CNN还可用于图像风格转换和图像生成任务。通过网络的训练，能够将一个图像的风格与另一个图像的内容相融合，创造出独特的艺术作品。此外，基于生成对抗网络（GANs）的CNN模型能够生成逼真的图像，广泛应用于增强现实、虚拟现实等领域。
  
#### 6. 图像降噪

CNN可应用于图像降噪，尤其是对于低质量或受噪声影响的图像。通过网络的训练，CNN能够学习如何消除图像中的噪声，使图像得以还原为更为清晰的版本。

#### 7. 图像超分辨率

CNN也可以用于图像超分辨率，即将低分辨率图像升级到高分辨率。通过学习高分辨率图像和其对应的低分辨率图像之间的关系，CNN可以实现在不丧失细节的情况下提升图像质量。

#### 10. 医疗影像分析

在医学领域，CNN 在分析医学影像方面扮演着关键的角色。它在肿瘤检测、疾病诊断、器官分割等任务中发挥作用，为医生提供更为准确的诊断和治疗支持。

卷积神经网络在图像处理领域的应用多种多样，其卓越的特征提取和自动学习能力使其成为处理图像数据的首选工具。无论是物体辨识还是图像生成，无论涉及医疗领域还是艺术创作，CNN都展示出惊人的潜力和效能，为图像处理领域带来了深刻的革新。
### 循环神经网络


1. **自然语言处理（NLP）：** 自然语言处理（NLP）领域是循环神经网络（RNN）取得显著成功的一个引人注目的领域。在机器翻译方面，RNN 通过对源语言和目标语言之间的序列关系进行建模，能够更好地捕捉翻译过程中的语境和语法规律，从而提高翻译的准确性和流畅性。在语言建模任务中，RNN 被广泛应用于生成连贯的文本，例如在自动文本摘要、文章创作等方面。其对上下文的敏感性使得生成的语言更具逻辑和一致性。

情感分析是另一个NLP领域，RNN在该任务中展现出色。通过对文本序列的逐词处理，RNN能够感知和理解文本中的情感变化，使得情感分析系统能够更准确地判断文本作者的情感倾向。这对于社交媒体监测、产品评论分析等应用至关重要。

除了以上提到的任务，RNN在NLP中还应用于问答系统、命名实体识别、对话生成等多个方面。其在处理变长序列和对上下文的灵活适应性使得在复杂的自然语言处理任务中表现卓越。通过不断的优化和改进，RNN在NLP领域的成功经验也为后续的模型提供了有益的启示。

1. **语音识别**：RNN 在语音识别领域发挥着重要的作用，其应用范围涵盖语音助手、语音命令识别等多个领域。通过处理音频序列，RNN 能够有效地将语音信息转换为对应的文本，为人机交互提供了便捷的方式。在语音助手中，用户可以通过自然语言与设备进行交流，RNN 有助于准确理解和执行用户的语音指令，提升了智能助手的交互性和实用性。在语音命令识别方面，RNN 的应用使得智能设备能够根据用户口头指令执行相应任务，例如调整音量、播放音乐等，为用户提供更智能、便捷的使用体验。这些应用不仅改善了日常生活中的交互方式，也在驾驶、医疗护理等领域发挥着重要的角色。随着技术的不断进步，RNN 在语音识别领域的应用将继续推动语音技术的发展，为人们创造更为智能、友好的语音交互环境。
     
2. **时间序列预测：** RNN在时间序列数据的预测方面展现出卓越的性能，广泛应用于金融、气象学、股票市场等领域。其独特的序列建模能力使其能够捕捉到时间序列中的复杂时序关系，从而提供了更为准确的预测结果。在金融领域，RNN被用于股票价格预测、汇率波动预测等任务，帮助投资者做出更明智的决策。在气象学中，RNN可应用于天气预测，考虑到气象数据的时变性，提供更精准的气象预报。而在股票市场中，RNN的应用有助于理解和预测市场趋势，为投资者提供更全面的市场信息。这些应用不仅在提高决策的准确性方面有重要意义，也为各行业在时间序列数据分析和预测方面提供了强大的工具。随着深度学习技术的不断进步，RNN在时间序列预测领域的发展前景更为广阔，为未来的科学研究和商业应用带来了更多的可能性。

1. **图像生成描述：** RNN结合卷积神经网络（CNN）的融合不仅能生成图像描述，更能为图像赋予生动的语言表达，这一技术在计算机视觉和图像理解领域掀起了一场革命。通过对图像进行卷积操作，CNN能够提取图像中的高级特征，而RNN则通过逐词建模的方式，将这些特征转化为自然语言。这种联合应用使得生成的描述不仅仅是对图像内容的简单解释，更是对图像语境的深度理解。在计算机视觉中，这项技术被广泛应用于图像检索、辅助盲人理解图像内容、图像注释等方面。在艺术创作中，RNN和CNN的结合使得计算机能够更具创造性地理解图像并生成富有表现力的语言描述，促进了计算机与艺术之间的跨界合作。这种图像生成描述的应用不仅拓展了图像处理的应用场景，也为推动人工智能在视觉和语言交互领域的发展贡献了重要力量。
    
5. **医学影像分析：** RNN 在医学领域的应用不仅仅局限于处理生命体征监测数据或医学图像序列，而是成为医学影像分析的一项关键技术。在处理时间序列的生命体征监测数据方面，RNN 能够动态地捕捉患者的生理变化，为医生提供更全面的临床信息，支持疾病的早期诊断和治疗决策。而在医学图像序列方面，RNN 通过对图像序列进行逐帧分析，能够更好地理解和识别医学图像中的病变、器官结构等关键信息，从而促进了医学影像的自动化分析和精准医疗的实现。这种技术的应用不仅提高了医学影像的解读效率，还为个性化治疗、远程监测等医疗应用带来了新的可能性。RNN 在医学影像领域的进展为医疗健康提供了更先进的工具，为未来医疗技术的发展奠定了坚实的基础。


## 4.2 Dataset

### 4.2.2 CIFAR-10
CIFAR-10 是一个彩色图片数据集，它有 10 个类别: "airplane", "automobile", "bird", "cat","deer", "dog", "frog", "horse", "ship", "truck"。每张图片都是  3 * 32 * 32 ，也即 3 通道（RGB）彩色图片，分辨率为 32 * 32。

该数据集共有 60000 张彩色图像，这些图像是 32 * 32，分为 10 个类，每类 6000 张图。这里面有 50000 张用于训练，构成了 5 个训练批，每一批 10000 张图；另外 10000 用于测试，单独构成一批。测试批的数据里，取自 10 类中的每一类，每一类随机取 1000 张。抽剩下的就随机排列组成了训练批。注意一个训练批中的各类图像并不一定数量相同，总的来看训练批，每一类都有 5000 张图。

Here are the classes in the dataset, as well as 10 random images from each: [CIFAR-10 and CIFAR-100 datasets (toronto.edu)](https://www.cs.toronto.edu/~kriz/cifar.html)
![[Pasted image 20231125140355.png]]


## 4.4 分类器选择
在对 CIFAR-10 数据集进行分类任务时，VGG 和 ResNet 都可以表现得很好，但它们有一些不同的特点。

1. **VGG:**
    - VGG 架构相对简单，由多个卷积层和池化层组成，具有深层次的网络结构。
    - VGG 的主要思想是使用小尺寸的卷积核（3x3）和深层网络来增加模型的深度。
    - 在 CIFAR-10 上，VGG 的性能可能会受到参数数量的限制，因为 CIFAR-10 是一个相对较小的数据集。
2. **ResNet:**
    - ResNet（残差网络）引入了残差学习，通过使用残差块允许网络直接学习残差，从而更容易训练深层网络。
    - ResNet 的主要优势在于它能够训练非常深的网络而不会出现梯度消失的问题。
    - 在 CIFAR-10 上，由于 ResNet 的残差结构，通常能够更好地捕捉图像的特征，尤其是对于更复杂的模式。

总体而言，对于 CIFAR-10 这样的小型图像分类任务，ResNet 往往会比 VGG 更好。ResNet 的残差结构使得训练更加容易，且通常能够在较少的训练轮次内取得更好的性能。

ResNet18, ResNet34, ResNet50, ResNet101, ResNet152 等是 ResNet 的几个变种，它们的区别主要在于网络深度和参数量的不同。其中，ResNet18 和 ResNet34 是比较浅的网络，适合于小规模数据集的训练；ResNet50、ResNet101 和 ResNet152 则是比较深的网络，适合于大规模数据集的训练。

CIFAR-10 数据集是一个小型数据集，因此本报告采用 ResNet18 来完成分类任务。
## 4.4 Classifier Introduction
[1512.03385.pdf (arxiv.org)](https://arxiv.org/pdf/1512.03385.pdf)

ResNet18 的基本含义是，网络的基本架构是 ResNet，网络的深度是 18 层。但是这里的网络深度指的是网络的权重层，也就是包括池化，激活，线性层。而不包括批量化归一层，池化层。  

ResNet 结构
![[v2-181cd2dc1d4dc7f3cb05f844d96017f4_1440w.webp]]

ResNet 的核心思想是引入一个 identity shortcut connection，直接跳过一层或多层，通过添加额外的连接来解决深度神经网络训练中的**梯度消失和梯度爆炸**等问题，从而允许构建非常深的神经网络。如下图所示：
![[Pasted image 20231226153158.png]]
Figure 2. Residual learning: a building block.

ResNet18 中有很多重复的单元，它们都有跨层直连的 shortcut。ResNet 中，将一个跨层直连的单元称为 Residual block。 ResNet18 中，每两个 Residual block 组成一个 layer。除第一层卷积外，有 4 个 layer。

下图就是一个 ResNet18的基本网络架构，其中并未加入批量化归一和池化层。

ResNet18 的网络架构 
![[Pasted image 20231226153806.png]]

# 5 Results and discussion


#### 5.2.1.2 结果分析



# 6 Concclusion
结论：

(1) 随机森林和 KNN 在 Fashion-MNIST 和 CIFAR-10 数据集上的分类效果均较好，其中随机森林在 Fashion-MNIST 数据集上的表现更优。
(2) 随机森林和 KNN 在分类结果上的差异主要体现在精度、召回率和 F1-score 上，其中随机森林在精度和 F1-score 上表现更优，而 KNN 在召回率上表现更优。
(3) 随机森林和 KNN 在分类结果上的差异可能与数据集的特征分布、类别间的相似度以及模型的参数设置等因素有关。


# 7 References



# 8 Appendices