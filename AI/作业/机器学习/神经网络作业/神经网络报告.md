[pytorch利用卷积神经网络进行CIFAR-10图像分类_使用 pytorch 和 cnn 对 cifar-10 数据集进行图像分类-CSDN博客](https://blog.csdn.net/qq_28368377/article/details/106245489)


报告写法：[构建报告 - 报告写作 - LibGuides at University of Reading](https://libguides.reading.ac.uk/reports/structuring)
# 使用 ML 算法和方法进行分类
# 目录
# 1 Abstract

Keywords:
# 2 Introduction

  
人工神经网络（Artificial Neural Networks, ANN）是受到人类大脑神经元网络结构启发而设计的一类机器学习模型。它们模拟了生物神经网络的基本工作原理，包括多个神经元（也称为节点或单元）之间的连接、信息传递和权重调整。人工神经网络在机器学习领域的应用广泛，它们的灵活性和强大的学习能力使其成为解决复杂任务和大规模数据集的理想选择。

本报告的目标是使用人工神经网络知识以及用于生成能够对图像进行分类的神经网络模型的方法，并描述循环神经网络的用例。

本文的结构如下︰

# 3 Related work
以下是人工神经网络相关的经典文献和相关工作

Y. Lecun 等人提出了 LeNet，这是一个最早发布的卷积神经网络之一，在计算机视觉任务中展现了高效性能，用于识别图像中的手写数字。
[Gradient-based learning applied to document recognition | IEEE Journals & Magazine | IEEE Xplore](https://ieeexplore.ieee.org/document/726791)

A. Krizhevsky 等人提出了 AlexNet，这是一个深度卷积神经网络，首次在 ImageNet 图像分类竞赛中获得显著性能提升。
[[PDF] ImageNet classification with deep convolutional neural networks | Semantic Scholar](https://www.semanticscholar.org/paper/ImageNet-classification-with-deep-convolutional-Krizhevsky-Sutskever/abd1c342495432171beb7ca8fd9551ef13cbd0ff)

Karen Simonyan 和 Andrew Zisserman 提出了 VGGNet，使用了非常小的卷积核和一个更深的卷积神经网络结构。在 mageNet 挑战赛中获得成功并可以推广到其他数据集。
[[1409.1556] Very Deep Convolutional Networks for Large-Scale Image Recognition (arxiv.org)](https://arxiv.org/abs/1409.1556)

Lin 等人于2013年提出的 NiN 深度神经网络结构。NiN 的设计旨在增加网络的表达能力，改进特征提取的方式，并减轻过拟合的问题。创造性的在每个像素的通道上分别使用多层感知机。
[[1703.03130] A Structured Self-attentive Sentence Embedding (arxiv.org)]( https://arxiv.org/abs/1703.03130 )

Szegedy 等人在 2014 年提出 GoogLeNet，这是一个著名的深度卷积神经网络架构，主要用于图像分类任务。其创新点之一是引入了 Inception 模块，使得网络更加深层、宽阔，同时仍然保持高效。
[Rethinking the Inception Architecture for Computer Vision | IEEE Conference Publication | IEEE Xplore --- 重新思考计算机视觉的 Inception 架构 | IEEE 会议出版物 | IEEE探索](https://ieeexplore.ieee.org/document/7780677)

He 等人 ResNet ，提出的一种深度卷积神经网络结构，旨在解决深度神经网络中的梯度消失和梯度爆炸问题。在2015年的 ImageNet 挑战赛获得冠军，引入了 residual blocks，对如何建立深层神经网络产生深远影响。
[[1703.03130] A Structured Self-attentive Sentence Embedding (arxiv.org)](https://arxiv.org/abs/1703.03130)

Zachary 等人在其综述中全面评述循环神经网络（RNN）在序列学习任务中的应用。详细介绍了 RNN 的基本原理、结构、梯度传播算法以及在实际任务中的表现。

[[1506.00019] 用于序列学习的递归神经网络的批判性综述 (arxiv. org)]( https://arxiv.org/abs/1506.00019 )

F.A. Gers 等人提出了 LSTM，解决了传统 RNN 中的长期依赖问题。使用了自适应的"forget gate"来防止网络崩溃。
[Learning to forget: continual prediction with LSTM | IET Conference Publication | IEEE Xplore](https://ieeexplore.ieee.org/document/818041)

Ilya Sutskever 等人提出了一种通用的端到端序列学习方法，展示了 LSTM 在序列到序列学习中的成功应用，在 WMT'14 数据集的英法翻译任务重达到了较高分数。
[[1409.3215] Sequence to Sequence Learning with Neural Networks (arxiv.org)](https://arxiv.org/abs/1409.3215)
# 4 Main body
## 4.1 优化人工神经网络的方法
人工神经网络的优化目标是通过调整网络参数，使其在特定任务上最小化损失函数，提高模型的泛化能力，最大化性能指标，加速收敛过程，降低计算和存储成本，提高鲁棒性，以及适应不同任务和领域，从而实现对复杂数据关系的学习和智能决策。通常要最大限度减少神经网络的损失函数，除了使用优化算法来减少训练误差之外，我们还需要注意过拟合。

### 随机梯度下降
随机梯度下降（SGD）是机器学习中常用的优化算法之一。它是一种迭代的优化算法，用于最小化损失函数，并更新模型参数以适应训练数据。与传统的梯度下降算法相比，随机梯度下降每次迭代只使用一个样本来计算梯度，因此具有更快的训练速度。
小批量随机梯度下降（Mini-Batch Stochastic Gradient Descent，Mini-Batch SGD）是在训练神经网络时使用的一种优化算法。与传统的梯度下降算法不同，小批量SGD每次更新模型参数时仅使用小批量（mini-batch）随机选择的训练样本的梯度信息。

### 动量法
动量法（Momentum）由 Polyak 提出，是一种用于优化算法的技术，特别是在神经网络的训练中常被使用。动量法用过去梯度的平均值来替换梯度，这大大加快了收敛速度。 对于无噪声梯度下降和嘈杂随机梯度下降，动量法都是可取的。动量法可以防止在随机梯度下降的优化过程停滞的问题。
Polyak, B. T. (1964). Some methods of speeding up the convergence of iteration methods. _USSR Computational Mathematics and Mathematical Physics_, _4_(5), 1–17.

### AdaGrad 算法
Duchi, J., Hazan, E., & Singer, Y. (2011). Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(Jul), 2121-2159.

Adagrad 算法是一种梯度下降法，它是对批量梯度下降法的改进，但并不是对动量法的改进。Adagrad 算法的目的是在解决优化问题时自动调整学习率，以便能够更快地收敛。

在优化问题中，我们通常需要找到使目标函数最小的参数值。批量梯度下降法是一种求解此类问题的方法，它在每次迭代时使用整个数据集来计算梯度。然而，批量梯度下降法的收敛速度可能较慢，因为它需要较多的计算。Adagrad 算法在每次迭代中，会根据之前的梯度信息自动调整每个参数的学习率。

Adagrad算法会在每次迭代中计算每个参数的梯度平方和，并使用这些平方和来调整学习率。这样，Adagrad算法就可以使用较小的学习率来解决那些更难优化的参数，而使用较大的学习率来解决更容易优化的参数。

### RMSProp 算法
[1] Tieleman, T., & Hinton, G. (2012). Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2), 26-31.

RMSProp 优化算法和 AdaGrad 算法唯一的不同，就在于累积平方梯度的求法不同。RMSProp 算法不是像 AdaGrad 算法那样暴力直接的累加平方梯度，而是加了一个衰减系数来控制历史信息的获取多少。鉴于神经网络都是非凸条件下的，RMSProp 在非凸条件下结果更好,MSProp被证明有效且实用的深度学习网络优化算法。

### AdaDelta 算法

AdaDelta 算法针对 AdaGrad 算法在迭代后期可能较难找到有用解的问题做了改进 [1]。AdaDelta 算法没有学习率这一超参数。
[1] Zeiler, M. D. (2012). ADADELTA: an adaptive learning rate method. arXiv preprint arXiv: 1212.5701.

### Adam 算法
Kingma, D. P., & Ba, J. (2014). Adam: a method for stochastic optimization. _arXiv preprint arXiv: 1412.6980_.

adam 是 Kingma 等人提出的一种随机优化方法，该算法是在梯度下降算法(SGD)的理念上，结合 Adagrad 和 RMSProp 算法提出的，计算时基于目标函数的一阶导数，保证了相对较低的计算量。adma 的优点如下：参数更新的大小不随着梯度大小的缩放而变化；更新参数时的步长的边界受限于超参的步长的设定；不需要固定的目标函数；支持稀疏梯度；它能够自然的执行一种步长的退火。
Adam 算法主要是在 REMSprop 的基础上增加了 momentum，并进行了偏差修正。 
Adam 优点: 
(1) 参数更新的大小不随着梯度大小的缩放而变化；
(2) 更新参数时的步长的边界受限于超参的步长的设定；
(3) 不需要固定的目标函数；支持稀疏梯度；它能够自然的执行一种步长的退火。

Adam 缺点：
(1)可能不收敛(ref: On the Convergence of Adam and Beyond, ICLR2018 )
(2)可能错过全区最优解 (ref1: The Marginal Value of Adaptive Gradient Methods in Machine Learning, NIPS2017 ; ref2: Improving Generalization Performance by Switching from Adam to SGD))

adam 尽管有步长自动退火的功能，但是仍然可以手动设置学习率的递减，并且会有比较好的效果(bert 和 xlnet 训练是均有用到)。考虑到其收敛性问题，adam 可以结合 SGD 同时使用，即先进行 Adam 训练，再进行 SGD。
## 卷积神经网络和循环神经网络的用例

在深度学习领域，循环神经网络（RNN）和卷积神经网络（CNN）作为两类强大的神经网络结构，已经在各个领域展现了出色的应用。本节将讨论它们各自的用例，突显它们在不同任务中的优势和适用性。

### 卷积神经网络的用例

卷积神经网络（Convolutional Neural Networks，CNN）是深度学习领域的重要成果，其在机器视觉和图像处理领域中具有广泛应用。

#### 1. 特征提取

卷积神经网络（CNN）的设计灵感源自人类视觉系统，其核心构建块是卷积层（Convolutional Layer）。在这一层中，通过滤波器（又称卷积核），网络对输入图像进行扫描和提取特征。这一过程模拟了人眼对图像的感知方式。不同的滤波器可以捕捉到图像中的边缘、纹理、颜色等多样特征，将原始像素级的信息逐渐转化为富含语义信息的特征图。这种设计使得CNN能够更有效地学习和理解图像的复杂结构，为图像处理和识别任务提供了强大的能力。

#### 2. 物体识别和分类

CNN在图像分类任务中表现卓越。通过叠加多个卷积层和池化层，CNN能够逐层提取图像中更加抽象的特征，使其能够巧妙地区分不同类别的物体，如猫、狗、汽车等。在训练过程中，CNN通过自动学习，发现哪些特征对于区分不同类别的物体具有判别性，从而实现对物体的高效识别和分类。这种层级学习的方式使得CNN成为处理图像任务的强大工具。

#### 3. 目标检测

除了用于分类任务，CNN 还可以应用于目标检测。通过将卷积网络与边界框回归结合，CNN 能够有效地定位图像中多个目标的位置，并为每个目标分配相应的类别标签。这种技术在自动驾驶、安防监控、医疗影像等领域具有重要的应用价值。

#### 4. 图像分割

CNN在图像分割任务中也展现卓越性能。通过充分利用卷积层和反卷积层，CNN能够将图像精准分割成不同的区域，并为每个区域赋予相应的类别。这一技术在医疗影像、地质勘探、自然灾害评估等多个领域具有重要的意义。

#### 5. 风格转换和图像生成

CNN还可用于图像风格转换和图像生成任务。通过网络的训练，能够将一个图像的风格与另一个图像的内容相融合，创造出独特的艺术作品。此外，基于生成对抗网络（GANs）的CNN模型能够生成逼真的图像，广泛应用于增强现实、虚拟现实等领域。
  
#### 6. 图像降噪

CNN可应用于图像降噪，尤其是对于低质量或受噪声影响的图像。通过网络的训练，CNN能够学习如何消除图像中的噪声，使图像得以还原为更为清晰的版本。

#### 7. 图像超分辨率

CNN也可以用于图像超分辨率，即将低分辨率图像升级到高分辨率。通过学习高分辨率图像和其对应的低分辨率图像之间的关系，CNN可以实现在不丧失细节的情况下提升图像质量。

#### 10. 医疗影像分析

在医学领域，CNN 在分析医学影像方面扮演着关键的角色。它在肿瘤检测、疾病诊断、器官分割等任务中发挥作用，为医生提供更为准确的诊断和治疗支持。

卷积神经网络在图像处理领域的应用多种多样，其卓越的特征提取和自动学习能力使其成为处理图像数据的首选工具。无论是物体辨识还是图像生成，无论涉及医疗领域还是艺术创作，CNN都展示出惊人的潜力和效能，为图像处理领域带来了深刻的革新。
### 循环神经网络


1. **自然语言处理（NLP）：** 自然语言处理（NLP）领域是循环神经网络（RNN）取得显著成功的一个引人注目的领域。在机器翻译方面，RNN 通过对源语言和目标语言之间的序列关系进行建模，能够更好地捕捉翻译过程中的语境和语法规律，从而提高翻译的准确性和流畅性。在语言建模任务中，RNN 被广泛应用于生成连贯的文本，例如在自动文本摘要、文章创作等方面。其对上下文的敏感性使得生成的语言更具逻辑和一致性。

情感分析是另一个NLP领域，RNN在该任务中展现出色。通过对文本序列的逐词处理，RNN能够感知和理解文本中的情感变化，使得情感分析系统能够更准确地判断文本作者的情感倾向。这对于社交媒体监测、产品评论分析等应用至关重要。

除了以上提到的任务，RNN在NLP中还应用于问答系统、命名实体识别、对话生成等多个方面。其在处理变长序列和对上下文的灵活适应性使得在复杂的自然语言处理任务中表现卓越。通过不断的优化和改进，RNN在NLP领域的成功经验也为后续的模型提供了有益的启示。

1. **语音识别**：RNN 在语音识别领域发挥着重要的作用，其应用范围涵盖语音助手、语音命令识别等多个领域。通过处理音频序列，RNN 能够有效地将语音信息转换为对应的文本，为人机交互提供了便捷的方式。在语音助手中，用户可以通过自然语言与设备进行交流，RNN 有助于准确理解和执行用户的语音指令，提升了智能助手的交互性和实用性。在语音命令识别方面，RNN 的应用使得智能设备能够根据用户口头指令执行相应任务，例如调整音量、播放音乐等，为用户提供更智能、便捷的使用体验。这些应用不仅改善了日常生活中的交互方式，也在驾驶、医疗护理等领域发挥着重要的角色。随着技术的不断进步，RNN 在语音识别领域的应用将继续推动语音技术的发展，为人们创造更为智能、友好的语音交互环境。
     
2. **时间序列预测：** RNN在时间序列数据的预测方面展现出卓越的性能，广泛应用于金融、气象学、股票市场等领域。其独特的序列建模能力使其能够捕捉到时间序列中的复杂时序关系，从而提供了更为准确的预测结果。在金融领域，RNN被用于股票价格预测、汇率波动预测等任务，帮助投资者做出更明智的决策。在气象学中，RNN可应用于天气预测，考虑到气象数据的时变性，提供更精准的气象预报。而在股票市场中，RNN的应用有助于理解和预测市场趋势，为投资者提供更全面的市场信息。这些应用不仅在提高决策的准确性方面有重要意义，也为各行业在时间序列数据分析和预测方面提供了强大的工具。随着深度学习技术的不断进步，RNN在时间序列预测领域的发展前景更为广阔，为未来的科学研究和商业应用带来了更多的可能性。

1. **图像生成描述：** RNN结合卷积神经网络（CNN）的融合不仅能生成图像描述，更能为图像赋予生动的语言表达，这一技术在计算机视觉和图像理解领域掀起了一场革命。通过对图像进行卷积操作，CNN能够提取图像中的高级特征，而RNN则通过逐词建模的方式，将这些特征转化为自然语言。这种联合应用使得生成的描述不仅仅是对图像内容的简单解释，更是对图像语境的深度理解。在计算机视觉中，这项技术被广泛应用于图像检索、辅助盲人理解图像内容、图像注释等方面。在艺术创作中，RNN和CNN的结合使得计算机能够更具创造性地理解图像并生成富有表现力的语言描述，促进了计算机与艺术之间的跨界合作。这种图像生成描述的应用不仅拓展了图像处理的应用场景，也为推动人工智能在视觉和语言交互领域的发展贡献了重要力量。
    
5. **医学影像分析：** RNN 在医学领域的应用不仅仅局限于处理生命体征监测数据或医学图像序列，而是成为医学影像分析的一项关键技术。在处理时间序列的生命体征监测数据方面，RNN 能够动态地捕捉患者的生理变化，为医生提供更全面的临床信息，支持疾病的早期诊断和治疗决策。而在医学图像序列方面，RNN 通过对图像序列进行逐帧分析，能够更好地理解和识别医学图像中的病变、器官结构等关键信息，从而促进了医学影像的自动化分析和精准医疗的实现。这种技术的应用不仅提高了医学影像的解读效率，还为个性化治疗、远程监测等医疗应用带来了新的可能性。RNN 在医学影像领域的进展为医疗健康提供了更先进的工具，为未来医疗技术的发展奠定了坚实的基础。


## 4.2 Dataset

### 4.2.2 CIFAR-10
CIFAR-10 是一个彩色图片数据集，它有 10 个类别: "airplane", "automobile", "bird", "cat","deer", "dog", "frog", "horse", "ship", "truck"。每张图片都是  3 * 32 * 32 ，也即 3 通道（RGB）彩色图片，分辨率为 32 * 32。

该数据集共有 60000 张彩色图像，这些图像是 32 * 32，分为 10 个类，每类 6000 张图。这里面有 50000 张用于训练，构成了 5 个训练批，每一批 10000 张图；另外 10000 用于测试，单独构成一批。测试批的数据里，取自 10 类中的每一类，每一类随机取 1000 张。抽剩下的就随机排列组成了训练批。注意一个训练批中的各类图像并不一定数量相同，总的来看训练批，每一类都有 5000 张图。

Here are the classes in the dataset, as well as 10 random images from each: [CIFAR-10 and CIFAR-100 datasets (toronto.edu)](https://www.cs.toronto.edu/~kriz/cifar.html)
![[Pasted image 20231125140355.png]]

## 4.4 Classifier Introduction
[【Tensorflow 2.0 正式版教程】ResNet分类CIFAR-10 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/86665955)
[【使用TensorFlow构建图像分类模型CIFAR-10】_tensorflow cifar10-CSDN博客](https://blog.csdn.net/qq_66726657/article/details/132838479)

通过分析卷积神经网络和循环神经网络的用例，使用卷积神经网络对数据集分类更为合理。 ResNet 在

## 4.4 分类器选择的合理性
### 4.4.1 随机森林

(1) 高准确性： 随机森林是一种集成学习方法，通过组合多个决策树的预测结果，可以提高整体模型的准确性。每个决策树都是基学习器，通过投票或平均的方式来进行分类，减少了过拟合的风险。
(2) 对特征的自适应性： 随机森林对于特征的选择具有自适应性，它会在每个决策树的训练过程中随机选择一部分特征进行划分。这有助于减少特征之间的相关性，提高模型的泛化能力，避免过度依赖某些特征。
(3) 处理大规模数据： 随机森林通常对大规模数据表现良好，因为可以并行训练多个决策树。这有助于加速模型的训练过程，使得随机森林在大规模数据集上具有可扩展性。
(4) 能够处理高维数据： 随机森林在高维数据上的表现相对较好，不容易受到维度灾难的影响。通过随机选择特征，可以降低维度的复杂性，提高模型的鲁棒性。
(5) 抗过拟合能力：随机森林通过引入随机性，包括随机选择样本和随机选择特征，减缓了过拟合的风险。这使得模型更具泛化能力，对新数据的适应性较好。
(6) 提供特征重要性评估： 随机森林可以输出各个特征的重要性，帮助理解模型对问题的解释，有助于特征选择和模型解释。

总体而言，随机森林是一种强大的分类方法，适用于各种问题，并在许多实际场景中表现出色。
### 4.1.2 KNN 
选择使用 KNN（K-最近邻）方法的合理性可以从以下几个方面考虑：

1. 简单实现： KNN 是一种非常简单直观的分类算法。它不需要训练阶段，而是在预测时直接根据最近邻的样本进行分类。这种简单性使得 KNN 易于理解和实现。
2. 无参数： KNN 是一种无参数的算法，不需要对模型进行复杂的参数调整。这使得在处理一些复杂的问题时，不需要过多的领域知识，而且可以更加灵活地应用在不同类型的数据上。
3. 适用于小规模数据集： 如果数据集相对较小，KNN 可以是一个合适的选择。由于 KNN 算法在训练时不需要构建模型，而是在预测时计算距离，因此对于小规模数据集而言，计算成本相对较低。
4. 适用于局部性较强的问题： KNN 的核心思想是利用邻近样本的信息进行分类，因此适用于局部性较强的问题。如果数据在特征空间中有较好的聚集性，KNN 可能能够取得不错的效果。
5. 不受数据分布影响： KNN 对于数据的分布没有过多的假设，适用于各种类型的数据分布情况。这使得它在处理一些复杂的、非线性的问题时表现较为出色。

然而，需要注意的是，KNN 也有一些缺点，例如对于大规模数据集的计算成本较高，对噪声和异常值敏感等。在实际应用中，需要根据具体问题和数据的特点来综合考虑是否选择 KNN 算法。
# 5 Results and discussion

## 5.2 CIFAR-10

### 5.2.1 随机森林
#### 5.2.1.1 分类结果
分类结果信息表格：

| class | precision | recall | f1-score | support |
|-------|-----------|--------|----------|---------|
| 0     | 0.54      | 0.56   | 0.55     | 1000    |
| 1     | 0.52      | 0.53   | 0.52     | 1000    |
| 2     | 0.36      | 0.33   | 0.35     | 1000    |
| 3     | 0.33      | 0.27   | 0.30     | 1000    |
| 4     | 0.39      | 0.38   | 0.38     | 1000    |
| 5     | 0.40      | 0.39   | 0.39     | 1000    |
| 6     | 0.47      | 0.56   | 0.51     | 1000    |
| 7     | 0.51      | 0.45   | 0.48     | 1000    |
| 8     | 0.59      | 0.62   | 0.60     | 1000    |
| 9     | 0.47      | 0.55   | 0.51     | 1000    |
||||||
| accuracy    | -   | -   | 0.46     | 10000   |
| macro avg   | 0.46| 0.46| 0.46     | 10000   |
| weighted avg| 0.46| 0.46| 0.46     | 10000   |

混淆矩阵：
```
[[[8529  471]
  [ 438  562]]

 [[8513  487]
  [ 475  525]]

 [[8415  585]
  [ 666  334]]

 [[8460  540]
  [ 733  267]]

 [[8400  600]
  [ 622  378]]

 [[8421  579]
  [ 614  386]]

 [[8367  633]
  [ 435  565]]

 [[8567  433]
  [ 547  453]]

 [[8573  427]
  [ 382  618]]

 [[8391  609]
  [ 452  548]]]
```

#### 5.2.1.2 结果分析
(1) Accuracy 为 0.46，说明整体分类效果一般。
(2) Precision 精确度的平均值为 0.46，加权平均也为 0.46，说明模型对负样本的区分能力一般。
(3) Recall 的平均值为 0.46。说明模型对正样本的识别能力一般。
(4) F1-score 的加权平均为 0.46，说明模型不太稳健
(5) Macro average 和 Micro average 相等，说明每个类别对整体性能的贡献基本相同，模型具有较为均衡的分类能力。

混淆矩阵反映了模型在每个类别上的具体表现。例如，在类别 0 中，有 8529 个样本被正确分类为类别 0，但有 471 个样本被错误分类为其他类别。同样，在类别 0 中，有 438 个样本被错误分类为类别 1，有 562 个样本被正确分类为类别 0。

类别 2、3、4、5、6、7、9 的精确度较低，可能是因为这些类别的样本在数据集中分布较为分散或者类别之间的特征差异较小，导致模型难以准确分类。

类别 3、4、5、6、7 的召回率较低，说明模型在这些类别中漏检了很多正样本。可能是因为这些类别的样本特征与其他类别相似，使得模型难以正确识别。

总体而言，模型在 CIFAR-10 数据集上的分类效果较一般，可能需要进一步优化模型结构或调整参数以提高性能。
### 5.2.2 KNN

#### 5.2.2.1 分类结果
分类结果信息表格：

| class | precision | recall | f1-score | support |
|-------|-----------|--------|----------|---------|
| 0     | 0.38      | 0.54   | 0.45     | 1000    |
| 1     | 0.65      | 0.20   | 0.31     | 1000    |
| 2     | 0.23      | 0.45   | 0.30     | 1000    |
| 3     | 0.29      | 0.22   | 0.25     | 1000    |
| 4     | 0.24      | 0.51   | 0.33     | 1000    |
| 5     | 0.39      | 0.22   | 0.28     | 1000    |
| 6     | 0.35      | 0.25   | 0.29     | 1000    |
| 7     | 0.68      | 0.21   | 0.32     | 1000    |
| 8     | 0.40      | 0.66   | 0.50     | 1000    |
| 9     | 0.70      | 0.14   | 0.23     | 1000    |
||||||
| accuracy    | -   | -   | 0.34     | 10000   |
| macro avg   | 0.43| 0.34| 0.33     | 10000   |
| weighted avg| 0.43| 0.34| 0.33     | 10000   |

混淆矩阵：
```
[[[8135  865]
  [ 463  537]]

 [[8888  112]
  [ 795  205]]

 [[7446 1554]
  [ 548  452]]

 [[8479  521]
  [ 783  217]]

 [[7408 1592]
  [ 486  514]]

 [[8659  341]
  [ 780  220]]

 [[8532  468]
  [ 752  248]]

 [[8900  100]
  [ 790  210]]

 [[8012  988]
  [ 345  655]]

 [[8939   61]
  [ 860  140]]]
```

#### 5.2.2.2 结果分析
首先，让我们解释表格中的各项指标：
(1) Accuracy 为 0.34，说明整体分类效果一般。
(2) recision 的平均值为 0.43，加权平均也为 0.43，说明模型对负样本的区分能力一般。
(3) Recall 召回率的平均值为 0.34，说明模型对正样本的识别能力较弱。
(4) F1-score 的加权平均为 0.33，模型说明模型不够稳健
(5) Macro average 和 Micro average 相等，说明每个类别对整体性能的贡献基本相同，模型具有较为均衡的分类能力。

混淆矩阵反映了模型在每个类别上的具体表现。例如，在类别 0 中，有 8135 个样本被正确分类为类别 0，但有 865 个样本被错误分类为其他类别。同样，在类别 0 中，有 463 个样本被错误分类为类别 1，有 537 个样本被正确分类为类别 0。

类别 1、2、3、5、6、8、9 的精确度较低，可能是因为这些类别的样本在数据集中分布较为分散或者类别之间的特征差异较小，导致模型难以准确分类。

类别 1、5、8、9 的召回率较低，说明模型在这些类别中漏检了很多正样本。可能是因为这些类别的样本特征与其他类别相似，使得模型难以正确识别。

总体而言，模型在 CIFAR-10 数据集上的分类效果较一般，可能需要进一步优化模型结构或调整参数以提高性能。
# 6 Concclusion
结论：

(1) 随机森林和 KNN 在 Fashion-MNIST 和 CIFAR-10 数据集上的分类效果均较好，其中随机森林在 Fashion-MNIST 数据集上的表现更优。
(2) 随机森林和 KNN 在分类结果上的差异主要体现在精度、召回率和 F1-score 上，其中随机森林在精度和 F1-score 上表现更优，而 KNN 在召回率上表现更优。
(3) 随机森林和 KNN 在分类结果上的差异可能与数据集的特征分布、类别间的相似度以及模型的参数设置等因素有关。


# 7 References

[Fashion Images Classification using Machine Learning, Deep Learning and Transfer Learning Models | IEEE Conference Publication | IEEE Xplore](https://ieeexplore.ieee.org/document/9786364)

# 8 Appendices