> 本文由 [简悦 SimpRead](http://ksria.com/simpread/) 转码， 原文地址 [zhuanlan.zhihu.com](https://zhuanlan.zhihu.com/p/405658103)

**目录**

1.  混淆矩阵
2.  准确率
3.  精确率
4.  召回率
5.  F1 score
6.  参考资料

在机器学习的分类任务中，绕不开准确率 (accuracy)，精确率 (precision)，召回率 (recall)，PR 曲线，F1 score 这几个评估分类效果的指标。而理解这几个评价指标各自的含义和作用对全面认识分类模型的效果有着重要的作用。

本文将对这几个评价指标进行讲解，并结合 sklearn 库进行代码实现。

## 混淆矩阵

在介绍分类任务各个指标之前，需要先了解混淆矩阵（Confusion Matrix）的概念，因为混淆矩阵可以使后续计算准确率，精确率，召回率思路更加清晰。混淆矩阵如下图所示：

![[1d28d2802c54910dd2254a1c1028b6ce_MD5.jpg]]

**真正例和真反例是被正确预测的数据，假正例和假反例是被错误预测的数据**。

*   TP（True Positive）：被正确预测的正例。即该数据的真实值为正例，预测值也为正例的情况；
*   TN（True Negative）：被正确预测的反例。即该数据的真实值为反例，预测值也为反例的情况;
*   FP（False Positive）：被错误预测的正例。即该数据的真实值为反例，但被错误预测成了正例的情况；
*   FN（False Negative）：被错误预测的反例。即该数据的真实值为正例，但被错误预测成了反例的情况。

当理解上面四个值之后，接下来将介绍 Accuracy, Precision, Recall，F1 score。

## 准确率 Accuracy

准确率（Accuracy）表示**分类正确的样本占总样本个数的比例**。

分类正确的样本有两部分组成，分别是预测为正且真实为正的情况，即 TP；还有是预测为负且真实也为负的情况，即 TN。

总样本个数即为 TP, FP, TN, FN 之和。

故计算公式如下：

$$Accuracy=\frac{TP+TN}{TP+FP+TN+FN}$$

Accuracy 是衡量分类模型的最直白的指标，但缺陷也是明显的。假设有 100 个样本，其中有 99 个都是正样本，则分类器只需要一直预测为正例，就可以得到 99% 的准确率，实际上这个分类器性能是很低下的。也就是说，当不同类别的样本所占的比例严重不平衡时，占比大的类别会是影响准确率的最主要的因素。所以，**只有当数据集各个类别的样本比例比较均衡时，Accuracy 这个指标才是一个比较好的衡量标准。因此，必须参考其他指标才能完整评估模型的性能。**

下面来看下 sklearn 中计算 accuracy 的示例：

```c++
import numpy as np
from sklearn.metrics import accuracy_score

y_pred = [2, 1, 1, 0]
y_true = [0, 1, 2, 3]
print(accuracy_score(y_true, y_pred)) # 0.25
print(accuracy_score(y_true, y_pred, normalize=False)) # 1

# 在具有二元标签指示符的多标签分类案例中
print(accuracy_score(np.array([[0,1], [1,1]]), np.ones((2, 2)))) # 0.5

```

函数接口的描述：

> 准确度分类得分  
> 在多标签分类中，此函数计算子集精度：为样本预测的标签集必须_完全匹配 `y_true`（实际标签）中相应的标签集。  
> 参数  
> `y_true`: 一维数组，或标签指示符 / 稀疏矩阵，实际（正确的）标签.  
> `y_pred`: 一维数组，或标签指示符 / 稀疏矩阵，分类器返回的预测标签.  
> `normalize`: 布尔值, 可选的 (默认为 True). 如果为 False，返回分类正确的样本数量，否则，返回正 确分类的得分.  
> `sample_weight`: 形状为 [样本数量] 的数组，可选. 样本权重.  
> 返回值  
> `score`: 浮点型  
> 如果 normalize 为 True，返回正确分类的得分（浮点型），否则返回分类正确的样本数量（整型）.  
> 当 normalize 为 True 时，最好的表现是 score 为 1，当 normalize 为 False 时，最好的表现是 score 未样本数量.

## 精确率 Precision

精确率（Precision）又叫查准率，**表示预测结果为正例的样本中实际为正样本的比例。**

计算公式为：

$Precision=\frac{TP}{TP+FP}$

使用场景：当反例被错误预测成正例（FP）的代价很高时，适合用精确率。根据公式可知，精确率越高，FP 越小。比如在垃圾在垃圾邮件检测中，假正例意味着非垃圾邮件（实际为负）被错误的预测为垃圾邮件（预测为正）。如果一个垃圾邮件监测系统的查准率不高导致很多非垃圾邮件被归到垃圾邮箱里去，那么邮箱用户可能会丢失或者漏看一些很重要的邮件。

下面来看下 sklearn 中计算 precision 的示例：

```
from skearn.metrics import precision_score

y_pred = [0, 2, 1, 0, 0 ,1]
y_true = [0, 1 ,2, 0 ,1, 2]

print(precision_score(y_true, y_pred, average='micro')) # 0.3333333333333333
print(precision_score(y_true, y_pred, average='macro')) # 0.2222222222222222
print(precision_score(y_true, y_pred, average='weighted')) # 0.2222222222222222
print(precision_score(y_true, y_pred, average=None)) # [0.66666667 0.         0.        ]

```

函数接口的描述：

> 计算精确率  
> 精确率是 tp / (tp + fp) 的比例，其中 tp 是真正性的数量，fp 是假正性的数量. 精确率直观地可以说是分类器不将负样本标记为正样本的能力.  
> 精确率最好的值是 1，最差的值是 0.  
> 参数  
> _y_true_: 一维数组，或标签指示符 / 稀疏矩阵，实际（正确的）标签.  
> _y_pred_: 一维数组，或标签指示符 / 稀疏矩阵，分类器返回的预测标签.  
> labels : 列表，可选值. 当 average != binary 时被包含的标签集合，如果 average 是 None 的话还包含它们的顺序. 在数据中存在的标签可以被排除，比如计算一个忽略多数负类的多类平均值时，数据中没有出现的标签会导致宏平均值（marco average）含有 0 个组件. 对于多标签的目标，标签是列索引. 默认情况下，y_true 和 y_pred 中的所有标签按照排序后的顺序使用.  
> pos_label : 字符串或整型，默认为 1. 如果 average = binary 并且数据是二进制时需要被报告的类. 若果数据是多类的或者多标签的，这将被忽略；设置 labels=[pos_label] 和 average != binary 就只会报告设置的特定标签的分数.  
> average : 字符串，可选值为 [None, ‘binary’ (默认), ‘micro’, ‘macro’, ‘samples’, ‘weighted’]. 多类或 者多标签目标需要这个参数. 如果为 None，每个类别的分数将会返回. 否则，它决定了数据的平均值类型.  
> _‘binary’_: 仅报告由 pos_label 指定的类的结果. 这仅适用于目标（y_{true, pred}）是二进制的情况.  
> _‘micro’_: 通过计算总的真正性、假负性和假正性来全局计算指标.  
> _‘macro’_: 为每个标签计算指标，找到它们未加权的均值. 它不考虑标签数量不平衡的情况.  
> _‘weighted’_: 为每个标签计算指标，并通过各类占比找到它们的加权均值（每个标签的正例数）. 它解决了’macro’的标签不平衡问题；它可以产生不在精确率和召回率之间的 F-score.  
> ‘samples’: 为每个实例计算指标，找到它们的均值（只在多标签分类的时候有意义，并且和函数 accuracy_score 不同）.  
> sample_weight : 形状为 [样本数量] 的数组，可选参数. 样本权重.  
> _返回值_  
> _precision_: 浮点数 (如果 average 不是 None) 或浮点数数组, shape =[唯一标签的数量]  
> 二分类中正类的精确率或者在多分类任务中每个类的精确率的加权平均.

为了更加直白的讲解 sklearn 中 precision 的多个计算方式，现介绍下两个与多分类相关的概念，然后讲上面的代码是如何计算的。

*   Macro average

宏平均是指在计算均值时使每个类别具有相同的权重，最后结果是每个类别的指标的算术平均值。

*   Micro average

微平均是指计算多分类指标时赋予所有类别的每个样本相同的权重，将所有样本合在一起计算各个指标。

由 precision_score 的接口文档可知，当 average=None 时，得到的结果是是每个类别的 precision。上面的 y_true 有三个类别，分别是 0， 1， 2。每个类别的 TP, FP, FN 如下表所示：

![[d0861e8738e3f169d86a4bbaf99a5731_MD5.jpg]]

那么，可以得到每个类别的精确率了。

$P_{0}=\frac{2}{1+2}\approx0.667$P_{0}=\frac{2}{1+2}\approx0.667

$P_{1}=\frac{0}{0+2}\ =0$P_{1}=\frac{0}{0+2}\ =0

$P_{2}=\frac{0}{0+1}\ =0$P_{2}=\frac{0}{0+1}\ =0

则 Macro average 也可以得出，为 $(P_{0}+P_{1}+P_{2})/3\approx0.222$(P_{0}+P_{1}+P_{2})/3\approx0.222

Micro average 的计算需要从每个样本考虑，所有样本中预测正确的有两个，则 TP 为 2，剩下的 4 个预测结果都可以看成 FP，则 Micro Precision 等于 $2/(2+4)\approx0.333$2/(2+4)\approx0.333

average='weighted'考虑的是每一类与所有样本的占比，然后求加权平均值。因为这里每类的样本数都是 2，与总体样本的占比都是 1/3，故 average='weighted'的精确率为

$P_{w}=\frac{1}{3}*P_{0}+\frac{1}{3}*P_{1}+\frac{1}{3}*P_{2}\approx0.222$P_{w}=\frac{1}{3}*P_{0}+\frac{1}{3}*P_{1}+\frac{1}{3}*P_{2}\approx0.222

**对比下 Macro 和 Micro：**
*   如果每个类别的样本数量差不多，那么宏平均和微平均没有太大差异
*   如果每个类别的样本数量差异很大，那么注重样本量多的类时使用微平均，注重样本量少的类时使用宏平均
*   如果微平均大大低于宏平均，那么检查样本量多的类来确定指标表现差的原因
*   如果宏平均大大低于微平均，那么检查样本量少的类来确定指标表现差的原因

## 召回率 Recall

召回率（Recall）又被称为查全率，**表示预测结果为正样本中实际正样本数量占全样本中正样本的比例。**

计算公式为：

$Recall=\frac{TP}{TP+FN}$

使用场景：当正例被错误的预测为反例（FN）产生的代价很高时，适合用召回率。根据公式可知，召回率越高，FN 越小。比如说在银行的欺诈检测或医院的病患者检测中，如果将欺诈性交易（实际为正）预测为非欺诈性交易（预测为负），则可能会给银行带来非常严重的损失。再比如以最近的新冠疫情为例，如果一个患病者（实际为正）经过试剂检测被预测为没有患病（预测为负），这样的假反例或者说假阴性产生的风险就非常大。

sklearn 中 recall_score 方法和 precision_score 方法的参数说明都是一样的。所以这里不再重复，只是把函数和返回值说明贴在下面：

> 计算召回率  
> 召回率是比率 tp / (tp + fn)，其中 tp 是真正性的数量，fn 是假负性的数量. 召回率直观地说是分类器找到所有正样本的能力.  
> 召回率最好的值是 1，最差的值是 0.  
> 返回值  
> recall: 浮点数 (如果 average 不是 None) 或者浮点数数组，shape = [唯一标签的数量]  
> 二分类中正类的召回率或者多分类任务中每个类别召回率的加权平均值.

```
from sklearn.metrics import recall_score

y_true = [0, 1, 2, 0, 1, 2]
y_pred = [0, 2, 1, 0 ,0, 1]
print(recall_score(y_true, y_pred, average='macro'))  # 0.3333333333333333
print(recall_score(y_true, y_pred, average='micro'))  # 0.3333333333333333
print(recall_score(y_true, y_pred, average='weighted'))  # 0.3333333333333333
print(recall_score(y_true, y_pred, average=None))  # [1. 0. 0.]

```

Recall 和 Precision 只有计算公式不同，它们 average 参数为’macro’，‘micro’，'weighted’和 None 时的计算方式都是相同的，具体计算可以使用上节列出来的 TP、FP、FN 表，这里不再赘述。

## F1 score
F1 score 是**精确率和召回率的一个加权平均。**

F1 score 的计算公式如下：

$F_{1}=2*\frac{Precision*Recall}{Precision+Recall}$

**Precision 体现了模型对负样本的区分能力，Precision 越高，模型对负样本的区分能力越强；Recall 体现了模型对正样本的识别能力，Recall 越高，模型对正样本的识别能力越强。F1 score 是两者的综合，F1 score 越高，说明模型越稳健。**

sklearn 中 f1_score 方法和 precision_score 方法、recall_score 方法的参数说明都是一样的，所以这里不再重复，只是把函数和返回值说明贴在下面：

> 计算 F1 score，它也被叫做 F-score 或 F-measure.  
> F1 score 可以解释为精确率和召回率的加权平均值. F1 score 的最好值为 1，最差值为 0. 精确率和召回率对 F1 score 的相对贡献是相等的. F1 score 的计算公式为：  
> F1 = 2 * (precision * recall) / (precision + recall)  
> 在多类别或者多标签的情况下，这是权重取决于 average 参数的对于每个类别的 F1 score 的加权平均值.  
> 返回值  
> f1_score : 浮点数或者是浮点数数组，shape=[唯一标签的数量]  
> 二分类中的正类的 F1 score 或者是多分类任务中每个类别 F1 score 的加权平均.

下面来看下 sklearn 中计算 F1 的示例：

```
from sklearn.metrics import f1_score

y_true = [0, 1, 2, 0, 1, 2]
y_pred = [0, 2, 1, 0, 0, 1]
print(f1_score(y_true, y_pred, average='macro'))  # 0.26666666666666666
print(f1_score(y_true, y_pred, average='micro'))  # 0.3333333333333333
print(f1_score(y_true, y_pred, average='weighted'))  # 0.26666666666666666
print(f1_score(y_true, y_pred, average=None))  # [0.8 0.  0. ]

```

总结：当 FP 和 FN 造成的代价差不多的时候，可以直接用 Accuracy。但是当 FP 和 FN 产生的代价差别很大的时候，可以考虑更好的度量方式，比如 Precision, Recall 和 F1 score。

## micro avg、macro avg 和 weighted avg

  在机器学习和[深度学习](https://so.csdn.net/so/search?q=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&spm=1001.2101.3001.7020)模型评估结果中，我们经常会遇到 micro avg、macro avg 和 weighted avg。本文将会介绍这三种模型评估指标的计算方式以及它们之间的区别。  
  我们以 `sklearn.metrics.classification_report` 的输出结果作为展示，示例模型评估结果如下：  

![[c3d5e406ad8c5643cdcef5233224e324_MD5.png]]

  maro avg 的中文名称为 `宏平均`，其计算方式为每个类型的 P、R 的算术平均，我们以 F1 的 macro avg 为例，上述输出结果中的 macro avg F1 值的计算方式为：

```
macro avg F1 = (0.90+0.73)/2=0.815
```

  weighted avg 的计算方式与 micro avg 很相似，只不过 weighted avg 是用每一个类别样本数量在所有类别的样本总数的占比作为权重，因此 weighted avg 的计算方式为：

```
weighted avg = 0.90*1207/1756+0.73*549/1756=0.8468
```

  micro avg 的中文名称为 `微平均`，是对数据集中的每一个示例不分类别进行统计建立全局混淆矩阵，然后计算相应的指标。在微平均评估指标中，样本数多的类别主导着样本数少的类，其计算公式如下：  

![[e00c6080ab72ee090295b30de341c2c7_MD5.png]]

  
其中 TP 为被正确地划分为正例的个数，即实际为正例且被分类器划分为正例的样本数，FP 为被错误地划分为正例的个数，即实际为负例但被分类器划分为正例的样本数。  
  以下面的代码为例:

```
# -*- coding: utf-8 -*-
from sklearn.metrics import confusion_matrix, precision_score

y_true = ["A", "A", "A", "A", "B", "B", "C", "C", "C", "C", "C"]
y_pred = ["A", "B", "A", "A", "B", "A", "B", "C", "C", "C", "C"]
print(confusion_matrix(y_true, y_pred))
print(precision_score(y_true, y_pred, average='micro'))
```

输出的混淆矩阵和 micro avg precision 为：

```
[[3 1 0]
 [1 1 0]
 [0 1 4]]
0.7272727272727273
```

对于类别 A，它的 TP=3, FP=1；对于类别 B，它的 TP=1， FP=1；对于类别 C，它的 TP=4，FP=1，因此 micro avg precision 为：

```
(3+1+4)/(3+1+1+1+4+1)=0.7273
```

  本文的目的在于简单介绍模型评估指标 micro avg、macro avg 和 weighted avg 的区别以及它们的计算方式，网上有很多文章存在错误，应该引起重视。后面有机会再详细介绍。  
  本文到此结束，感谢阅读~