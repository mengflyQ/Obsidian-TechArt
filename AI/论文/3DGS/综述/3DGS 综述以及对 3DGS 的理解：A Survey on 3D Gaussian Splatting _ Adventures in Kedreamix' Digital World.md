今天想介绍的是`ZJU`带来的`3DGS`的首篇综述`A Survey on 3D Gaussian Splatting` 这是论文链接 [arXiv:2401.03890](https://arxiv.org/abs/2401.03890)，结合一些资料，趁这个机会好好学习一下 3DGS，加油入坑！！！

首先说一些自己的理解，3DGS 之所以爆火，很大程度在于他的实时性，而这一部分极大程度得益于他定制的算法与自定义 CUDA 内核。除此之外，**Gaussian Splatting** 根本不涉及任何神经网络，甚至没有一个小型的 MLP，也没有什么 “神经” 的东西，场景本质上只是空间中的一组点。在大家都在研究数十亿个参数组成的模型的人工智能世界里，这种方法越来越受欢迎，令人耳目一新。它的想法源于 “Surface splatting”（2001 年），说明经典的计算机视觉方法仍然可以激发相关的解决方案。它简单明了的表述方式使 **Gaussian Splatting** 特别容易解释，这也是为什么在某些应用中选择它而不是 NeRFs。

## [](#引言-INTRODUCTION "引言 INTRODUCTION")引言 INTRODUCTION

NeRF 自从 2020 年开始，在多视角合成中做出来巨大的贡献，他利用神经网络，实现了空间坐标到颜色和密度的映射的，然 NeRF 的方法是计算密集型的，通常需要大量的训练时间和大量的渲染资源，特别是高分辨率的输出。

[![[43d37514f960ad6601292f411006095c_MD5.png]]](https://pic1.zhimg.com/80/v2-c828848317a156fc6dd17c9a5310dd03.png)

NeRF

针对这些问题，3DGS 出现了，3DGS 采用显式表示和高度并行的工作流程，有利于更高效的计算和渲染，其创新在于其独特地融合了可微分管道和基于点的渲染技术的优点，通过用可学习的 3D 高斯函数表示场景，保留了连续体积辐射场的理想特性，这对于高质量图像合成至关重要，同时避免了与空白空间渲染相关的计算开销，这是传统 NeRF 方法的常见缺点，而 3DGS 很好的解决了这个问题，在不影响视觉质量的情况下达到了实时渲染。

论文中也发现，自 3DGS 出现以来，2023 年有很多的论文在 arXiv 中挂出来，所以基于此也写了这样一个综述，同时促进 3DGS 领域的进一步研究和创新

[![[91bc31fc619906289d72d0df4cfb48ec_MD5.png]]](https://picx.zhimg.com/80/v2-167cd8779af5c5550c15156e2b9b52c0.png)

The number of papers on 3DGS is increasing every month.

以下是论文架构的图，论文的大概架构如下所示，可以看到这篇综述撰写的一个逻辑，还是非常好的，接下来，我会顺着这个架构进行解读论文来学习

*   第 2 部分：主要是一些问题描述和相关研究领域的一些简要的背景
*   第 3 部分：介绍 3DGS，包括 3DGS 的多视角的合成和 3DGS 的优化
*   第 4 部分：3DGS 产生重大影响的各种应用领域和任务，展示了其多功能性
*   第 5 部分：对 3DGS 进行了一些比较和分析
*   第 6、7 部分：对一些未来的开放性工作进行总结和调查

[![[bdb850421aed372f43dc389648d0f4ff_MD5.png]]](https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png)

Structure of the overall review.

## [](#背景-BACKGROUND "背景 BACKGROUND")背景 BACKGROUND

背景主要分两部分讲解

*   辐射场的概念：隐式和显式
*   有关辐射场的场景重建、渲染等领域相关介绍

### [](#问题定义 "问题定义")问题定义

#### [](#辐射场 "辐射场")辐射场

辐射场是实际上是对三维空间中光分布的表示，它捕捉了光与环境中的表面和材质相互作用的方式。从数学上来说，辐射场可被描述为一个函数, 其中将点和球坐标下的方向映射为非负的辐射值。辐射场有显示表达和隐式表达，可用于场景表示和渲染。

#### [](#隐式辐射场 "隐式辐射场")隐式辐射场

隐式辐射场是辐射场中的一种，在表示场景中的光分布时，不需显式定义场景的集合形状。这里面最常见的就是 NeRF，使用神经网络来学习连续的体积表示。在 NeRF 中，使用 MLP 网络用于将一组空间坐标 和观察方向 映射到颜色和密度值。任何点处的辐射不是显式存储的，而是通过查询神经网络实时计算得出。因此，该函数可以写成：

这种方式的好处是构建了一个可微且紧凑的复杂场景，但是由于我们总是需要对光线进行采样和体渲染的计算，会导致计算负载比较高。

#### [](#显式辐射场 "显式辐射场")显式辐射场

与隐式不同的是，显示是直接表示光在离散空间结构中的分布，比如体素网格或点云。该结构中的每个元素都存储了其在空间中相应位置的辐射信息，而不是像 NeRF 一样去执行查询的操作，所以他会更直接也更快的得到每个值，但是同时也需要更大内存使用和导致较低的分辨率。通常我们可以表示为：

其中，`DataStructure`可以是网格或点云，而是一个根据观察视线方向修改辐射的函数。

#### [](#3D-Gaussian-Splatting-（两全其美） "3D Gaussian Splatting （两全其美）")3D Gaussian Splatting （两全其美）

3DGS 通过利用 3D 高斯函数作为其表示形式，充分利用了显示辐射场和隐式辐射场的优势。这些高斯函数被优化用于准确表示场景，结合了基于神经网络的优化和显式结构化数据存储的优点。这种混合方法能进行高质量渲染，同时具有更快的训练和实时性能，3D 高斯表达可表示为：

其中 是具有平均值 和协方差 的高斯函数， 表示与视图相关的颜色。

#### [](#显式与隐式的理解 "显式与隐式的理解")显式与隐式的理解

这里放一张理解显示隐式图像的图片，我还是觉得相当不错的

[![[7959f0fcec9741f651ed061c30e6942f_MD5.jpg]]](https://pic1.zhimg.com/80/v2-e79d0183806753d34863598e544a0517.jpeg)

显式隐式表达

### [](#背景和术语 "背景和术语")背景和术语

许多技术和研究学科与 `3DGS` 有着密切的关系，以下各节将对此进行简要介绍。

#### [](#场景重建与渲染 "场景重建与渲染")场景重建与渲染

**场景重建**：从一组图像集合或其它数据建立场景的三维模型。

**渲染**：将计算机可读取的信息（如场景中的 3D 物体）转化为图像。  
早期技术基于光场生成逼真的图像，运动结构（SfM）与多视图立体匹配（MVS）算法通过从图像序列估计 3D 结构来增强光场。

#### [](#神经渲染和辐射场 "神经渲染和辐射场")神经渲染和辐射场

**神经渲染**：将深度学习与传统图形技术结合生成逼真的图像。早期方法使用 CNN 估计混合权重或纹理空间解决方案。

**辐射场**：一种函数表达，描述从各方向穿过空间各点的光的量。NeRF 使用神经网络建模辐射场。

#### [](#体积表示和光线行进 "体积表示和光线行进")体积表示和光线行进

**体积表达**：不仅将物体和场景建模为表面，还将其其建模为充满材料或空白空间的体积。这样可以对如雾、烟或半透明材料进行更精确的渲染。

**光线行进**：是体积表达渲染图像的技术，通过增量跟踪穿过 “体” 的光线来渲染图像。NeRF 引入重要性采样和位置编码增强合成图像的质量，虽然能得到高质量的图像，但这一方法计算量大。

#### [](#基于点的渲染 "基于点的渲染")基于点的渲染

基于点的渲染是一种使用点而非传统多边形来可视化 3D 场景的技术。该方法特别适用于渲染复杂、非结构化或稀疏的几何数据。点可以通过添加额外属性，如可学习的神经描述符来进行增强，并且可以高效地进行渲染，但这种方法可能会出现渲染中的空洞或混叠效应等问题。3DGS 通过使用各向异性高斯进行更连贯的场景表达。

## [](#用于显式辐射场的3DGS "用于显式辐射场的3DGS")用于显式辐射场的 3DGS

3DGS 能够实时渲染高分辨率的图像，并且不需要神经网络，是一个突破。

这一块主要围绕两块进行讲解

*   3DGS 的前向过程
*   3DGS 的优化过程

### [](#学习3D高斯函数进行新视角合成 "学习3D高斯函数进行新视角合成")学习 3D 高斯函数进行新视角合成

假如现在有一个场景，目的是生成特定视角下的相机图像。NeRF 对每一个像素使用光线行进和采样点，影响其实时性；而 3DGS 将 3D 高斯投影到图像平面，称为 “泼溅”，如下图所示。然后对高斯进行排序并计算每个像素的值。NeRF 和 3DGS 的渲染可视为互逆关系。

[![[320a5f235d379139ed17467f2cbe645e_MD5.png]]](https://pic1.zhimg.com/80/v2-9d5fff5c2390526cd03e5a14fd13f4fe.png)

3DGS 的 Splatting 泼溅

这里面有个点很有意思，为什么说是互逆关系，我参考了知乎的一篇文章 [3D Gaussian Splatting 中的数学推导](https://zhuanlan.zhihu.com/p/666465701)的说明，我觉得这个说的还不错。

首先，我们回忆一下体渲染的这个事情。假设读者跟我一样是从 NeRF 才接触体渲染的，那么回顾一下 NeRF 中，沿着一个像素，发出一条射线，然后这条射线 “射向体数据”（在 NeRF 里就是沿着光线进行采样，然后查询采样点的属性）的过程。这个过程可以归结为一种`backward mapping`。

所以很自然的，会有一种`forward mapping`的办法。形式上，就是将整个 “体数据” 投影到此时位姿所对应的图像平面。这种办法的前提就不能是用 NeRF 那种隐式表达了，需要一些显式的表达才能支持这样直接的投影。例如以三个顶点长成的三角面基元（primitive），然后将这些许多的三角面直接投影到成像平面上，判断哪些像素是什么颜色，当有多个三角形投影时，根据他们的 “深度” 来判断前后顺序，然后进行熟悉的 alpha compositing。当然也会有其他基元，例如小的平面表示等等。

无论是`backward mapping`还是`forward mapping`，这个过程都涉及到将连续的表示变成离散的。在`backward mapping`里，是对场进行采样；在`forward mapping`里，是需要直接生成出基元，这也是一种连续化为离散。为了理解在这个过程中，高斯分布为什么重要，我们需要牵扯到信号与系统中的概念。与混过数字信号处理考试不同的是，我们要清楚此时引入信号与系统里的工具的目的是什么。回想刚才三角面基元的情景，在实际情境中，我们其实都接触不到 “连续” 的表达，比如三角面，我们只会记录它的三个顶点。当投影完成后，我们只能做一些有限的操作来阻止 “锯齿”，例如对结果进行一个模糊操作，这些操作一般都是局部的。我们这样做的目的，本质是“希望用离散的表达来重建原来的信号，进一步在重建好的信号上进行“resampling”。如果我们对处理后的结果，视觉上看起来没什么混叠或者锯齿上的问题，那就说明我们“resampling” 是成功的。

从下图也可以看到 NeRF 和 Gaussian 在概念上的区别，左边是 NeRF 沿着光线查询连续 MLP，右边是 Gaussian 一组与给定光线相关的离散的高斯分布

[![[14c35962ef014879686659e178e9290e_MD5.png]]](https://picx.zhimg.com/80/v2-08473faff1a084b3de92e2a86f69f0fd.png)

[![[842103fe7b2a0ddda9fa4256955813c5_MD5.png]]](https://picx.zhimg.com/80/v2-37166011e5e81d299598141028acff42.png)

difference between NeRF and Gaussian Splatting

首先简单介绍一下，3DGS 是如何表示真实场景的，前面也有提过，在 **Gaussian Splatting** 中，3D 世界用一组 3D 点表示，实际上是数百万个，大致在 0.5 到 5 百万之间。每个点是一个 3D 高斯，具有其独特的参数，这些参数是为每个场景拟合的，以便该场景的渲染与已知数据集图像紧密匹配，接下来就介绍他的属性。

[![[adf27afc852e0a55e847279e5100d1c1_MD5.png]]](https://pica.zhimg.com/80/v2-f440b37ac00a08977b2b6e5514ffec1f.png)

Representing a 3D world

*   **3D 高斯的属性**： 一个 3D 高斯主要包括，中心（位置）的均值、不透明度 、3D 协方差矩阵 和颜色 （一般是 RGB 或者是球谐（SH）系数）。 其中与视角有关， 由球谐函数表示。所有属性均可学习，都可以通过反向传播来学习和优化。
    
*   **视域剔除**：给定特定的相机姿态，该步骤会判断哪些高斯位于相机的视锥外，并在后续步骤中剔除之，以节省计算。
    
*   **Splatting 泼溅**：实际上只是 3D 高斯（椭圆体）投影到 2D 图像空间（椭圆）中进行渲染。给定视图变换 和 3D 协方差矩阵，我们可以使用使用以下公式计算投影 2D 协方差矩阵
    
    其中 为投影变换中仿射近似的雅可比矩阵。
    
*   **像素渲染**：如果不考虑并行，采用最简单的方式：给定像素 的位置，与其到所有重叠高斯函数的距离，即这些高斯函数的深度。这些可以通过观察变换 计算出来，形成高斯函数的排序列表。然后进行 alpha 混合，计算该像素的最终颜色：
    
    其中 是学习到的颜色，最终的不透明度 是学习的不透明度 与高斯的乘积:
    

其中 和 是投影空间中的坐标，同时我也找了个 gif 来可视化了一下 Gaussian Splatting 对位置 p 的影响：

![[f6309c7b9526457223fd000004a69e86_MD5.gif]]
3DGS

如果仔细看的话，我们会发现，实际上这个公式和[多变量正态分布的概率密度函数](https://en.wikipedia.org/wiki/Multivariate_normal_distribution)十分相像，是忽略了带有协方差行列式的标准化项，而是用不透明度来加权。

不过如果考虑并行的话加快速度，这种列表排序实际上很难并行化，所以很有可能这个渲染程度比 NeRF 还慢。为了实现实时渲染，3DGS 也做了一个 tradeoff，3DGS 做出了一些让步来适应**并行计算**。

[![[264b41b287615b1e14e53506d59394d7_MD5.png]]](https://picx.zhimg.com/80/v2-7cea6c4b183982cd921c0456d1f689b7.png)

Tiles(Patches)

*   **Tiles (Patches)**：为避免逐像素计算出现的成本，3DGS 改为 **patch** 级别的渲染。具体来说，首先将图像分割为多个不重叠的 patch，称为`tile`，每个图块包含 16×16 像素，如下图所示。3DGS 然后确定`tile`与投影高斯的相交情况，由于投影高斯可能会与多个`tile`相交，需要进行复制，并为每个复制体分配相关 tile 的标识符（如`tile`的 ID）。(不用判断每个像素与高斯的距离，而是判断 tile 就简单多了)
    
    [![[bb58a05f756d67ab7bb2aed96bd0baea_MD5.png]]](https://picx.zhimg.com/80/v2-c81242a6677621910801fcec4c0adbee.png)
    
    从下图可以看到排序的结果，在排序中，高位是 tile 的 ID，低位就是深度，一起进行排序，下面的图是 AI 葵视频的结果，还是很好理解的
    
    [![[6e88c1b21063210563092c4e7ec68914_MD5.png]]](https://pic1.zhimg.com/80/v2-5c74958d484c1d2588c20c8c30b58411.png)
    
    3DGS 排序
    
    [![[3c547e376865dbabc9c205438701d9d7_MD5.png]]](https://picx.zhimg.com/80/v2-3d6e3aec3a86c1d94354458830dbf17f.png)
    
    3DGS 排序例子 (AI 葵)
    
*   **并行渲染**：复制后，3DGS（对应字节的无序列表）结合包含了相关的 tile ID（对应字节的高位）和深度信息（对应字节的低位），如上图所示。由于每一块和每一像素的计算是独立的，所以可以基于 CUDA 编程的块和线程来实现并行计算，同时有利于访问公共共享内存并保持统一的读取顺序。排序后的列表可直接用于渲染（alpha 混合），如下图所示。
    
    [![[7b3c8cb945e0cf0e5a78d656afedf2e7_MD5.png]]](https://pic1.zhimg.com/80/v2-6393ea51f715d0d0baa880cd1890a549.png)
    
    并行渲染
    
    总的来说，3DGS 在前向过程中做出了一些近似计算，以提高计算效率并保留图像合成的高质量。
    

### [](#3DGS的优化 "3DGS的优化")3DGS 的优化

学习到这里，我们可能会有一个问题，怎么可能在空间中的一堆圆球中得到一个像样的图像的，确实是这样，如果没有进行优化，在渲染的时候就会出现很多伪影，从下图你可以看到。

[![[daf16572a7052d15c0f22caa3e1584bd_MD5.png]]](https://pic1.zhimg.com/80/v2-7ad69d962fb9a18d84747130af62fe15.png)

An example of renders of an under-optimized scene

3DGS 的核心是 **3D 高斯集合的优化过程**。一方面需要通过可微渲染来使高斯符合场景纹理，另一方面表达场景需要的高斯数量是未知的。这分别对应参数优化与密度控制两步，这两步在优化过程中交替进行。优化过程中，需要手动设置很多超参数。

#### [](#参数优化-Parameter-Optimization "参数优化 Parameter Optimization")参数优化 Parameter Optimization

*   **损失函数**：图像合成后，计算渲染图像与真实图像的差异作为损失：
    
    其中 是权重因子。与 NeRF 的损失函数略有不同，由于光线行进成本高昂，NeRF 通常在像素级别而不是图像级别进行计算，而 3DGS 是图像级别的。
    
*   **参数更新**：3D 高斯的多数参数可通过反向传播直接更新，但对于协方差矩阵 来说，需要半正定矩阵（这里面是一个定义，应该是多元正态分布的协方差矩阵是一个半正定矩阵），直接优化可能会产生非半正定矩阵，而只有半正定矩阵才有物理意义。因此，改为优化四元数和 3D 向量。将协方差矩阵分解：
    
    其中与分别由和推导得到的旋转和缩放矩阵。
    
    *   是一个对角缩放矩阵，含有 3 个参数
    *   是一个 3x3 的旋转矩阵，通过旋转四元数来表示
    
    对于不透明度, 其计算图较为复杂：。为避免自动微分的计算消耗，3DGS 还推导了与的梯度，在优化过程中直接计算之。
    

#### [](#密度控制-Density-Control "密度控制 Density Control")密度控制 Density Control

*   **初始化**：3DGS 建议从 SfM 产生的稀疏点云初始化或随机初始化高斯，可以直接调用 [COLMAP](https://colmap.github.io/) 库来完成这一步。。然后进行点的密集化和剪枝以控制 3D 高斯的密度。当由于某种原因无法获得点云时，可以使用随机初始化来代替，但可能会降低最终的重建质量。

[![[268b3bfd18dee331eb0714c20ba708e4_MD5.png]]](https://picx.zhimg.com/80/v2-0d67e5748993593a04ed46f7519e972e_720w.png)

A sparse 3D point cloud produced by SfM, means initialization

*   **点密集化**：在点密集化阶段，3DGS 自适应地增加高斯的密度，以更好地捕捉场景的细节。该过程特别关注缺失几何特征或高斯过于分散的区域。密集化在一定数量的迭代后执行，比如 100 个迭代，针对在视图空间中具有较大位置梯度（即超过特定阈值）的高斯。其包括在未充分重建的区域克隆小高斯或在过度重建的区域分裂大高斯。对于克隆，创建高斯的复制体并朝着位置梯度移动。对于分裂，用两个较小的高斯替换一个大高斯，按照特定因子减小它们的尺度。这一步旨在在 3D 空间中寻求高斯的最佳分布和表示，增强重建的整体质量。
    
    这一部分的意义是什么呢，因为 SGD 只能对现有点进行调整，但是在完全没有点或点太多的区域，很难找到好的参数，所以这就是点密集化的作用。
    
*   **点的剪枝**：点的剪枝阶段移除冗余或影响较小的高斯，可以在某种程度上看作是一种正则化过程。一般消除几乎是透明的高斯（α低于指定阈值）和在世界空间或视图空间中过大的高斯。此外，为防止输入相机附近的高斯密度不合理地增加，这些高斯会在固定次数的迭代后将设置为接近 0 的值。该步骤在保证高斯的精度和有效性的情况下，能节约计算资源。
    

[![[10d801a0c923416f1831394c37d24d56_MD5.png]]](https://picx.zhimg.com/80/v2-58c80507588563289c26e2ea4066ad81.png)

Adaptive Gaussian densification scheme.

### [](#用SH系数来表示颜色 "用SH系数来表示颜色")用 SH 系数来表示颜色

在计算机图形学中，用球谐函数（Spherical Harmonics，简称 SH）表示视角相关的颜色起着重要作用，最初是在 Plenoxels 中提出的。他能表示非兰伯特效应，比如金属表面的高光反射。不过这样也不是一定的，实际上也可以使用 3 个 RGB 值表示颜色，然后使用 Gaussian Splatting。

图形学全局环境光照技术与球谐函数息息相关，我们的环境光来源四面八方，可以理解为一个球面函数，当模拟漫反射环境光，我们用一张环境贴图进行采样，对每一个点进行半球采样出在这个像素上的颜色，**球谐光照**简单来说就是用几个系数存取了整张环境贴图包围在球上**法线方向**所对应的的颜色信息。在渲染过程中传入球谐系数。在模型上根据对应的法线信息，从球谐函数中获取对应的颜色信息。

球谐函数是定义在球面上的特殊函数，换句话说，可以对球面上的任意点计算这样一个函数并得到一个值。

这里我们简单理解一下，SH，球谐函数，归根到底只是一组基函数，至于这组基函数是怎么来的，不管他。简单点来说，每一个函数都可以由多个基函数组合起来，如果我们有很多基函数，我们可以通过对应的权重系数复原出原来的函数，不过本质上还是一个有损压缩，不一定那么准确，不过如果基函数越多，复原的函数越准确，但是计算量也变大了。

在球面基函数中，最多的就是球谐函数了。球谐函数有很多很好的性质，比如正交性，旋转不变性（这边就不介绍了）。正交性说明每个基函数都是独立的，每个基函数都不能用别的基函数加权得到。当 SH 的系数用的越多，那么表达能力就越强，跟原始的函数就越接近。（如果更详细的了解可以看看一些原理，我主要是宏观的了解 SH 是什么，简单理解就是他是一种颜色的表示）

[![[c7599237566a6164110af2e82dddd87e_MD5.png]]](https://pic1.zhimg.com/80/v2-9e660f32e92e1897aa986b0ab2ce073e.png)

当用来描述不同方向光照的 SH 基函数，我们一般用二阶或者三阶，比如下面的例子就是 3 阶的

[![[415ea98fc4daef7c3c1a9da41e1aabf9_MD5.png]]](https://picx.zhimg.com/80/v2-f6bfb715b846bf13c95013ca96c1d51d.png)

下面展示的是一个和 3 阶的球谐函数，一共包括 9 个学习系数，我们可以根据点的视角得到相关颜色，可以看到最后是 red 红色分量。

[![[f52037fb8baebc659116fa049b132cb3_MD5.png]]](https://pica.zhimg.com/80/v2-8241e4f7092a89a158df31b8cde94d33.png)

得到 l=2 和 9 个学习系数的点的视角相关颜色（红色分量）的过程

### [](#3DGS-流程 "3DGS 流程")3DGS 流程

最后根据论文的图来总结一下 3DGS 的流程

[![[240e2cca3626f9f0426feca044cdca75_MD5.png]]](https://pic1.zhimg.com/80/v2-fda180df51e9171e3e147f5b40e520b9.png)

3DGS 流程

1.  **Structure from Motion**：使用 SfM 从一组图像中估计出点云，可以直接调用 [COLMAP](https://colmap.github.io/) 库操作
    
    [![[accccedf9756f1d91c35266ffbd0626e_MD5.png]]](https://picx.zhimg.com/80/v2-961548f1a56fb5bc81bc8b349472d8ab.png)
    
    Structure from Motion
    

1.  **Convert to Gaussians**：将每个点建模成一个 3D 高斯图像。从 SfM 数据中，我们能推断出每个高斯图像的位置和颜色。但如果是要得到更高质量的表征的话，还需要对每个高斯函数进行训练，以推断出更精细的位置和颜色，并推断出协方差和透明度。
    
2.  **Training**：与神经网络类似，我们使用随机梯度下降法进行训练，但这里没有神经网络的层的概念 (都是 3D 高斯函数)。
    
    训练步骤如下:
    
    1.  用当前所有可微高斯函数渲染出图像
    2.  根据渲染图像和真实图像之间的差异计算损失
    3.  根据损失调整每个高斯图像的参数
    4.  根据情况对当前相关高斯图像进行点的密度控制
    
    步骤 1-3 比较简单，下面我们稍微解释一下第 4 步的工作:
    
    *   如果某高斯图像的梯度很大 (即它错得比较离谱)，则对其进行分裂或克隆
        *   如果该高斯图像很小，则克隆它
        *   如果该高斯图像很大，则将其分裂
    *   如果该高斯图像的 alpha 太低，则将其删除
    
    这么做能帮助高斯图像更好地拟合精细的细节，同时修剪掉不必要的高斯图像。
    
3.  **Differentiable Gaussian Rasterization**：3D Gaussian Splatting 实际上是一种光栅化的方法，将数据成像到屏幕上，与其他方法相比，他有两个特点
    
    1.  快
    2.  可微
    
    主要步骤如下：
    
    1.  针对给定相机视角，把每个 3D 高斯投影到 2D。
    2.  按深度对高斯进行排序。
    3.  对每个像素，从前到后计算每个高斯在该像素点的值，并将所有值混合以得到最终像素值。

### [](#3DGS-Limitations "3DGS Limitations")3DGS Limitations

**优点**

1.  高品质、逼真的场景
2.  快速、实时的渲染
3.  更快的训练速度

**缺点**

1.  防止模型优化中的 “破碎” 的高斯：点太大、太长、冗余等
2.  更高的显存使用率 (4GB 用于显示，12GB 用于训练)
3.  更大的磁盘占用 (每场景 1GB+)
4.  与现有渲染管线不兼容
5.  ~只能重建静态场景（但是好像现在动态的 Gaussian 也出来了，所以这个不算缺点了）~

## [](#应用领域和任务-APPLICATION-AREAS-AND-TASKS "应用领域和任务 APPLICATION AREAS AND TASKS")应用领域和任务 APPLICATION AREAS AND TASKS

### [](#同时定位和建图（SLAM） "同时定位和建图（SLAM）")同时定位和建图（SLAM）

SLAM 需要让设备实时理解自身位置并同时为环境建图，因此计算量大的表达技术难以应用。

传统 SLAM 使用点 / surfel 云或体素网格表达环境。3DGS 的优势在于高效性（自适应控制高斯密度）、精确性（各向异性高斯能建模环境细节）、适应性（能用于各种尺度和复杂度的环境）。

### [](#动态场景建模 "动态场景建模")动态场景建模

动态场景建模需要捕捉和表达场景随时间变化的的 3D 结构和外观。需要建立能精确反映场景中物体几何、运动和视觉方面的数字模型。4D 高斯泼溅通过扩展 3D 高斯溅射的概念，引入时间维度，使得可以表达和渲染动态场景。现在也有一些方法在研究在动态场景中的一些编辑的功能，与 3DGS 进行交互。

### [](#AI生成内容（AIGC） "AI生成内容（AIGC）")AI 生成内容（AIGC）

AIGC 是人工智能自动创建或极大修改的数字内容，可以模仿、扩展或增强人类生成的内容。

3DGS 的显式特性、实时渲染能力和可编辑水平使其与 AIGC 高度相关。例如，有方法使用 3DGS 与生成模型、化身或场景编辑结合，如 3DGS-Avatar。

### [](#自动驾驶 "自动驾驶")自动驾驶

自动驾驶的目标是在无人干涉的情况下导航并操作车辆，其主要目标是安全而高效地感知环境、做出决策和操作执行器。

其中，感知和理解环境需要实时重建驾驶场景，精确识别静态和动态物体，并理解其相互关系和运动。动态驾驶场景中，场景还会随时间连续变化。3DGS 可以通过混合数据点（如激光雷达点）将场景重建为连贯表达，有利于处理数据点变化的密度，以及静态背景和动态物体的精确重建。

## [](#性能比较-PERFORMANCE-COMPARISON "性能比较 PERFORMANCE COMPARISON")性能比较 PERFORMANCE COMPARISON

在这一部分，针对 3FGS 在上述的领域上的一些性能评估。

### [](#性能基准：定位 "性能基准：定位")性能基准：定位

*   数据集：Replica。
    
*   基准算法：Gaussian-SLAM、GS-SLAM、SplaTAM、GSS-SLAM。
    
*   评估指标：均方根误差（RMSE）、绝对轨迹误差（ATE），测量传感器运动轨迹上真实位置与估计位置欧式距离的均方根。
*   结果：基于 3D 高斯的 SLAM 方法能超过基于 NeRF 的密集视觉 SLAM。

[![[bed19f003bbf68a589451dfd275c6646_MD5.png]]](https://picx.zhimg.com/80/v2-3277500ac5a850accdd2891db0595ae6.png)

### [](#性能基准：静态场景渲染 "性能基准：静态场景渲染")性能基准：静态场景渲染

*   数据集：Replica。
*   基准算法：Gaussian-SLAM、GS-SLAM、SplaTAM、GSS-SLAM。
*   评估指标：峰值信噪比 (PSNR)、结构相似性 (SSIM)、学习的感知图像 patch 相似性 (LPIPS), 衡量 RGB 渲染性能。
*   结果：基于 3D 高斯的方法能超过基于 **NeRF** 的方法。

[![[bfabcbb75c2198d2f03ab715f1129e57_MD5.png]]](https://picx.zhimg.com/80/v2-94d0eea6ab0c03f32c82802f00a8102d.png)

### [](#性能基准：动态场景渲染 "性能基准：动态场景渲染")性能基准：动态场景渲染

*   数据集：D-NeRF。
*   基准算法：CoGS、4D-GS、GauFRe、4DGS。
*   评估指标：PSNR、SSIM、LPIPS, 用于衡量 RGB 渲染性能。
*   结果：3DGS 能大幅超过基于 NeRF 的 SOTA。但静态版本的 3DGS 对动态场景的重建是失败的。

[![[ded85ea29de519a60801a46e85e68296_MD5.png]]](https://picx.zhimg.com/80/v2-288ca353912cbbd221a111ee553ab607_720w.png)

### [](#性能基准：驾驶场景渲染 "性能基准：驾驶场景渲染")性能基准：驾驶场景渲染

*   数据集：nuScences。
*   基准算法：DrivingGaussian。
*   评估指标：PSNR、SSIM、LPIPS*（LPIPS× 1000）, 用于衡量 RGB 渲染性能。
*   结果：3DGS 方法能大幅超过基于 NeRF 的方法。

[![[6eea6b35a8aad50865659c42f00934c3_MD5.png]]](https://pic1.zhimg.com/80/v2-8b7bb910fbb86e3d1b23fc062260dc5d_720w.png)

### [](#性能基准：数字虚拟人 "性能基准：数字虚拟人")性能基准：数字虚拟人

该任务的目标是从给定的多视角视频渲染人体化身模型。

*   数据集：ZJU-MoCap。
*   基准算法：GART、Human101、HUGS、3DGS-Avatar。
*   评估指标：PSNR、SSIM、LPIPS* (LPIPS×1000) , 用于衡量 RGB 渲染性能
*   结果：基于 3DGS 的方法能在渲染质量和速度上均有优势。

[![[b174ff62b3de3608c38c125d6cc855c6_MD5.png]]](https://pica.zhimg.com/80/v2-d916a05d315e576e3cef738aa2306226.png)

## [](#未来研究方向-FUTURE-RESEARCH-DIRECTIONS "未来研究方向 FUTURE RESEARCH DIRECTIONS")未来研究方向 FUTURE RESEARCH DIRECTIONS

*   **数据高效的 3DGS 解决方案**：从少样本中进行新视图生成和场景重建很重要。目前的方法有探究引入深度信息、密集概率分布、像素到高斯的映射来促进该能力，实际上就是引入更多的信息。。此外，在观测不足的区域，3DGS 会产生伪影，可尝试在这些区域进行数据插值或积分。
*   **存储高效的 3DGS 解决方案**：3DGS 的可扩展性较差，在大尺度环境中需要大量的存储。需要优化训练阶段和模型的存储利用，而对于 NeRF 来说只需要存储学习到的 MLP 参数。可以探索更多高效的数据结构和先进的压缩技术，如 Light-Gaussian 等
*   **先进的渲染算法**：目前 3DGS 的渲染算法较为简单直接，可见性算法会导致高斯深度 / 混合顺序的剧烈切换，需要实施更先进的渲染算法，更好模拟光与材料属性的复杂相互作用。可结合传统计算机图形学的方法。此外，还可探索逆渲染。
*   **优化与正则化**： 各向异性高斯虽然有利于表示复杂几何体，但可能产生不希望的视觉伪影。例如，特别是在具有视角依赖外观的区域，大的 3D 高斯可能导致弹出伪影，突然出现或消失的视觉元素打破了沉浸感。使用正则化可以增加收敛速度，平滑视觉噪声或提高图像质量。此外，3DGS 中大量的超参数也会影响 3DGS 的泛化性。在 3DGS 的规则化和优化方面存在相当大的探索潜力。
*   **3D 高斯在网格重建中的应用**：可探索 3DGS 在网格重建中的潜力，从而缩小体积渲染和传统基于表面的方法的差距，以便提出新的渲染技巧和应用。
*   **赋予 3DGS 更多可能性**： 尽管 3DGS 具有显著潜力，但 3DGS 的全范围应用仍然未被充分挖掘。一个有前景的探索方向是用额外的属性增强 3D 高斯，例如为特定应用定制的语言和物理属性。此外，最近的研究开始揭示 3DGS 在多个领域的能力，例如相机姿态估计、捕捉手对象互动和不确定性量化。这些初步发现突出了跨学科学者进一步探索 3DGS 的重要机会。

## [](#参考文献-REFERENCES "参考文献 REFERENCES")参考文献 REFERENCES

1.  Kerbl, B., Kopanas, G., Leimkühler, T., & Drettakis, G. (2023). [3D Gaussian Splatting for Real-Time Radiance Field Rendering.](https://arxiv.org/abs/2308.04079) SIGGRAPH 2023.
2.  Mildenhall, B., Srinivasan, P. P., Tancik, M., Barron, J. T., Ramamoorthi, R., & Ng, R. (2020). [NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis.](https://arxiv.org/abs/2003.08934) ECCV 2020.
3.  Zwicker, M., Pfister, H., van Baar, J., & Gross, M. (2001). [Surface Splatting.](https://www.cs.umd.edu/~zwicker/publications/SurfaceSplatting-SIG01.pdf) SIGGRAPH 2001
4.  Luiten, J., Kopanas, G., Leibe, B., & Ramanan, D. (2023). [Dynamic 3D Gaussians: Tracking by Persistent Dynamic View Synthesis.](https://arxiv.org/abs/2308.09713) International Conference on 3D Vision.
5.  Zwicker, M., Pfister, H., van Baar, J., & Gross, M. (2001). [EWA Volume Splatting.](https://www.cs.umd.edu/~zwicker/publications/EWAVolumeSplatting-VIS01.pdf) IEEE Visualization 2001.
6.  Yu, A., Fridovich-Keil, S., Tancik, M., Chen, Q., Recht, B., & Kanazawa, A. (2023). [Plenoxels: Radiance Fields without Neural Networks.](https://arxiv.org/abs/2112.05131) CVPR 2022.
7.  [A Comprehensive Overview of Gaussian Splatting](https://towardsdatascience.com/a-comprehensive-overview-of-gaussian-splatting-e7d570081362)
8.  [Introduction to 3D Gaussian Splatting](https://github.com/huggingface/blog/blob/main/gaussian-splatting.md)
9.  [Sample Representation](https://docs.nerf.studio/nerfology/model_components/visualize_samples.html#d-frustum)
10.  [《3D Gaussian Splatting for Real-Time Radiance Field Rendering》3D 高斯的理论理解](https://zhuanlan.zhihu.com/p/664725693)
11.  [【论文笔记】A Survey on 3D Gaussian Splatting](https://blog.csdn.net/weixin_45657478/article/details/135603696)