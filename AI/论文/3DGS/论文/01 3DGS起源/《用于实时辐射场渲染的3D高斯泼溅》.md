---
title: 1 《用于实时辐射场渲染的3D高斯泼溅》
uid: "202404032200"
create_time: 2024-04-03 21:59
title translation: 
grade: 
author: 
date: 
DOI: 
url: 
banner: "[[Pasted image 20240403220617.png]]"
banner_x: 0.983
---

> [!abstract] 
最近，辐射场（Radiance Field）方法彻底改变了用多张照片或视频拍摄的场景的新视角合成。然而，要达到较高的视觉质量，仍然需要神经网络，而神经网络的训练和渲染成本很高，同时，最新的快速方法不可避免地要以速度换质量。对于无边界的完整场景（而不是孤立的物体）和 1080p 分辨率的渲染，目前没有一种方法能达到实时显示率。我们引入了三个关键要素，使我们能够实现最先进的视觉质量，同时保持有竞争力的训练时间，更重要的是，我们能够在 1080p 分辨率下实现高质量的实时（ ≥30 fps）新视角合成。
>
首先，从摄像机校准过程中产生的稀疏点开始，我们用3D高斯（3D Gaussians）来表示场景，这种3D高斯保留了用于场景优化的连续体积辐射场的理想特性，同时避免了在空白空间（empty space）进行不必要的计算；其次，我们对3D高斯进行交错优化/密度控制，特别是优化各向异性协方差，以实现对场景的精确表示；第三，我们开发了一种快速可见性感知渲染算法，该算法支持各向异性泼溅（splatting），既能加速训练，又能实现实时渲染。我们在几个已建立的数据集上展示了最先进的视觉质量和实时渲染。
> 
>**Keywords:** novel view synthesis, radiance fields, 3D gaussians, real-time rendering  
>**关键词：** 新视图合成、辐射场、3D高斯、实时渲染


# 1 引言
Mesh 和点云是最常见的三维场景表示法，因为它们是显式的，非常适合基于 GPU/CUDA 的快速光栅化。相比之下，最新的神经辐照场（NeRF）方法建立在连续场景表示法的基础上，通常使用体积光线步进法优化多层感知器（MLP），对捕捉到的场景进行新视角合成。

同样，迄今为止最高效的辐射场解决方案也是建立在连续表示法的基础上，对存储在体素、哈希 grid 或点中的值进行插值。虽然这些方法的连续性有助于优化，但渲染所需的随机采样成本很高，而且会产生噪声。**我们引入了一种新方法，它结合了这两种方法的优点：我们的3D高斯表示法允许以最先进的（SOTA）视觉质量和有竞争力的训练时间进行优化，而我们基于 `tile` 的泼溅（splatting）解决方案确保了在之前发布的几个数据集（见图 1）上以 SOTA 质量进行 1080p 分辨率的实时渲染。**

![[Pasted image 20240403220617.png]] 
>图 1. 我们的方法实现了辐射场的实时渲染，其质量等同于之前质量最好的方法 (Barron et al., [2022](#bib.bib4))，而所需的优化时间与之前最快的方法 (Fridovich-Keil and Yu et al., [2022](#bib.bib14); Müller et al., [2022](#bib.bib33))相当。实现这一性能的关键在于新颖的3D 高斯场景表示法与实时可微分渲染器的结合，这大大加快了场景优化和新颖视图合成的速度。请注意，在训练时间与 InstantNGP 相当的情况下，我们获得了与他们相似的质量；虽然这是他们达到的最高质量，但通过 51 分钟的训练，我们获得了最先进的质量，甚至略优于 Mip-NeRF360。

**我们的目标是对多张照片拍摄的场景进行实时渲染，并在典型真实场景中以与以往最高效方法同样快的优化速度创建表征（representation，表示）。** 最近的方法实现了快速训练，但难以达到当前 SOTA NeRF 方法（即 Mip-NeRF360 ）的视觉质量，后者需要长达 48 小时的训练时间。快速但质量较低的辐射场方法可以根据场景实现交互式渲染（每秒 10-15 帧），但无法实现高分辨率下的实时渲染。

**我们的解决方案由三个主要部分组成。**
1. **首先，我们引入 3D 高斯作为一种灵活而富有表现力的场景表示。** 我们从与之前的 NeRF 类似的方法相同的输入开始，即使用运动结构（SfM）校准的摄像机，并使用作为 SfM 过程一部分免费生成的稀疏点云初始化 3D 高斯集。与大多数需要多视图立体（MVS）数据的基于点的解决方案相比，我们只需将 SfM 点作为输入即可获得高质量的结果。
   需要注意的是，对于 NeRF-synthetic 数据集，即使采用随机初始化，我们的方法也能达到很高的质量。我们的研究表明，3D高斯是一个很好的选择，因为它们是一种可微分的体积表示法，但也可以通过将它们投影到2D，并应用标准的 Alpha 混合法，使用与 NeRF 相当的图像形成模型，非常高效地对它们进行光栅化处理。
2. **我们方法的第二个组成部分是优化3D 高斯的属性**--三维位置、不透明度 $、\alpha$ 、各向异性协方差和球谐系数等属性，并与自适应密度控制步骤交错进行，在优化过程中添加或偶尔删除3D 高斯。通过优化程序，可以得到一个相当紧凑、非结构化（比如点云，点与点之间是断开的，称为非结构化。mesh 是不间断的，称为结构化）和精确的场景表示（所有测试场景的高斯数均为 100-500 万）。
3. 我们方法的第三个也是最后一个要素是我们的**实时渲染解决方案**，它使用了快速 GPU 排序算法，并受到基于 tile 的光栅化的启发，沿用了最近的工作成果(Lassner and Zollhofer, [2021](#bib.bib28))。不过，得益于我们的3D高斯表示法，我们可以执行各向异性泼溅（splatting），可见性排序（得益于排序和 Alpha 混合），并实现快速渲染。并根据需要跟踪遍历尽可能多的排序泼溅（splatting），从而实现快速、准确的后向传递。

**总之，我们做出了以下贡献：**
- 引入各向异性3D高斯作为辐射场的高质量、非结构化表示。
- 一种3D高斯特性优化方法，与自适应密度控制交错使用，可为捕捉到的场景创建高质量的表现形式。
- 针对 GPU 的快速可微分渲染方法具有可视性感知功能，可通过各向异性泼溅（splatting）和快速反向传播实现高质量的新视图合成。

我们在之前发布的数据集上获得的结果表明，我们可以通过多视角捕捉优化3D高斯，并获得与之前最佳隐式辐射场方法相同或更好的质量。我们还能达到与最快方法类似的训练速度和质量，更重要的是，我们首次为新视角合成提供了高质量的实时渲染。

# 2 相关工作
我们首先简要介绍了传统重建，然后讨论了基于点的渲染和辐射场工作，并讨论了它们的相似性；辐射场是一个庞大的领域，因此我们只关注直接相关的工作。
>如需了解该领域的全部内容，请参阅近期出色的调查报告 (Tewari et al., [2022](#bib.bib50); Xie et al., [2022](#bib.bib55)).

## 2.1 传统场景重建和渲染
最早的新颖视图合成方法基于光场，首先是密集采样（Gortler 等人，1996 年；Levoy 和 Hanrahan，1996 年），然后是非结构化捕捉（Buehler 等人，2001 年）。从运动看结构（SfM）（Snavely 等人，2006 年）的出现开创了一个全新的领域，可以利用照片集合合成新颖的视图。SfM 在相机校准过程中估算出稀疏的点云，最初用于简单的三维空间可视化。

随后的多视角立体（MVS）多年来产生了令人印象深刻的全三维重建算法（Goesele 等人，2007 年），使得多种视角合成算法得以发展（Eisemann 等人，2008 年；Chaurasia 等人，2013 年；Hedman 等人，2018 年；Kopanas 等人，2021 年）。所有这些方法都是将输入图像重新投影并混合到新视角相机中，并使用几何图形来指导这种重新投影。

这些方法在许多情况下都能产生出色的结果，但通常无法完全恢复未重建区域，或当 MVS 生成不存在的几何图形时无法完全恢复 "过度重建"。最新的神经渲染算法（Tewari 等人，2022 年）大大减少了此类伪影，并避免了在 GPU 上存储所有输入图像的巨大成本，在大多数方面都优于这些方法。

## 2.2 神经渲染和辐射场

深度学习技术很早就被用于新视图合成；CNN 被用于估计混合权重或纹理空间解决方案。使用基于 MVS 的几何图形是大多数这些方法的主要缺点；此外，使用 CNN 进行最终渲染经常会导致时间闪烁。

用于新视图合成的体积表示由 Soft3D 首创；随后提出了与体积光线行进相结合的深度学习技术，该技术基于连续可变密度场来表示几何形状。由于查询体积需要大量样本，因此使用体积光线行进法进行渲染的成本很高。神经辐射场（NeRFs）引入了重要性采样和位置编码来提高质量，但使用了大型多层感知器，对速度产生了负面影响。NeRF 的成功催生了大量解决质量和速度问题的后续方法，这些方法通常通过引入正则化策略来实现；目前在新视图合成图像质量方面最先进的方法是 Mip-NeRF360。虽然渲染质量非常出色，但训练和渲染时间仍然极长；我们能够在提供快速训练和实时渲染的同时，达到甚至超过这一质量。

最新的方法主要通过以下三种设计选择来加快训练和/或渲染速度：使用空间数据结构来存储（神经）特征，这些特征随后会在体积射线行进过程中进行插值；不同的编码方式；以及 MLP 的容量。这些方法包括空间离散化的不同变体、编码本以及哈希表等编码方式，从而可以使用较小的 MLP 或完全放弃神经网络。

这些方法中最著名的是 InstantNGP，它使用哈希网格和占位网格来加速计算，并使用较小的 MLP 来表示密度和外观；还有 Plenoxels，它使用稀疏体素网格来插值连续密度场，并能完全放弃神经网络。**这两种方法都依赖于球谐函数 ：前者直接表示方向效果，后者将其输入编码到色彩网络**。虽然这两种方法都能提供出色的效果，但这些方法仍难以有效地表现空白空间，部分原因取决于场景/拍摄类型。此外，图像质量在很大程度上受限于用于加速的结构网格的选择，而渲染速度则受限于在给定的光线行进步骤中需要查询许多样本。

**我们使用的非结构化、显式 GPU 友好型 3D 高斯，无需神经组件即可实现更快的渲染速度和更好的质量。****

## 2.3 基于点的渲染和辐射场
**基于点的方法可有效渲染断开的非结构化几何样本（即点云）**。在最简单的形式中，点样本渲染光栅化了一组具有固定大小的非结构化点，为此，它可以利用图形 API 本地支持的点类型或 GPU 上的并行软件光栅化。点采样渲染虽然忠实于底层数据，但却存在漏洞，会造成混叠，而且严格来说是不连续的。基于点的高质量渲染的开创性工作通过 "泼溅（splatting） "范围大于像素的点基元来解决这些问题，例如圆形或椭圆形圆盘、椭圆体或曲面。

最近，人们对基于可变点的渲染技术产生了兴趣。通过神经特征对点进行增强，并使用 CNN 进行渲染，可实现快速甚至实时的视图合成；不过，这些技术的初始几何图形仍依赖于 MVS，因此继承了 MVS 的缺陷，最明显的是在无特征/有光泽区域或薄结构等困难情况下过度或不足重建。

**基于点的 alpha 混合和 NeRF 风格的体积渲染在本质上共享相同的图像形成模型**。具体来说，颜色 $C$ 是通过沿射线的体积渲染得到的。
$$
C=\sum_{i=1}^{N}T_{i}(1-\exp(-\sigma_{i}\delta_{i}))\mathbf{c}_{i}\quad其中\quad T_{i}=\exp\left(-\sum_{j=1}^{i-1}\sigma_{j}\delta_{j}\right)\tag{1}  
$$
其中密度 $\sigma$、透射率 $T$ 和颜色 $c$ 的样本是沿着光线以间隔 $\delta_i$ 采集的。这可以重写为
$$
C=\sum_{i=1}^{N}T_{i}\alpha_{i}\mathbf{c}_{i}\tag{2}
$$
其中 $\displaystyle\alpha_{i}=(1-\exp(-\sigma_{i}\delta_{i}))\quad T_{i}=\prod_{j=1}^{\iota-1}(1-\alpha_{i})$

典型的基于神经点的方法 (e.g., [Kopanas et al. 2022, 2021])通过混合像素上重叠的 𝒩 有序点来计算像素的颜色 $C$。
$$
C=\sum_{i\in\mathcal{N}}c_{i}\alpha_{i}\prod_{j=1}^{i-1}(1-\alpha_{j})\tag{3}
$$
其中，$c_i$ 是每个点的颜色，$\alpha_i$ 是通过对具有协方差 $\Sigma$ 的二维高斯乘以学习到的每个点的不透明度求得的。

从（2）（3）的公式中，我们可以清楚地看到图像形成模型是相同的。但是，渲染算法却截然不同：
- NeRF 是一种隐含地表示空/占空间的连续表示法；采样需要进行昂贵的随机采样，因此会产生噪声和计算费用。
- 相比之下，点是一种非结构化的离散表示法，具有足够的灵活性，可以创建、破坏和位移与 NeRF 类似的几何体。正如之前的工作所示，这是通过优化不透明度和位置实现的，同时避免了全体积表示法的缺点。

Pulsar 实现了快速球形光栅化，这给我们基于 tile 和排序的渲染器带来了启发。然而，根据上述分析，**我们希望在排序的 splats 上保持（近似）传统的 alpha 混合，以获得体积渲染的优势：我们的光栅化遵循可见性顺序，这与他们不依赖顺序的方法截然不同。**

此外，我们对像素中的所有 splats 进行梯度反向传播，并对各向异性 splats 进行光栅化处理。这些元素都有助于提高我们结果的视觉质量（见第 7.3 节）。此外，上文提到的其他方法也使用 CNN 进行渲染，这会导致时间上的不稳定性。然而，Pulsar 和 ADOP 的渲染速度成为我们开发快速渲染解决方案的动力。

虽然基于点的漫反射神经点渲染专注于镜面效果，通过使用 MLP 克服了这种时间不稳定性，但仍需要 MVS 几何图形作为输入。该类别中最新的方法不需要 MVS，也使用 SH 指示方向；但它只能处理一个物体的场景，并且需要遮罩进行初始化。虽然对于小分辨率和低点计数来说速度很快，但目前还不清楚它如何能扩展到典型数据集的场景。

**我们使用3D高斯进行更灵活的场景表示，避免了对 MVS 几何图形的需求，并通过基于 tile 的投影高斯渲染算法实现了实时渲染。**

最近的一种方法采用径向基函数方法用点来表示辐射场。他们在优化过程中采用了点剪枝和致密化技术，但使用的是体积射线行进法，无法实现实时显示率。

在人体表现捕捉领域，3D高斯被用于表示捕捉到的人体；最近，它们又被用于视觉任务的体积光线行进。神经体积基元也在类似情况下被提出。虽然这些方法启发了我们选择3D高斯作为场景表示，但它们侧重于重建和渲染单个孤立物体（人体或面部）的特定情况，导致场景的深度复杂度较小。

**相比之下，我们对各向异性协方差的优化、交错优化/密度控制，以及高效的深度排序渲染，使我们能够处理包括背景在内的完整复杂场景，无论是室内还是室外，深度复杂度都很高。**

#  3 概述

我们方法的输入是一组静态场景图像，以及通过 SfM 校准的相应摄像机，该校准会产生稀疏点云作为副作用。根据这些点，我们创建了一组3D高斯（第 4 章），由位置（均值）、协方差矩阵和不透明度 $\alpha$ 定义，从而实现非常灵活的优化机制。这使得三维场景的表示相当紧凑，部分原因是较高各向异性的体积 splats 可以用来紧凑地表示精细结构。

按照标准做法，辐射场的方向外观成分（颜色）通过球谐系数表示。我们的算法通过对3D 高斯参数（即位置、协方差、 $\alpha$ 和 SH 系数）的一系列优化步骤，并交错进行高斯密度的自适应控制操作，来创建辐射场表示（第 5 节）。我们的方法之所以高效，关键在于我们基于 tile 的光栅化器（第 6 章），它允许各向异性 splats 的 $\alpha$ 混合，并通过快速排序遵守可见性顺序。快速栅格化器还包括通过跟踪累积的 $\alpha$ 值进行快速后向传递，对可接收梯度的高斯数量没有限制。图 2 展示了我们的方法概览。

![[Pasted image 20240404102548.png]]
>图 2. 优化从稀疏的 SfM 点云开始，创建一组3D高斯。然后，我们对这组高斯的密度进行优化和自适应控制。在优化过程中，我们使用基于 tile 的快速渲染器，与 SOTA 快速辐射场方法相比，训练时间更短。训练完成后，我们的渲染器就可以对各种场景进行实时导航。

# 4 可微分的3D GS
**我们的目标是从无法线的稀疏 (SfM) 点集合出发，优化场景表示，从而实现高质量的新视角合成。** 为此，我们需要一种基元，它继承了可微分体积表示法的特性，同时又是非结构化的、明确的，可以进行非常快速的渲染。我们选择3D 高斯（3D Gaussians），因为它是可微分的，可以很容易地投射到2D tile上，从而实现快速 $alpha$ 混合渲染。

我们的表示方法与之前使用2D点的方法有相似之处，都假定每个点都是具有法线的小平面圆。鉴于 SfM 点的极度稀疏性，很难估算法线。同样，从这样的估计中优化噪声非常大的法线也非常具有挑战性。**相反，我们将几何建模为一组无需法线的3D高斯**。
**我们的高斯是由世界空间中以点（均值）$\mu$ 为中心的全三维协方差矩阵 $\Sigma$ 定义的：**
$$
G(x)~=e^{-\frac{1}{2}(x)^{T}\Sigma^{-1}(x)}
\tag{4}$$ 
在混合过程中，该高斯乘以 $\alpha$ 。

不过，我们需要将3D高斯投影到2D空间以进行渲染。给定观察变换 $W$ 后，摄像机坐标中的协方差矩阵 $\Sigma'$ 如下所示：
$$\Sigma^{\prime}=JW\Sigma W^\top J^\top \tag{5}$$
其中，  $J$ 是投影变换仿射近似的雅可比矩阵。Zwicker 等人 ( 2001a) 的研究也表明，如果跳过 Σ′ 的第三行和第三列，就可以得到 2 × 的结果。就会得到一个 2 ×2 方差矩阵，其结构和性质与以前的工作（Kopanas 等人，2021 年）中从有法线的平面点出发的矩阵相同。

一个显而易见的方法是直接优化协方差矩阵 Σ ，以获得代表辐射场的3D 高斯。然而，协方差矩阵只有在半正定（positive semi-definite）才具有物理意义。**在对所有参数进行优化时，我们使用梯度下降法，但这种方法不容易产生有效矩阵，而且更新步骤和梯度很容易产生无效协方差矩阵。因此，我们选择了一种更直观、但具有同等表达能力的优化表示方法**。3D 高斯的协方差矩阵 $\Sigma$ 类似于描述椭圆体的构型。给定缩放矩阵 $S$ 和旋转矩阵 $R$ ，我们就可以找到相应的 $\Sigma$ ：
$$\Sigma=RSS^\top R^\top \tag{6}$$
为了对这两个因素进行独立优化，我们将它们分开存储：三维向量 $s$ 用于缩放，四元数 $q$ 表示旋转。我们可以简单地将它们转换为各自的矩阵并进行组合，同时确保对 $q$ 进行归一化处理，以获得有效的单位四元数。

**为了避免在训练过程中自动微分带来的巨大开销，我们明确推导出了所有参数的梯度。** 精确导数计算的详情见附录 A。这种适合优化的各向异性协方差表示法，使我们能够对3D高斯进行优化，以适应拍摄场景中不同形状的几何图形，从而获得相当紧凑的表示法。图 3 展示了这种情况。

![[Pasted image 20240404105713.png]]
>图 3.我们将优化后的3D高斯缩小 60%（最右侧）。这清楚地显示了3D高斯的各向异性形状，优化后的3D高斯能紧凑地表现复杂的几何图形。左图为实际渲染图像。

# 5 高斯的自适应密度控制优化
我们方法的核心是优化步骤，它创建了一组密集的3D高斯，准确地代表了用于自由视角合成的场景。**除了位置 $p$ , 透明度 $\alpha$ 和协方差 $\Sigma$ 之外，我们还优化了代表每个高斯的颜色 $c$ 的 SH 系数，以正确捕捉与视角相关的场景外观。这些参数的优化与控制高斯密度的步骤交错进行，以更好地表现场景。**
## 5.1 优化
优化基于连续的迭代渲染，并将生成的图像与捕获数据集中的训练视图进行比较。由于三维到2D投影的不确定性，几何体的位置难免会不正确。因此，我们的优化需要能够创建几何图形，并在几何图形被错误定位时将其销毁或移动。3D高斯协方差参数的质量对表示的紧凑性至关重要，因为只需少量各向异性的大高斯就能捕捉到大面积的同质区域。

**我们采用随机梯度下降技术进行优化，充分利用标准的 GPU 加速框架，并根据最近的最佳实践(Fridovich-Keil and Yu et al., [2022](#bib.bib14); Sun et al., [2022](#bib.bib47))，为某些操作添加定制的 CUDA 内核**。特别是，我们的快速光栅化（见第 6 章）对我们的优化效率至关重要，因为它是优化的主要计算瓶颈。

我们对 $\alpha$ 使用了 sigmoid 激活函数，以将其限制在 $[0,1)$ 范围内，从而获得平滑的梯度。

我们将初始协方差矩阵估计为各向同性高斯矩阵，其轴等于最近三点距离的平均值。

**我们使用了与 Plenoxels 类似的标准指数衰减调度技术，但只针对位置**。**损失函数为 $\mathcal{L}_1$ 结合 $D-SSIM$ 项：**
$$\mathcal{L}=(1-\lambda)\mathcal{L}_1+\lambda\mathcal{L}_{D-SSIM} \tag{7}$$

们在所有测试中都使用了 $\lambda=0.2$ 。我们将在第 7.1 节中详细介绍学习计划和其他要素。

## 5.2.高斯的自适应控制
我们从 SfM 的初始稀疏点集合开始，然后应用我们的方法自适应地控制单位体积内的高斯数量及其密度，使我们能够从初始的稀疏高斯集到更密集的高斯集，从而更好地代表场景，并获得正确的参数。**在优化预热（ warm-up，见第 7.1 节）之后，我们每迭代 100 次就会进行一次高斯密集化处理，并删除任何基本透明的高斯，即 $\alpha$ 小于阈值 $\epsilon_{\alpha}$ 的高斯。**

**我们对高斯的自适应控制需要填充空白区域。** 它不仅关注几何特征缺失的区域（"重建不足"），也关注高斯覆盖场景大面积的区域（通常对应 "重建过度"）。我们观察到，两者都有较大的视空间位置梯度。直观地说，这可能是因为它们对应的区域还没有得到很好的重建，而优化试图移动高斯来纠正这一点。**由于这两种情况都适合进行高斯密集化处理，因此我们会对视图空间位置梯度平均值超过阈值 $\tau_{pos}$ 的高斯进行密集化处理**。我们在测试中将其设置为 0.0002 。

接下来，我们将详细介绍这一过程，如图 4 所示。
![[Pasted image 20240404110957.png]]
>图 4.我们的自适应高斯密集化方案。
>第一行（重建不足）：当小尺度几何图形（黑色轮廓）覆盖不足时，我们会克隆相应的高斯。
>第二行（重建过度）：如果小尺度几何体由一个大的 splats 块表示，我们就将其一分为二。

对于处于未完全重建区域的小高斯，我们需要覆盖必须创建的新几何体。为此，我们最好克隆高斯，只需创建一个大小相同的副本，并沿位置梯度方向移动即可。
另一方面，高方差区域中的大高斯需要分割成更小的高斯。我们用两个新的高斯来替换这些高斯，并用实验确定的 $\phi =1.6$ 因子来划分它们的比例。我们还使用原始3D高斯作为采样的 PDF 来初始化它们的位置。

在第一种情况下，我们会检测并处理增加系统总体积和高斯数量的需求；而在第二种情况下，我们会保留总体积，但增加高斯数量。**与其他体积表示法类似，我们的优化可能会陷入浮点靠近输入摄像头的情况；在我们的案例中，这可能会导致高斯密度的不合理增加。减缓高斯数量增加的有效方法是每 $N=3000$ 迭代一次，将 $\alpha$ 值设为接近零。** 优化后，高斯的 $\alpha$ 值会增加，同时我们的剔除方法可以去除 $\alpha$ 小于  $\epsilon_{\alpha}$ 的高斯，如上所述。**高斯可能会缩小或增大，也可能与其他高斯严重重叠，但我们会定期删除世界空间中非常大的高斯和视图空间中足迹较大的高斯。**

**这种策略可以很好地控制高斯总数。与其他方法不同的是，我们的模型中的高斯始终保持欧几里得空间中的基元；对于遥远或大型高斯，我们不需要空间压缩、扭曲或投影策略。**

# 6 高斯的快速微分光栅器

我们的目标是实现快速的整体渲染和快速排序，以允许近似的 $\alpha$ 混合（包括各向异性的 splats ），并避免之前工作（Lassner 和 Zollhofer，2021 年）中存在的对可接收梯度的splats数量的硬性限制。 

为了实现这些目标，我们设计了一种**基于 tile 的高斯泼溅光栅化器**，其灵感来自于最近的软件光栅化方法(Lassner and Zollhofer, [2021](#bib.bib28))，**可以一次对整个图像的基元进行预排序，避免了以往 alpha 混合解决方案所遇到的按像素排序的问题**。我们的快速光栅化器**可以对任意数量的混合高斯进行高效的反向传播，额外内存消耗低，每个像素只需要一个恒定的开销**。我们的光栅化流水线是**完全可微分**的，**在投影到2D（第 4 章）的情况下，可以对各向异性的 splats 进行光栅化**，类似于之前的2D 泼溅方法（Kopanas 等人，2021 年）。

我们的方法首先**将屏幕分割成 16 × 16 个 tile** ，然后根据视锥和每个 tile 剔除（cull）3D高斯。具体来说，我们只保留置信区间为 99% 且与视锥相交的高斯。此外，我们还使用一个保护带 (guard band) 来剔除极端位置的高斯（即均值接近近平面但远离视锥的高斯），因为计算它们的投影2D协方差将是不稳定的。然后，我们根据每个高斯重叠的 tile 数量对其进行实例化，并为每个实例分配一个结合了观察空间深度和 `tile ID` 的键。

然后，我们使用**单个快速 GPU Radix 排序** (Merrill and Grimshaw, [2010](#bib.bib31)) 根据这些键对高斯进行排序。需要注意的是，没有额外的按像素排序的点，混合是基于这种初始排序执行的。因此，我们的 $\alpha$ 混合在某些配置中可能是近似的。不过，当 splats 的大小接近单个像素时，这些近似值就可以忽略不计了。我们发现，这种选择大大提高了训练和渲染性能，而不会在聚合场景中产生明显的人工痕迹。

After sorting Gaussians, we produce a list for each tile by identifying the first and last depth-sorted entry that splats to a given tile. For rasterization, we launch one thread block for each tile. 
在对高斯进行排序后，我们会通过识别第一个和最后一个深度排序的条目来为每个 tile 生成一个列表，这些条目会泼溅到给定的 tile 上。在光栅化过程中，我们为每个 tile 启动一个线程块（thread block）。
对高斯进行排序后，我们通过识别第一个和最后一个映射到给定图块的深度排序条目来为每个图块生成一个列表
在对高斯进行排序后，我们为每个瓦片生成一个列表，方法是找出第一个和最后一个深度排序的条目，并将其拼接到给定的瓦片上

每个 block 首先协同将高斯数据包加载到共享内存中，然后针对给定像素，通过前后遍历列表来累积颜色和 $\alpha$ 值，从而最大限度地提高数据加载/共享和处理的并行性。当某一像素达到目标饱和度 $\alpha$ 时，相应的线程就会停止。每隔一定时间，我们会对一个 tile 中的线程进行查询，当所有像素都达到饱和（即 $alpha$ 变为 1）时，整个 tile 的处理就会终止。有关排序的详细信息和整个光栅化方法的高级概述见附录 C。

**在光栅化过程中， $alpha$ 的饱和度是唯一的停止标准**。与之前的工作不同，我们不限制接受梯度更新的混合基元的数量。

我们强制执行这一特性，是为了让我们的方法能够处理具有任意不同深度复杂性的场景，并对其进行精确学习，而无需诉诸特定场景的超参数调整。因此，**在 backward pass（后向处理）过程中，我们必须恢复 forward pass （前向处理）过程中每个像素的完整混合点序列**。一种解决方案是在全局内存中存储每个像素任意长的混合点列表（Kopanas 等人，2021 年）。**为了避免隐含的动态内存管理开销，我们选择再次遍历每个 tile 列表**；我们可以重复使用前向遍历中的高斯排序数组和 tile 范围。为了方便梯度计算，我们现在从后向前遍历它们。

遍历从影响 tile 中任何像素的最后一个点开始，并再次以协作方式将点加载到共享内存中。此外，只有当每个像素点的深度小于或等于在 forward pass 中对其颜色有贡献的最后一个点的深度时，才会开始对其进行（昂贵的）重叠测试和处理。计算第 4 章所述的梯度需要原始混合过程中每一步累积的不透明度值。我们可以只存储前向传递结束时的总累积不透明度，而不是在后向传递中遍历一个逐渐缩小的不透明度显式列表，从而恢复这些中间不透明度。具体来说，在前向遍历过程中，每个点都会存储最终累积的不透明度 （opacity） $\alpha$ ；在后向遍历过程中，我们将其除以每个点的 $\alpha$ ，从而得到梯度计算所需的系数。

# 7  实现、成果和评估
Implementation, results and evaluation

![[Pasted image 20240404114050.png]]
>图 5 我们展示了我们的方法与以往方法的比较，以及相应的从保留的测试视图中获得的 GT 图像。场景自上而下依次为来自 Mip-NeRF360 数据集的自行车、花园、树桩、柜台和房间；来自深度混合数据集（Hedman et al. 非明显的质量差异用箭头/嵌入式标注。

![[Pasted image 20240404114228.png]]
>图 6.对于某些场景（如上图），我们可以看到，即使迭代 7K 次（该场景 ∼ 5 分钟），我们的方法也能很好地捕捉列车。在迭代 30K 次（ ∼ 35 分钟）时，背景伪影明显减少。对于其他场景（如下图），差异几乎不明显；7K 次迭代（ ∼ 8 分钟）的质量已经非常高了。

## 7.1 实现
我们使用 PyTorch 框架在 Python 中实现了我们的方法，并编写了用于光栅化的自定义 CUDA 内核，这些内核是以前方法的扩展版本(Kopanas et al., [2021](#bib.bib26))，并使用英伟达 CUB 排序例程进行快速 Radix 排序(Merrill and Grimshaw, [2010](#bib.bib31))。我们还使用用于交互式查看的开源 SIBR(Bonopera et al., [2020](#bib.bib5))构建了一个交互式查看器。我们用它来测量我们实现的帧速率。源代码和所有数据可在以下网址获取： https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/

**优化细节**
为了保持稳定，我们在较低分辨率下进行计算 "热身（warm-up）"。具体来说，我们使用 4 倍较小的图像分辨率开始优化，并在迭代 250 次和 500 次后进行两次升采样。
SH 系数优化对缺乏角度信息很敏感。对于典型的 "NeRF-like "捕捉，即通过围绕中心物体的整个半球拍摄的照片来观测中心物体，优化效果很好。但是，如果捕捉的角度区域缺失（例如，捕捉场景的角落或进行 "内向外"（Hedman 等人，2016 年）捕捉时），优化就会产生完全错误的 SH 零阶分量（即基色或漫反射色）值。**为了解决这个问题，我们首先只对零阶成分进行优化，然后每迭代 1000 次就引入一个 SH 波段（one band of the SH），直到所有 4 个 SH 波段都得到体现。**


## 7.2 结果与评估
略
![[Pasted image 20240405142231.png]]
## 7.3 消融实验
略

## 7.4  局限性
我们的方法并非没有局限性。**在场景观察不佳的区域，我们会产生伪影**；在这些区域，其他方法也很难做到（如图 11 中的 Mip-NeRF360）。尽管各向异性高斯具有上述许多优点，但我们的方法也会产生拉长的伪影或 "斑点状 "高斯（见图 12）；同样，以前的方法在这种情况下也很难奏效。
![[Pasted image 20240404114845.png]] 
>图 11.伪影对比：Mip-NeRF360 具有 "浮点 "和颗粒状外观（左图，前景），而我们的方法会产生粗糙的各向异性高斯，造成低细节视觉效果（右图，背景）。火车场景。


![[Pasted image 20240404115112.png]]
>图 12在与训练时看到的视图重叠较少的视图中，我们的方法可能会产生伪影（右图）。同样，Mip-NeRF360 在这些情况下也会产生伪像（左图）。DrJohnson 场景。


当我们的优化产生大面积高斯时，偶尔也会出现 "弹出 "伪影（popping artifacts）；这往往发生在视图相关的区域。产生这些 "弹出伪影 "的原因之一是光栅化器中的保护带琐碎地剔除了高斯。如果采用更有原则的剔除方法，就能减少这些伪影。
另一个因素是我们的简单可见度算法，它可能导致高斯突然切换深度/混合顺序。这可以通过抗锯齿来解决，我们将这作为未来的工作。此外，我们目前没有对优化进行任何正则化处理；这样做将有助于解决未见区域和弹出伪影问题。

虽然我们在全面评估中使用了相同的超参数，但早期实验表明，在超大型场景（如城市数据集）中，降低位置学习率可能是收敛的必要条件。

尽管与之前基于点的方法相比，我们的方案非常紧凑，但**内存消耗却明显高于基于 NeRF 的解决方案**。在训练大型场景时，我们的未优化原型的 GPU 内存消耗峰值可能超过 20 GB。不过，通过对优化逻辑进行细致的底层实施（类似于 InstantNGP），这一数字可以大大降低。

渲染训练好的场景需要足够的 GPU 内存来存储完整模型（大型场景需要几百兆字节），光栅化器还需要 30-500 MB，具体取决于场景大小和图像分辨率。我们注意到，还有很多机会可以进一步降低我们方法的内存消耗。点云压缩技术是一个研究得很透彻的领域（De Queiroz 和 Chou，2016 年）；看看如何将这些方法应用到我们的表示法中会很有趣。

# 8 讨论与结论

我们首次提出了在各种场景和捕捉方式中真正实现实时、高质量辐射场渲染的方法，同时所需的训练时间也可与之前最快的方法相媲美。我们选择的3D高斯基元既保留了用于优化的体积渲染特性，又能直接实现基于 tile 的快速光栅化。**我们的工作表明，与普遍接受的观点相反，要实现快速、高质量的辐射场训练，并不完全需要连续表示法。**

我们大部分（ ∼ 80%）的训练时间都花在 Python 代码上，因为我们用 PyTorch 构建了我们的解决方案，以便其他人也能轻松使用我们的方法。只有光栅化程序是以优化的 CUDA 内核实现的。我们希望将剩余的优化工作完全移植到 CUDA 中，例如 InstantNGP（Müller 等人，2022 年）中所做的那样，这样就能为那些对性能要求极高的应用进一步大幅提速。

我们还展示了基于实时渲染原理、利用 GPU 的强大功能和软件光栅化流水线架构的速度的重要性。这些设计选择是提高训练和实时渲染性能的关键，在性能上比以往的体积光线步进技术更具竞争优势。

**我们很想知道，我们的高斯模型能否用于对捕捉到的场景进行网格重建**。鉴于网格的广泛使用，这不仅具有实际意义，还能让我们更好地了解我们的方法在体积表示法和表面表示法之间的连续性中的确切位置。

总之，我们提出了首个辐射场实时渲染解决方案，其渲染质量可与之前最昂贵的方法相媲美，训练时间可与现有最快的解决方案相媲美。


