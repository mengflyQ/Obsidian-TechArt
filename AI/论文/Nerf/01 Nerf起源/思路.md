
# 管线

![[论文：Nerf将场景表示为用于视图合成的神经辐射场#^iuvbi2]]
![[Pasted image 20240324145434.png]]
>将隐式的模型信息存入神经网络，而不是 mesh 点云等

> [!NOTE]
> 隐式表达都是连续函数，在需要渲染时对函数进行离散化采样。
> ![[Pasted image 20240320145451.png|350]]
> 比如: 绿色区域都表示一个函数 f (xyz)，选其中的红色的一条线段，则在线段上取一个点，渲染时就可以离散化的按照函数得到对应的 f (x0, y0, z0) => color c; .

我们将连续场景表示为 5D 向量值函数，其输入为采样点位置 $x=(x,y,z)$ 和相机观察方向 $(θ，∅)$，其输出为发出的颜色 $c=(r,g,b)$ 和**体积密度** $σ$。在实践中，我们把方向 $(θ，∅)$ 表示为三维笛卡尔坐标的单位向量 $d$。
>采样点位置是通过相机位姿计算得到，数据集的原始输入实际是图片和相机位姿
![[Pasted image 20240324143427.png]]
> $(θ，∅)$ 即球坐标表示

**5D 场景表示函数（辐射场）** 可以写为： $F_Θ:(x, d)→(c, σ)$ ，即从每个输入 5D 坐标映射到其相应的体积密度和颜色。

# 输入
多角度位姿的图像：
1. 照片清晰无模糊
2. 光照条件相同，大致在同一个尺度下 
3. 要有足够多的图片输入
4. 位姿要准确

# 神经网络解析
![[论文：Nerf将场景表示为用于视图合成的神经辐射场#^lpcbbf]]

![[Pasted image 20240324145805.png]]
再来看看神经网络的结构，位置编码后的位置输入 $\gamma(x)$ 经过 8 层 256 维的全连接 RelU 层，包括将此位置信息接入到第 5 层的激活层的 SKIP 连接（代码中接入到激活之后)(残差结构：有助于解决梯度消失和梯度爆炸的问题); 第 9 层用于输出体素密度  $\sigma$  和 256 维的特征向量，使用 RelU 来保证 $\sigma$ 不会包含负数值; 该特征向量与经过位置编码的 wd)串联，再经过一个 128 维的全连接层，最后输出与方向  $\gamma(d)$  相关的，在位置 x 的颜色预测。 **$\gamma(x)$ 表示的是经过位置编码后的坐标信息，由原本的 3 维信息升维到了 60 维，L=10;  $\gamma(d)$ 表示经过位置编码后的观察方向，由 2 维升到了 24 维，L=4; **
![[Pasted image 20240324150211.png]]


# 体渲染
![[Pasted image 20240324144529.png]]

![[Pasted image 20240324150351.png]]
采样点沿着射线取，输入到网络推理出每个采样点的 rgba，然后通过体渲染公式（c）得到该射线对应到 2D 图片上的像素值 (d)。

公式中的
$\sigma$：体积密度
$T(t)$：体渲染的透射率

**采样点是离散的，所以不能直接通过积分计算 $C(r)$ ，实际使用黎曼和法估算积分 $\hat{C}(\mathbf{r})$ 作为结果（下面的公式）**
其中：
$T_i$：表示经过前面的采样点后一共透射多少
$(1-exp(-\sigma_i\delta_{i}))$：1-当前采样点的透射率，即当前点不能透射的率，也可以看作不透明度。
**两者相乘表示该采样点保留了多少radiance**


# 优化
[[论文：Nerf将场景表示为用于视图合成的神经辐射场#5 Optimizing a Neural Radiance Field]]
虽说我们可以做到构建三维场了，但是还应该考虑效率和生成结果的质量究竟如何。简单分析一下啊，我们将连续的 c 函数和 o 函数分成了 N 段，用区间近似表示，这很明显，选的 N 过大训练时间肯定是漫长的，还有一个问题，高频信息肯定会在我们离散化阶段丢失一部分，这就导致我们重建出的结果表达低频信息效果比较好。
**再深入解释，则是 MLP 结构是全连接层，只进行矩阵的乘法和加法，本质上是进行线性运算，因此这种运算是平滑的，体现在训练结果上就偏向于模糊平滑**。下面是真实图片和普通方式构建神经网络的对比效果。.

**引入了两个改进，以实现高分辨率复杂场景的表示。**
1. 输入坐标的<mark style="background: #FF5582A6;">位置编码</mark>，有助于 MLP 表示高频函数（**在将输入传递到网络之前，使用高频函数将输入映射到更高维空间，能够更好地拟合包含高频变化的数据。**）
2. <mark style="background: #FF5582A6;">分层体积采样</mark>，允许我们有效地对该高频表示进行采样。

![[Pasted image 20240324145738.png]]

分层体素采样 (Hierarchical volume sampling)技术，是用来加速渲染过程的一个优化方案。普通情况下的密度场中，其实有很大一部分区域都是空的，均匀采样会在信息密集的地方采样频率较少。所以如同 LOD 技术一般，分层采样是一个极佳的思路。.
![[Pasted image 20240320160940.png]]
**如图所示，在射线采样时，首先按照大的尺度采样 N 个点，通过第一次采样点判断密度场 o 的密度集中区域，再对密度集中的区域进行更精细采样。其中分为粗网络（coarse）和精网络 (fine)。。**
![[Pasted image 20240324151932.png]]
这个归一化公式是一个 PDF 概率密度函数，即我根据 PDF 进行稠密采样。

# 迭代结果 loss
![[Pasted image 20240324152201.png]]
得到一次训练的结果后，我们需要通过残差来与原始图像对比，通过采样任意 R 集合内的数据，进行方差比较，随着训练次数的增加，Loss 值会逐渐降低，但这一过程往往时间会非常长。.

损失（loss）只是 coarse 渲染和 fine 渲染的渲染像素颜色和真实像素颜色之间的总平方误差： 
$$
\mathcal L=\sum_{\mathbf{r}\in\mathcal{R}}\left[\left\lVert\hat{C}_{c}(\mathbf{r})-C(\mathbf{r})\right\rVert_{2}^{2}+\left\lVert\hat{C}_{f}(\mathbf{r})-C(\mathbf{r})\right\rVert_{2}^{2}\right]\tag{6}
$$
# 训练
[[论文：Nerf将场景表示为用于视图合成的神经辐射场#4 Volume Rendering with Radiance Fields]] 

我们用照片生成场函数是困难的，但是我们可以先有一个场函数，然后通过体渲染得到一个 2D 图片，与原始图相比较，进而不断迭代场函数，最终得到一个可以高度匹配原始图的场。具体步骤如下：
1. **数据准备**: 准备一组包含二维图片和相应相机参数的训练数据。每张图片是从不同角度捕获的场景快照。每个训练样本由一张图片和对应的相机参数组成。 
2. **视线光线的生成:** 对于每张图片，根据相机参数生成视线光线。这些光线起始于相机位置，穿过图像平面上的每个像素，并延伸到场景中。
3. **光线与体积函数的交互**: 对于每条视线光线，将光线的起始点传入体积函数（通常是一个多层感知器)。体积函数会输出沿着光线的颜色和密度值。
4. **损失计算**: 将体积函数输出的颜色与实际图像中对应像素的颜色进行比较，计算损失。损失衡量了预测图像与真实图像之间的差异。.
5. **反向传播和优化**: 使用反向传播算法，将损失反向传播到神经网络中，计算梯度并进行权重更新。这个过程采用优化算法（如随机梯度下降）来逐步调整神经网络的权重，以最小化损失函数。.
6. **迭代训练**: 重复以上步骤，逐渐迭代训练神经网络。每次迭代都会使用不同的训练样本，以便网络能够从各个角度和视角学习场景的三维表示。.
7. **收敛与模型保存**: 训练过程会持续一段时间，直到模型达到一定的收敛程度，即损失足够小。在训练完成后，可以保存训练好的体积函数模型。

总结来说，Scene Modeling 阶段就是通过在多个视角下的输入图像和相机参数，让神经网络学习到一个能够将三维空间点映射到颜色和密度值的体积函数。这个过程实际上是在从输入图像中学习场景的三维结构和属性，并将其表示为神经网络的权重。这使得网络在渲染阶段能够根据相机参数和光线来生成场景的虚拟图像
到这里，理论上就可以使用神经网络去构建三维场了，NeRF 的基本原理也就是这样的，但是! BUT! 懂得都懂，事情还没有这么简单。.
# 总结
1、首先我们通过拍照等方式，获取物体不同角度的图片。.
2、对图片像素取样进行光线采集，先在三维空间利用几何关系和内参矩阵 K 求得表示光线方向的向量，再利用外参矩阵将相机坐标系变换到世界坐标系。.
3、采样射线，按照分层采样方式采样 Nc + Nf 个点。
4、将点集进行位置编码，升维到高纬度。
5、将编码后的坐标送入 8 层 MLP 神经网络中，在第 5 层时再次加入坐标信息以提高准确度，此时已经得到密度场，然后再送入编码后的视角信息，经过神经网络后变为 RGB 颜色信息。.
6、体渲染目标函数场，得到训练结果与原图进行残差比较，迭代 3~5 步骤。
7、输出保存函数场，运用体渲染得到最终成像。.

总的来说，与点云、体素、三角网格等可以通过遍历存储空间中的所有元素来访问的显式几何不同，**为了访问隐式几何，我们只能选择空间坐标作为采样点的输入（毕竟 groundtrue 是图片而不是三维形体）**。隐式场景将输出这些点的几何密度和颜色。而神经隐式几何则是通过神经网络将上述输入和输出进行转换。通过对光线上的一系列采样点进行加权积分，就可以渲染出一个像素的颜色（在这一步就将隐式的采样点转回图片的像素点）。这就是渲染 NeRF 神经辐射场的大致流程
# 拓展
## 优缺点
优点：
1. 新视点合成效果逼真
2. 输入图像比较稠密
3. Continuous 3D Shapes（表达连续 3D 形状）
   
缺点：
1. 每个场景必须重新训练, 没有泛化能力
2. 输入图像比较稠密
3. 只能处理静态场景
4. 光照必须固定 
5. 训练速度慢
6. 无法编辑

![[Pasted image 20240401144413.png]]
## mip-NeRF
这项技术看名字就知道，原理上是有 mipmap 的思想的，前面提到的分级光线米样具实已绘足及种思想小，别线上木样点减少可以适度提升效率，那是不是可以采用更好的方式减少射线数量，这样米样点己个是更少」。及里的优化地刀则是主要将发射的射线改成了圆锥，这是因为近大远小的问题，射线从相机发出，其实较远的地方贡献度就比较低了，因此
我们将射线改成圆锥，将采样点改成采样圆台区域。,

![[Pasted image 20240320162332.png]]


这样做的另外好处是，圆锥体采样提高了抗锯齿能力，这就像是一张 Texture 纹理，单纯逐像素采样是会有锯齿的，但如果采用双线性插值的方式采样，那肯定是效果好很多。.

## PlenOctrees

PlenOctrees 在训练模型阶段，与传统 NeRF 技术是相同的，这项技术的贡献主要体现在渲染模型阶段。我们得到场函数后，依旧需要再投射光线、采样和渲染，而一般来说，场中大部分的内容都是空白的，因此这个技术使用了基于稀疏体素的八叉树对场函数空间划分，只在信息密集的地方进行多采样渲染。.
还有，为了保留诸如镜面反射之类的与视图相关的效果，PlenOctrees 通过封球谐函数对外观进行分解。.
![[Pasted image 20240320162619.png]]

看似简单的操作，带来的效率是恐怖的，在渲染质量基本相同的情况下，它的速度比传统 NeRF 快了 3000 倍，可以做到 800*800 分辨率的图片进行 150 帧的实时渲染。"
## Instant-ngp
## Plenoxels.
机器学习又困难，效率又低，那有没有一种不需要机器学习的方式来重建神经场呢? 有! 而且效果还挺好。
## Block-nerf

## NeusSDF

